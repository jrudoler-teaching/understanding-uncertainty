[
  {
    "objectID": "assignments/assignment-00.html",
    "href": "assignments/assignment-00.html",
    "title": "Assignment 00",
    "section": "",
    "text": "print(\"Hello, World!\")\n\nHello, World!"
  },
  {
    "objectID": "notebooks/lecture-02.html",
    "href": "notebooks/lecture-02.html",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathbb{P}}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability",
    "href": "notebooks/lecture-02.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\\]\nBy definition this quantity cannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and it must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs).\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)). If we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\n\nProbability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\).\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows: \\[\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\\]\n\n\nAddition and multiplication rules (and / or)\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that \\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as \\(B\\) above, so we can use the same calculation: \\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\).\n\n\n\n\n\n\nNote on notation\n\n\n\n\n\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively. The notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively. So we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads. Technically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs).\n\n\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events \\(A\\) and \\(B\\), the following rules hold:\n\nAddition rule: For any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\n\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram. \n\n\n\n\nMultiplication rule: For any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset.\n\n\n\n\n\n\n\nConditional probability\n\n\n\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”. The conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day.\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\nSo the conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\\]\n\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”\n\n\n\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n\nIndependence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\nHow does this relate to the multiplication rule? If \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\). That is, knowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply \\(\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\).",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-functions",
    "href": "notebooks/lecture-02.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\nHowever, it is often more convenient to work with probability functions. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails).\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\n\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values.\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n\n\n\n\n\n\n\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\nCode\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#random-variables",
    "href": "notebooks/lecture-02.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\).\nWe use probability functions to describe the probabilities associated with random variables. Specifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nBernoulli random variable\n\n\n\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin.\n\n\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUniform random variable\n\n\n\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\).\n\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "href": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nWe call the probability function for a random variable a probability distribution, which describes how the probabilities are distributed across the possible values of the random variable.\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point.\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. This is called a histogram.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\")\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nThe histogram is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the histogram of the data, we would see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height in the histogram. Let’s check it out:\n\n\nCode\n# load in the dice rolls data\ndice_rolls_df = pd.read_csv(\"../data/dice_rolls.csv\")\nprint(\"Total number of dice rolls:\", len(dice_rolls_df))\ndice_rolls_df.head(10)\n\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1\n\n\n\n\n\n\n\n\n\nCode\nplt.hist(dice_rolls_df['rolls'], bins=np.arange(0.5, 7.5, 1), density=True)\nplt.title('Histogram of Dice Rolls')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.xticks(np.arange(1, 7))\nplt.grid(axis='y', alpha=0.75)\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html",
    "href": "notebooks/lecture-00.html",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#welcome",
    "href": "notebooks/lecture-00.html#welcome",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#why-statistics",
    "href": "notebooks/lecture-00.html#why-statistics",
    "title": "Lecture 00",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#description",
    "href": "notebooks/lecture-00.html#description",
    "title": "Lecture 00",
    "section": "Description",
    "text": "Description\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features.\n\n\nCode\n# from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.rename(columns={\"neighbourhood_group\": \"borough\"})\nairbnb = airbnb.dropna(subset=[\"borough\", \"price\", \"long\", \"lat\"])\nairbnb[\"borough\"] = airbnb[\"borough\"].str.lower()\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"manhatan\", \"manhattan\")\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"brookln\", \"brooklyn\")\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n# print the first 5 rows\nairbnb[:5]\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n3\n1002755\nNaN\n85098326012\nunconfirmed\nGarry\nbrooklyn\nClinton Hill\n40.68514\n-73.95976\nUnited States\n...\n$74\n30.0\n270.0\n7/5/2019\n4.64\n4.0\n1.0\n322.0\nNaN\nNaN\n\n\n4\n1003689\nEntire Apt: Spacious Studio/Loft by central park\n92037596077\nverified\nLyndon\nmanhattan\nEast Harlem\n40.79851\n-73.94399\nUnited States\n...\n$41\n10.0\n9.0\n11/19/2018\n0.10\n3.0\n1.0\n289.0\nPlease no smoking in the house, porch or on th...\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\nNow there’s a lot you can do, but let’s start by visualizing the prices of listings.\n\n\nCode\n# plot a histogram of the price column\nplt.figure(figsize=(10, 5))\nplt.hist(airbnb['price'], bins=50)\nplt.title('Prices of Airbnb Listings in NYC')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\nCode\nairbnb[\"price\"].describe()\n\n\ncount    102316.000000\nmean        625.291665\nstd         331.677344\nmin          50.000000\n25%         340.000000\n50%         624.000000\n75%         913.000000\nmax        1200.000000\nName: price, dtype: float64\n\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the geopandas library to plot the locations of listings on a map of New York City.\n\n\nCode\nimport geopandas as gpd\nfrom geodatasets import get_path\n# load the shapefile of NYC neighborhoods\nnyc_neighborhoods = gpd.read_file(get_path('nybb'))\nnyc_neighborhoods = nyc_neighborhoods.to_crs(epsg=4326)  # convert to WGS84\n# plot the neighborhoods with airbnb listings\nnyc_neighborhoods.plot(figsize=(8, 8), color='white', edgecolor='black')\n# plot the airbnb listings on top of the neighborhoods\n# use the 'long' and 'lat' columns to create a GeoDataFrame\nairbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb['long'], airbnb['lat']), crs='EPSG:4326')\n# set the coordinate reference system to WGS84\nairbnb_gdf.plot(ax=plt.gca(), column=\"price\", markersize=3, alpha=0.05, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\nplt.title('Airbnb Listings in NYC')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics only describe the data.\nWhy is this limiting? After all, we like data – it tells us things about the world and it’s objective and quantifiable.\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\nLet’s look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small “sample” or subset of the data?\n\n\nCode\n# separately plot 3 samples of airbnb listings\nfig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(4):\n    # sample 1000 listings\n    sample = airbnb.sample(100, random_state=i)\n    # plot the neighborhoods with airbnb listings\n    nyc_neighborhoods.plot(ax=ax[i], color='white', edgecolor='black')\n    # plot the airbnb listings on top of the neighborhoods\n    # use the 'long' and 'lat' columns to create a GeoDataFrame\n    airbnb_gdf_sample = gpd.GeoDataFrame(sample, geometry=gpd.points_from_xy(sample['long'], sample['lat']), crs='EPSG:4326')\n    # set the coordinate reference system to WGS84\n    airbnb_gdf_sample.plot(ax=ax[i], column=\"price\", markersize=3, alpha=0.8, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\n    ax[i].set_title(f'Airbnb Listings in NYC (Sample {i+1})\\n Average Price: ${sample[\"price\"].mean():.2f}')\n    ax[i].set_xlabel('Longitude')\n    ax[i].set_ylabel('Latitude')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\n\n\n\n\n\nSample vs. Population\n\n\n\n\n\nA population is the entire set of data that you are interested in. A sample is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.\nNote that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.\nOften, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#inference",
    "href": "notebooks/lecture-00.html#inference",
    "title": "Lecture 00",
    "section": "Inference",
    "text": "Inference\nSo what if we want to answer questions about a population based on a sample? This is where inference comes in. Specifically, we want to use the given sample to infer something about the population.\nHow do we do this if we can’t ever see the entire population? The answer is that we need a link which connects the sample to the population – specifically, we can explicitly treat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\nThere is always a DGP\n\n\n\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.\nThere has to be a DGP, even if we don’t know what it is. The DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\nThis is where the model comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess – a guess about where your data come from.\nFor example, we might assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. (So the probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.)\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\n\nCode\n# number of listings per borough\nborough_counts = airbnb['borough'].value_counts().rename_axis('borough').reset_index(name='count')\nborough_counts['borough'] = borough_counts['borough'].str.title()  # capitalize\n# normalize the counts to proportions\nborough_counts['proportion'] = borough_counts['count'] / borough_counts['count'].sum()\n\nplt.figure(figsize=(8, 5))\nplt.bar(borough_counts['borough'], borough_counts['proportion'])\nplt.axhline(y=1/5, color='r', linestyle='--', label='Equal Proportion (20%)')\nplt.title('Proportion of Airbnb Listings per Borough')\nplt.xlabel('Borough', fontsize=14)\nplt.ylabel('Proportion', fontsize=14)\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe inference question we want to ask is roughly:\n“If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\nNotice that this is a question about the probability of the sample, given a certain model of the DGP. In our Airbnb example above, it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\nWhat should we do now? Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. After all, the model is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\n\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n\n\n\n\n\nDon’t try this at home!\n\n\n\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. This requires domain knowledge and subject matter expertise. In the Airbnb example, you would want to know something about the five boroughs of New York City before starting your analysis. Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption (Manhattan sees vastly more tourism than the other boroughs, and Brooklyn and Queens have by far the most residents according to recent census data).\nStatistical inference is a powerful tool, but it is not a substitute for understanding the data and the context in which it was collected. Modeling might be guesswork, but it is best if it is informed guesswork.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#prediction",
    "href": "notebooks/lecture-00.html#prediction",
    "title": "Lecture 00",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data, we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. We won’t go into the details of the model we use here, but the basic idea is that we assume the features of the listing (e.g., price) is related to the probability of it being in a certain borough (perhaps more expensive listings are more likely to be in Manhattan, for example).\n\n\n\n\n\n\nFitting a model\n\n\n\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nThe model for a coin flip, for example, has a single parameter: the probability of landing on heads. If you turn the knob to 0.5, you get a fair coin; if you turn it to 1.0, you get a coin that always lands on heads; if you turn it to 0.0, you get a coin that always lands on tails.\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data.\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport seaborn as sns\n\n# prepare the data for linear regression\nfeature_columns = [\"price\", \"room_type\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"review_rate_number\"]\nairbnb_clean = airbnb.dropna(subset=feature_columns + [\"borough\"])\nX = airbnb_clean[feature_columns]\n# convert categorical variables to lowercase\nX.loc[:, \"room_type\"] = X[\"room_type\"].str.lower()\ny = airbnb_clean[\"borough\"].values.reshape(-1)\n# convert categorical variables to dummy variables\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\nX[\"rating_interaction\"] = X[\"reviews_per_month\"] * X[\"review_rate_number\"]\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n# fit the logistic regression model\nmodel = LogisticRegression(max_iter=1000, solver=\"newton-cholesky\")\nmodel.fit(X_train, y_train)\n# make predictions on the test set\ny_pred = model.predict(X_test)\n# calculate the prediction accuracy\naccuracy = np.mean(y_pred == y_test)\nprint(f\"{40*'='}\")\nprint(f'Prediction Accuracy: {accuracy:.2%}')\nprint(f\"{40*'='}\")\n# calculate the confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=airbnb['borough'].unique())\n# confusion_matrix = [confusion_matrix[i].T for i in range(len(confusion_matrix))] \nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\nax = ax.flatten()\nax[-1].axis('off')  # turn off the last subplot since we have only 5 boroughs\n# plot the confusion matrix for each borough\nfor i, borough in enumerate(airbnb['borough'].unique()):\n    sns.heatmap(confusion_matrix[i], annot=True, fmt='d', cmap='Blues', ax=ax[i], cbar=False)\n    ax[i].set_xlabel('Predicted', fontsize=10)\n    ax[i].set_ylabel('Actual', fontsize=10)\n    ax[i].set_xticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\n    ax[i].set_yticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\nplt.tight_layout()\n_ = plt.show()\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================\n\n\n\n\n\n\n\n\n\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\n\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. For now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. For example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance.\n\n\n\nNow let’s take a look at the distribution of the model’s predictions.\n\n\nCode\n# plot the counts of predicted listings per borough side by side with the actual counts\nimport seaborn as sns\nplot_df = pd.DataFrame({\n    'predicted': pd.Series(y_pred.flatten()),\n    'actual': pd.Series(y_test.flatten())\n})\nplot_df = plot_df.melt(var_name='type', value_name='borough')\nax = sns.catplot(x='borough', hue='type', data=plot_df, kind='count', height=5, aspect=1.2, alpha=0.7)\nsns.move_legend(ax, loc=\"center right\", title=None)\nplt.title('Predicted vs Actual Airbnb Listings per Borough')\nplt.xlabel('Borough')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n\n\n\n\n\nPrediction and inference can interact\n\n\n\n\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "understanding-uncertainty",
    "section": "",
    "text": "This website contains course materials for the Understanding Uncertainty course, which is a part of the US-Israel Academic Bridge Fellowship hosted at the University of Pennsylvania.\n\n\n\nSomething from DALL-E’s imagination"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nDescription\nDue Date\n\n\n\n\nAssignment 1"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: Understanding Uncertainty: An Introduction to Statistics through Data Generating Processes and Computational Simulations\nCourse Schedule: 4 Weeks, 3 sessions per week, 1.5 hours per session\n\n\n\nWeek\nSession\nTopics Covered\nActivities/Assignments\n\n\n\n\n1\nSession 1\n- Course Introduction- Understanding Data Generating Processes (DGPs)- Basic Python Syntax and Data Types\n- Install Python and Jupyter- Assignment: Write a simple Python program link\n\n\n\nSession 2\n- Data Structures in Python (Lists, Tuples, Dictionaries)- Importing and Exporting Data- Generating Random Numbers\n- Load and explore datasets- Assignment: Simulate a simple DGP\n\n\n\nSession 3\n- Descriptive Statistics- Measures of Central Tendency and Variability- Data Visualization with Matplotlib and Seaborn\n- Plot histograms and boxplots- Assignment: Analyze summary statistics of a dataset\n\n\n2\nSession 4\n- Probability Concepts- Discrete and Continuous Distributions- Simulating Random Events\n- Simulate coin tosses and dice rolls- Assignment: Probability simulations\n\n\n\nSession 5\n- Exploring Different Distributions- Normal, Binomial, Poisson, Exponential Distributions- Central Limit Theorem (CLT)\n- Visualize distributions and CLT demonstrations- Assignment: CLT simulation project\n\n\n\nSession 6\n- Understanding Uncertainty- Confidence Intervals- Margin of Error- Introduction to Hypothesis Testing\n- Calculate confidence intervals- Assignment: Formulate and test hypotheses\n\n\n3\nSession 7\n- Resampling Methods- Bootstrapping and Permutation Tests- Practical Applications\n- Conduct bootstrap and permutation tests- Assignment: Resampling project\n\n\n\nSession 8\n- Bias and Variance Trade-off- Overfitting and Underfitting- Model Complexity and Generalization\n- Visualize bias-variance trade-off- Assignment: Experiment with model complexity\n\n\n\nSession 9\n- Introduction to Machine Learning- Supervised vs. Unsupervised Learning- Training and Testing Sets- Evaluation Metrics\n- Split data and evaluate models- Assignment: Build and evaluate predictive models\n\n\n4\nSession 10\n- Regularization Techniques- Ridge and Lasso Regression- Fairness in Machine Learning\n- Implement regularization- Assignment: Regularization and fairness assessment\n\n\n\nSession 11\n- Population Statistics and Representation- Sampling Methods and Sampling Bias- Ethical Considerations in Data Science\n- Simulate sampling strategies- Assignment: Design a fair sampling plan\n\n\n\nSession 12\n- Course Review and Future Directions- Student Project Presentations- Trends in Statistics and Data Science\n- Present final projects- Course Feedback Survey"
  },
  {
    "objectID": "notebooks/lecture-01.html",
    "href": "notebooks/lecture-01.html",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "",
    "text": "This lecture will be, intentionally, a bit of a whirlwind. That’s because with the advent of large language models (LLMs) like ChatGPT, Claude, Gemini, etc. knowing how to program in specific languages like Python is becoming less important. You don’t need that much practice or to focus on the syntax of a specific language.\nInstead, the important thing is to understand the core concepts involved in programming, which are largely universal across languages. This high-level understanding will allow you to use LLMs effectively to write code in any language, including Python. If you don’t understand the concepts, you won’t be able to identify when the LLM is making mistakes or producing suboptimal code.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#variables-and-types",
    "href": "notebooks/lecture-01.html#variables-and-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables and types",
    "text": "Variables and types\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\n\n\n\n\n\n\nFunctions act on variables\n\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable.\n\n\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations.\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean\nTrue or False values, e.g., True, False\n\n\n\nLet’s discuss a few important ones in more detail\n\n\n\n\n\n\nEverything is an object\n\n\n\nIn Python, everything is an object. This means that even basic data types like integers and strings are treated as objects with methods and properties. For example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\nSee the later section on Object-Oriented Programming for more details.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#lists",
    "href": "notebooks/lecture-01.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\nWe often need to store multiple values together. The most basic way to achieve this is with a list. A list is an ordered collection of items that can be of any type, including other lists. “Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\nprint(my_list[0])  # Output: 1\n\n1\n\n\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#arrays-numpy",
    "href": "notebooks/lecture-01.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\nPackages\n\n\n\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array().\n\n\n\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]\n\n\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]\n\n\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\n\nAdvanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\n\nCode\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]\n\n\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [1 2 3 4 5]\nBoolean mask: [False False  True  True  True]\nFiltered array: [3 4 5]",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dictionaries",
    "href": "notebooks/lecture-01.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}\n\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25\n\n\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dataframes",
    "href": "notebooks/lecture-01.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns). Think of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago\n\n\n\n\n\n\n\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#conditional-logic",
    "href": "notebooks/lecture-01.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false.\n\n\n\nHere’s an example of how to use these statements. Play around with the code below to see how it works. You can change the value of age to see how the output changes based on different conditions.\n\n\n\n\n\n\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false.\n\n\n\n\n\n\nBoolean expressions\n\n\n\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#loops",
    "href": "notebooks/lecture-01.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops.\n\nFor Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one.\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4. The enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n\n\n\n\nWhile Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#functions-and-functional-programming",
    "href": "notebooks/lecture-01.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result). You can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters. Here’s an example:\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\nProbably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]\n\n\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#object-oriented-programming",
    "href": "notebooks/lecture-01.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and \\ behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created.\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse\n\n\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality.\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\n\n\n\n\n\n\n\n\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#summary",
    "href": "notebooks/lecture-01.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#footnotes",
    "href": "notebooks/lecture-01.html#footnotes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing.↩︎",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-10.html",
    "href": "notebooks/lecture-10.html",
    "title": "Lecture 10: ANOVA",
    "section": "",
    "text": "We want to emphasize that nearly any statistical method with a closed-form solution can be replicated with a simulation.\nTake ANOVA, or analysis of variance, for example. The ANOVA test is used to determine whether there are any statistically significant differences between the means of two or more groups. It is a parametric test that assumes the data is normally distributed and that the variances of the groups are equal.\nThe exact test we compute is the F-test, which compares the variance between groups to the variance within groups. The null hypothesis is that all group means are equal, while the alternative hypothesis is that at least one group mean is different. The F-test relies on the fact that the ratio of two independent chi-squared variables (i.e. a normal variable, squared) follows an F-distribution.\nHere we will simulate the F-test by generating random data from a normal distribution and computing the F-statistic for different group means. We will see that under the null hypothesis (group means are equal), the F-statistic follows an F-distribution with degrees of freedom equal to the number of groups minus one and the total number of observations minus the number of groups.\nIn the next lecture we will compare the F-test to a non-parametric permutation test, which does not make any assumptions about the distribution of the data.",
    "crumbs": [
      "Home",
      "Lecture 10: Classification"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html",
    "href": "notebooks/lecture-03.html",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "",
    "text": "Now that we have a good understanding of the basics of probability, we can start to explore how we deal with randomness computationally.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "href": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from probability distributions",
    "text": "Sampling from probability distributions\n\nA sample is a subset of data drawn from a more general population. That population can be thought of as a probability distribution – this distribution essentially describes how likely you are to observe different values when you sample from it.\nWe will quickly review some important concepts related to sampling.\n\nIndependent and identically distributed (IID) sampling\nWhen we sample from a probability distribution, we often assume that the samples are independent and identically distributed (IID). This means that each sample is drawn from the same distribution and that the samples do not influence each other.\nCoin flips are a good example of IID sampling. If you flip a fair coin multiple times, each flip has the same probability of being heads or tails (this is the “identically distributed” part), and the outcome of one flip does not affect the outcome of another (this is the “independent” part). The same is true for rolling a die!\nWe often apply this concept to more complex random processes as well, where we do not have such a clear understanding of the underlying process. For example, if we are sampling the heights of people in a city, we might assume that each person’s height is drawn from the same distribution (the distribution of heights in that city) and that one person’s height does not affect another’s. Whether or not the IID assumption holds in practice is an important question to consider when analyzing data – for example, do you think that the heights of people in a family are independent of each other?\n\n\nSampling with and without replacement\nAnother important concept in sampling is the distinction between sampling with replacement and sampling without replacement.\n\nSampling with replacement means that after we draw a sample from the population, we put it back before drawing the next sample. This means that the same object / instance can be selected multiple times.\nSampling without replacement means that once we draw a sample, we do not put it back before drawing the next sample. This means that each individual can only be selected once. This can introduce dependencies between samples, as the population changes after each draw.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#simulating-random-processes",
    "href": "notebooks/lecture-03.html#simulating-random-processes",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating random processes",
    "text": "Simulating random processes\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code.\n\n\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\nNormal distribution\n\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean).\n\n\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\nrng = np.random.default_rng(seed=42)  # Create a random number generator with a fixed seed\nsamples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\nplt.hist(samples, bins=10, density=True)\nplt.show()\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]\n\n\n\n\n\n\n\n\n\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\n# Sampling without replacement\nsubsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n# another way to sample\npd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10)\n\n\nSample:\n [-0.09848548  0.14242574 -0.42832782 -1.32269961 -0.35213355  0.62559039\n  0.11668581  0.54315427 -1.13328721 -1.27568632]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n93\n-0.079718\n\n\n64\n0.711227\n\n\n73\n0.497161\n\n\n36\n-0.113947\n\n\n57\n1.494941\n\n\n65\n0.793347\n\n\n49\n0.067579\n\n\n31\n-0.406415\n\n\n23\n-0.154529\n\n\n0\n0.304717\n\n\n\n\n\n\n\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\npossible_rolls = [1, 2, 3, 4, 5, 6]\nprobabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\nrng.choice(possible_rolls, p=probabilities, size=100)\n\n\narray([1, 3, 4, 2, 4, 6, 1, 6, 6, 4, 2, 6, 2, 4, 6, 6, 6, 4, 6, 6, 4, 6,\n       1, 3, 6, 2, 2, 6, 6, 6, 6, 6, 2, 5, 4, 1, 6, 5, 5, 2, 2, 6, 1, 3,\n       3, 5, 6, 5, 1, 3, 6, 6, 6, 6, 6, 1, 4, 6, 4, 6, 5, 6, 6, 6, 6, 6,\n       6, 4, 5, 6, 2, 4, 6, 6, 2, 4, 2, 1, 6, 5, 4, 6, 1, 3, 6, 6, 6, 6,\n       5, 6, 6, 6, 6, 6, 6, 2, 6, 5, 6, 6])\n\n\n\nSimulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code).\nFor example, let’s say we want to simulate a process where we first flip a coin, and if it comes up head we roll a die, but if it comes up tails we",
    "crumbs": [
      "Home",
      "Lecture 03: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#simulating-a-random-sample",
    "href": "notebooks/lecture-03.html#simulating-a-random-sample",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating a random sample",
    "text": "Simulating a random sample\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code.\n\n\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\nNormal distribution\n\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean).\n\n\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\nrng = np.random.default_rng(seed=42)  # Create a random number generator with a fixed seed\nsamples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\nplt.hist(samples, bins=10, density=True)\nplt.show()\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]\n\n\n\n\n\n\n\n\n\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\n# Sampling without replacement\nrng = np.random.default_rng(seed=42)\nsubsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n# another way to sample; note RNG can be different in different packages\npd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10, replace=False, random_state=42)\n\n\nSample:\n [-1.32269961 -1.13328721 -0.01680116 -1.68286977  0.54315427 -1.68733443\n  0.85797588 -0.85304393 -0.04992591 -0.36305385]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n83\n-0.381738\n\n\n53\n-0.319671\n\n\n70\n-1.275686\n\n\n45\n0.218689\n\n\n44\n0.116686\n\n\n39\n0.650593\n\n\n22\n1.222541\n\n\n80\n0.456775\n\n\n10\n0.879398\n\n\n0\n0.304717\n\n\n\n\n\n\n\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\npossible_rolls = [1, 2, 3, 4, 5, 6]\nprobabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\nrng.choice(possible_rolls, p=probabilities, size=100)\n\n\narray([1, 3, 4, 2, 4, 6, 1, 6, 6, 4, 2, 6, 2, 4, 6, 6, 6, 4, 6, 6, 4, 6,\n       1, 3, 6, 2, 2, 6, 6, 6, 6, 6, 2, 5, 4, 1, 6, 5, 5, 2, 2, 6, 1, 3,\n       3, 5, 6, 5, 1, 3, 6, 6, 6, 6, 6, 1, 4, 6, 4, 6, 5, 6, 6, 6, 6, 6,\n       6, 4, 5, 6, 2, 4, 6, 6, 2, 4, 2, 1, 6, 5, 4, 6, 1, 3, 6, 6, 6, 6,\n       5, 6, 6, 6, 6, 6, 6, 2, 6, 5, 6, 6])\n\n\n\nSimulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nConsider dealing poker hands from a standard deck of cards. When you deal a hand, you draw cards one at a time, and each card drawn affects the next card that can be drawn (because you do not put the card back into the deck).\n\n\nCode\n# Make a deck of cards (Ace is 1, King is 13)\ndeck = np.arange(1, 14).repeat(4)  # 4 suits, each with cards 1 to 13\nprint(\"Deck of cards before shuffling:\\n\", deck)\ndeck = np.random.permutation(deck)\nprint(\"Deck of cards after shuffling:\\n\", deck)\n# deal 2 cards to each of 4 players\nrng = np.random.default_rng(seed=21)\n# Get flat indices of 8 cards from deck\n# we want to know exactly which cards are dealt\nchosen_indices = rng.choice(len(deck), size=8, replace=False)\nhands = deck[chosen_indices].reshape(4, 2)\nprint(\"Hands dealt to players:\\n\", hands)\n# remove the dealt cards from the deck\nremaining_deck = np.delete(deck, chosen_indices)\nprint(\"Remaining cards in the deck:\\n\", remaining_deck)\nboard = np.random.choice(remaining_deck, size=5, replace=False)\nprint(\"Community cards on the board:\\n\", board)\n\n\nDeck of cards before shuffling:\n [ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12\n 13 13 13 13]\nDeck of cards after shuffling:\n [12  3  3  1  5 13  3  6 13  4  8  5 11  7  8  3  7 12  2 11  1 10  9 13\n  9 11  7  5  6  8  1 10 10 11  4  2  4  8  6 13  6 10 12  9  2  7  4 12\n  2  1  5  9]\nHands dealt to players:\n [[ 1  5]\n [ 2  7]\n [12  8]\n [ 9  2]]\nRemaining cards in the deck:\n [12  3  3  1 13  3  6 13  4  8  5 11  8  3  7 11  1 10 13  9 11  7  5  6\n  1 10 10 11  4  4  8  6 13  6 10 12  9  2  7  4 12  2  5  9]\nCommunity cards on the board:\n [ 5  4  1  3 13]\n\n\nBut it can get even more complex than that. In many real-world scenarios, the process of generating data involves multiple steps or conditions that affect the outcome.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code). We then tend to write loops that simulate the process step by step, keeping track of the state of things as we go along.\nLet’s consider an example of a musician busking for money in Rittenhouse Square. The musician’s earnings might depend on various factors like the weather and and the number of passersby. To keep it simple, let’s assume that the musician earns $3 for every passerby who stops to listen. Of course, not every passerby will stop – let’s pretend every passerby has the same 20% chance of stopping.\nThe musician might want to know how much money they can expect to earn in a day of busking. We can simulate this process by generating a random number of passersby and then calculating the earnings based on the stopping probability.\n\n\n\n\n\n\nPoisson distribution\n\n\n\nThe Poisson distribution is commonly used to model the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It assumes that the events occur independently and at a constant average rate. In our example, we can use the Poisson distribution to model the number of passersby in a given time period (e.g., one hour of busking).\n\n\n\n\nCode\nn_days = 5\n# simulate whether it rains each day\nrng = np.random.default_rng(seed=42)\nrain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\ntotal_earnings = 0 # initialize a variable to keep track of total earnings\nfor day in range(n_days):\n    # For each day, decide if it rains based on the probability\n    did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n    print(f\"Day {day + 1} ({rain_probabilities[day]:.2%} chance): {'Rain' if did_it_rain else 'No rain'}\")\n    # Based on the outcome, the number of passersby changes\n    if did_it_rain:\n        passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n    else:\n        passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n    print(f\"\\t Number of passersby: {passersby}\")\n    # Simulate the number who stop to listen to the busker\n    listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n    print(f\"\\t Number of listeners: {listeners}\")\n    # Compute the busker's daily earnings\n    earnings = 3 * listeners  # $3 per listener\n    print(f\"\\t Daily earnings: ${earnings}\")\n    print(\"-\" * 40)\n\n    total_earnings += earnings\n\nprint(f\"Total earnings over {n_days} days: ${total_earnings}\")\n\n\nDay 1 (54.18% chance): No rain\n     Number of passersby: 211\n     Number of listeners: 41\n     Daily earnings: $123\n----------------------------------------\nDay 2 (30.72% chance): No rain\n     Number of passersby: 226\n     Number of listeners: 54\n     Daily earnings: $162\n----------------------------------------\nDay 3 (60.10% chance): Rain\n     Number of passersby: 51\n     Number of listeners: 13\n     Daily earnings: $39\n----------------------------------------\nDay 4 (48.82% chance): Rain\n     Number of passersby: 56\n     Number of listeners: 17\n     Daily earnings: $51\n----------------------------------------\nDay 5 (6.59% chance): No rain\n     Number of passersby: 212\n     Number of listeners: 34\n     Daily earnings: $102\n----------------------------------------\nTotal earnings over 5 days: $477\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe key here is to re-use the logic of the weekly busking simulation in a loop that runs 1000 times. Each time we run the simulation, we get a different weekly outcome based on the random number generator. By averaging these outcomes, we can get a good estimate of the expected earnings over a week of busking.\nimport numpy as np\ndef busk_one_week(rng, n_days=7):\n    \"\"\"Simulate the earnings of a busker in Rittenhouse Square over the course of a week.\n    Args:\n        rng: A NumPy random number generator.\n        n_days: The number of days to simulate (default is 7).\n    Returns:\n        total_earnings: The total earnings over the week.\n    \"\"\"\n    rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\n    total_earnings = 0 # initialize a variable to keep track of total earnings\n    for day in range(n_days):\n        # For each day, decide if it rains based on the probability\n        did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n        # Based on the outcome, the number of passersby changes\n        if did_it_rain:\n            passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n        else:\n            passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n        # Simulate the number who stop to listen to the busker\n        listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n        # Compute the busker's daily earnings\n        earnings = 3 * listeners  # $3 per listener\n\n        total_earnings += earnings\n\n    return total_earnings\n\nn_simulations = 1000\nrandom_seed = 33\n\nrng = np.random.default_rng(seed=random_seed)\n\n# Use the above function to simulate the expected earnings of a busker in Rittenhouse Square over the course of a week.\n# Run the simulation 1000 times and calculate the average earnings over 1000 simulations.\nearnings_by_week = [busk_one_week(rng) for _ in range(n_simulations)]\naverage_earnings = np.mean(earnings_by_week)\naverage_earnings",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#expectation-and-variance",
    "href": "notebooks/lecture-02.html#expectation-and-variance",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation and variance",
    "text": "Expectation and variance\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\E[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\E[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\nWhy does this definition make sense? Think about what would happen if we repeated the random process many times. Values that occur more frequently will have a larger impact on the average, while values that occur less frequently will have a smaller impact. The expectation captures this idea by weighting each value by its probability.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#expectation",
    "href": "notebooks/lecture-02.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play.\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $10 and win $100 ($90 net profit) on one game, but if you lose $10 on the next 9 games you’re not making money in the long run. Even though $90 profit sounds great, the fact that it happens so infrequently (and you lose $10 90% of the time) means that your average profit is actually zero.\n\n\n\n\n\n\nGambling warning\n\n\n\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n\n\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\E[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\E[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html",
    "href": "notebooks/lecture-04.html",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#data-generating-processes",
    "href": "notebooks/lecture-04.html#data-generating-processes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#statistical-models",
    "href": "notebooks/lecture-04.html#statistical-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\nLet’s return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a Bernoulli distribution, which describes the probability of each outcome. Specifically,\n\\[P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}\\]\nwhere (X) is the outcome of the coin flip, (p) is the probability of heads, and (1 - p) is the probability of tails. If we assume a fair coin, then (p = 0.5).",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#summary",
    "href": "notebooks/lecture-00.html#summary",
    "title": "Lecture 00",
    "section": "Summary",
    "text": "Summary\nIn this introductory lecture we talked about the 3 objectives of data analysis: description, inference, and prediction. Hopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. Don’t worry, we’ll get there!\nNext up, we’ll cover some of the basic programming concepts that are important for data science. After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. From there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learning any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  }
]