[
  {
    "objectID": "assignments/final-project.html",
    "href": "assignments/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project for our course is designed to allow you to explore a topic of your choice and apply the concepts we’ve learned throughout the course. You will have the opportunity to select a topic that interests you, conduct research and source relevant data, analyze the data, and present your findings at one of our final class sessions."
  },
  {
    "objectID": "assignments/final-project.html#footnotes",
    "href": "assignments/final-project.html#footnotes",
    "title": "Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is of course exceptionally difficult to do this in your second or third language. Don’t worry about it – you’re not being graded and all that matters is that you do your best to communicate your ideas.↩︎"
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03: Sampling and Simulation",
    "section": "",
    "text": "Someone plays roulette as follows:\n\nBefore each spin, they roll a die.\nThey bet on red, as many dollars as the number rolled on the die. (If the die shows 1, they bet $1; if it shows 6, they bet $6.)\nIf red comes up, they win back their bet plus the same amount again (so if they bet $3, they win $6). If black (or green) comes up, they lose their bet (so if they bet $3, they lose that $3).\n\nNote that the probability of red is 18/38, black is 18/38, and green is 2/38.\nWrite code to simulate this game and answer the following questions:\n\nWhat is the expected value of a single spin of the roulette wheel? Calculate this by hand and by simulation. Your answers should match. (Answer in terms of dollars.)\nWhat is the expected value of playing this game for 100 spins? (Answer in terms of dollars.)\nWhat is the expected number of spins it will take until red comes up? (Answer in terms of spins.)\nWhat is the probability that the player will be ahead after 100 spins? 1000 spins? (Answer in terms of probability.)"
  },
  {
    "objectID": "assignments/assignment-03.html#problem-1-gambling-randomly",
    "href": "assignments/assignment-03.html#problem-1-gambling-randomly",
    "title": "Assignment 03: Sampling and Simulation",
    "section": "",
    "text": "Someone plays roulette as follows:\n\nBefore each spin, they roll a die.\nThey bet on red, as many dollars as the number rolled on the die. (If the die shows 1, they bet $1; if it shows 6, they bet $6.)\nIf red comes up, they win back their bet plus the same amount again (so if they bet $3, they win $6). If black (or green) comes up, they lose their bet (so if they bet $3, they lose that $3).\n\nNote that the probability of red is 18/38, black is 18/38, and green is 2/38.\nWrite code to simulate this game and answer the following questions:\n\nWhat is the expected value of a single spin of the roulette wheel? Calculate this by hand and by simulation. Your answers should match. (Answer in terms of dollars.)\nWhat is the expected value of playing this game for 100 spins? (Answer in terms of dollars.)\nWhat is the expected number of spins it will take until red comes up? (Answer in terms of spins.)\nWhat is the probability that the player will be ahead after 100 spins? 1000 spins? (Answer in terms of probability.)"
  },
  {
    "objectID": "assignments/assignment-03.html#problem-2-strategic-gambling",
    "href": "assignments/assignment-03.html#problem-2-strategic-gambling",
    "title": "Assignment 03: Sampling and Simulation",
    "section": "Problem 2: Strategic gambling",
    "text": "Problem 2: Strategic gambling\nI’m trying to develop a gambling strategy for roulette. I know that the expected value of a single spin is negative, so I don’t expect to win in the long run. But hey – gambling can be fun, right? And it’s much more fun if I win more than if I lose.\nLet’s stick to the “50-50” bets (which are actually slightly less than 50-50 due to the green slots). These have the simplest odds.\nThe basic idea is to bet on red, but to double my bet every time I lose. So if I start with a $1 bet and lose, I bet $2 on the next spin. If I lose again, I bet $4. The idea is that when I finally win, I’ll win back all my previous losses plus a profit of $1 (my original bet). So unless I run out of money, I should eventually win. (This is called the Martingale strategy) If I win a bet, I go back to repeating my original bet of $1.\nOf course, this strategy has a flaw: at some point you very well might run out of money if you keep losing. And since you keep doubling your bet, the amount you lose can grow very quickly.\nLet’s set up some parameters for this strategy:\n\nLet’s assume you start with a fixed budget of \\(B\\) dollars. If you lose \\(B\\) dollars, you’re out of money and you can’t continue, so you end up finishing your night with a loss of \\(B\\) dollars.\nLet’s say you are happy to walk away with a profit of \\(P\\) dollars. If you win \\(P\\) dollars, you stop playing and walk away happy. (Remember that the odds are rigged - if you play long enough, you are destined to lose, so you have to walk away at some point.)\n\nWrite code to simulate this strategy (at least 5000 times) and answer the following questions:\n\nLet’s say you start with \\(B = 50\\) and \\(P = 50\\). What is the probability that you will walk away happy? (Answer in terms of probability.)\nWhat is the expected value of playing this strategy? (Answer in terms of dollars.)\nI told you I like to win more often. If you want to increase your chances of walking away happy, how should you adjust the values of \\(B\\) and \\(P\\)? (Answer in terms of a strategy, not specific numbers.) How does this affect the expected value of playing this strategy?\nLet’s say you want to increase your winnings, but your budget is fixed at \\(B = 100\\). Compute the probability of walking away happy for different values of \\(P\\) (e.g., \\(P = 10\\), \\(P = 20\\), \\(P = 50\\), \\(P = 100\\)). What do you notice about the relationship between \\(P\\) and the probability of walking away happy? Try to make a plot of the probability of walking away happy as a function of \\(P\\). (Look at the matplotlib documentation or seaborn documentation for examples of how to make plots, or ask AI for help with plotting.)"
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01: Programming",
    "section": "",
    "text": "AI Notice: I expect that this entire assignment can be readily solved by today’s AI systems. I would encourage you to attempt the assignment yourself first, but if you find yourself stuck, you can use AI tools to help you. I have to emphasize that if you do so, it is crucial that you take the time to understand the AI’s solution and not just copy it. The goal is to learn and practice the concepts, not just to complete the assignment. If you don’t need AI to solve the assignment – great! It can still help – ask it to review your solution, critique it, or suggest improvements. This will help you learn even more about how to write good code."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-1-robust-addition",
    "href": "assignments/assignment-01.html#problem-1-robust-addition",
    "title": "Assignment 01: Programming",
    "section": "Problem 1: Robust addition",
    "text": "Problem 1: Robust addition\nWrite a function that takes in a list of numbers and returns their sum. However, if the list contains any non-numeric values, the function should ignore those values and only sum the numeric ones. The exception is if the non-numeric values can be converted to numeric types (like strings representing numbers), in which case they should be converted and included in the sum. If the list is empty or contains no numeric values, the function should return 0.\n\ndef robust_addition(numbers : list) -&gt; float:\n    # TODO: Implement the function to sum only numeric values in the list.\n    pass"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-2",
    "href": "assignments/assignment-01.html#problem-2",
    "title": "Assignment 01: Programming",
    "section": "Problem 2:",
    "text": "Problem 2:\nYou have a 1D NumPy array of student scores (0–100).\n\nWrite a function pass_rate(scores, threshold) that returns the fraction of scores ≥ threshold.\nWrite a function top_percentile(scores, p) that returns the score value at the p-th percentile (e.g. p=0.9 ⇒ 90th percentile).\n\nConstraints:\n\nUse no Python loops or list-comprehensions.\nLeverage NumPy’s casting behavior (boolean \\(\\to\\) integer).\n\n\nimport numpy as np\nfrom typing import Union\n\nArray1D = Union[np.ndarray, list[float]]\n\ndef pass_rate(scores: Array1D, threshold: float) -&gt; float:\n    \"\"\"\n    Return the proportion of entries in `scores` &gt;= threshold.\n    \"\"\"\n    arr = np.asarray(scores)\n    # TODO: compute proportion without loops\n    ...\n\ndef top_percentile(scores: Array1D, p: float) -&gt; float:\n    \"\"\"\n    Return the p-th percentile of `scores`, where 0 &lt; p &lt; 1.\n    \"\"\"\n    arr = np.asarray(scores)\n    # TODO: use a single np function\n    ..."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-3-palindrome-checker",
    "href": "assignments/assignment-01.html#problem-3-palindrome-checker",
    "title": "Assignment 01: Programming",
    "section": "Problem 3: Palindrome Checker",
    "text": "Problem 3: Palindrome Checker\nWrite a function that checks whether a given string is a palindrome (a word, phrase, number, or other sequence of characters that reads the same forward and backward, ignoring spaces, punctuation, and capitalization). The function should return True if the string is a palindrome and False otherwise.\n\ndef is_palindrome(s: str) -&gt; bool:\n    # TODO: Implement the function to check if the string is a palindrome.\n    # Ignore spaces, punctuation, and capitalization.\n    pass"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-4-multiple-data-tables-and-pandas-operations",
    "href": "assignments/assignment-01.html#problem-4-multiple-data-tables-and-pandas-operations",
    "title": "Assignment 01: Programming",
    "section": "Problem 4: Multiple data tables and Pandas operations",
    "text": "Problem 4: Multiple data tables and Pandas operations\nWe did not cover how to do all this in class, but here are some exercises to practice merging and querying data with Pandas. Please refer to the Pandas documentation, the links below, and feel free to ask AI assistants to point you to the right functions and methods.\nSome related reading: - Blog post on relational databases - Joins in SQL / relational databases - Joins in Pandas\nBelow we define the data for a set of tables related to car registrations and traffic violations. You should be able to use the Pandas library to perform various queries on this data. Try the following exercises:\n\nShow all violations for “Alice Smith” (D001).\nHint: Join drivers → registrations → violations and filter where dl_number == \"D001\".\nFind violations on cars older than 2018.\nHint: Join cars → violations and filter where year &lt; 2018.\nWhich plates have zero recorded violations?\nHint: Anti-join registrations (or cars) against violations.\nCount total violations per driver (include drivers with none).\nHint: Merge all tables, then .groupby(\"dl_number\") + .size() (or .count()).\nCompute average fine per driver, sorted descending.\nHint: Group by dl_number and use .mean() on the fine column.\nFor each car make (e.g. Ford, Toyota), what is the total number of violations?\nHint: Join cars → violations → group by make and use .size().\nWhat’s the average fine by vehicle year?\nHint: Merge cars → violations, group by year, and use .mean() on fine.\n\n\nimport pandas as pd\nfrom pandas import DataFrame\n\n# — Driver info — (10 drivers)\ndrivers = pd.DataFrame([\n    {\"dl_number\": \"D001\", \"name\": \"Alice Smith\",   \"age\": 34},\n    {\"dl_number\": \"D002\", \"name\": \"Bob Jones\",     \"age\": 28},\n    {\"dl_number\": \"D003\", \"name\": \"Carol Diaz\",    \"age\": 45},\n    {\"dl_number\": \"D004\", \"name\": \"David Lee\",     \"age\": 52},\n    {\"dl_number\": \"D005\", \"name\": \"Eva Chen\",      \"age\": 23},\n    {\"dl_number\": \"D006\", \"name\": \"Frank Moore\",   \"age\": 36},\n    {\"dl_number\": \"D007\", \"name\": \"Grace Patel\",   \"age\": 41},\n    {\"dl_number\": \"D008\", \"name\": \"Henry Zhao\",    \"age\": 29},\n    {\"dl_number\": \"D009\", \"name\": \"Ivy Nguyen\",    \"age\": 50},\n    {\"dl_number\": \"D010\", \"name\": \"Jack O'Connor\", \"age\": 31},\n])\n\n# — Car registrations (DL → plate) — (15 registrations)\nregistrations = pd.DataFrame([\n    {\"dl_number\": \"D001\", \"plate\": \"ABC-123\"},\n    {\"dl_number\": \"D001\", \"plate\": \"XYZ-999\"},\n    {\"dl_number\": \"D002\", \"plate\": \"JKL-456\"},\n    {\"dl_number\": \"D003\", \"plate\": \"MNO-321\"},\n    {\"dl_number\": \"D004\", \"plate\": \"PQR-654\"},\n    {\"dl_number\": \"D005\", \"plate\": \"STU-111\"},\n    {\"dl_number\": \"D005\", \"plate\": \"VWX-222\"},\n    {\"dl_number\": \"D006\", \"plate\": \"YZA-333\"},\n    {\"dl_number\": \"D007\", \"plate\": \"BCD-444\"},\n    {\"dl_number\": \"D007\", \"plate\": \"EFG-555\"},\n    {\"dl_number\": \"D008\", \"plate\": \"HIJ-666\"},\n    {\"dl_number\": \"D009\", \"plate\": \"KLM-777\"},\n    {\"dl_number\": \"D009\", \"plate\": \"NOP-888\"},\n    {\"dl_number\": \"D010\", \"plate\": \"QRS-999\"},\n])\n\n# — Car info (plate → make/model/year) — (14 cars)\ncars = pd.DataFrame([\n    {\"plate\": \"ABC-123\", \"make\": \"Toyota\",  \"model\": \"Camry\",   \"year\": 2018},\n    {\"plate\": \"XYZ-999\", \"make\": \"Honda\",   \"model\": \"Civic\",   \"year\": 2020},\n    {\"plate\": \"JKL-456\", \"make\": \"Ford\",    \"model\": \"Escape\",  \"year\": 2019},\n    {\"plate\": \"MNO-321\", \"make\": \"Tesla\",   \"model\": \"Model 3\", \"year\": 2021},\n    {\"plate\": \"PQR-654\", \"make\": \"Nissan\",  \"model\": \"Altima\",  \"year\": 2017},\n    {\"plate\": \"STU-111\", \"make\": \"Chevy\",   \"model\": \"Malibu\",  \"year\": 2016},\n    {\"plate\": \"VWX-222\", \"make\": \"Kia\",     \"model\": \"Soul\",    \"year\": 2022},\n    {\"plate\": \"YZA-333\", \"make\": \"BMW\",     \"model\": \"X3\",      \"year\": 2015},\n    {\"plate\": \"BCD-444\", \"make\": \"Audi\",    \"model\": \"A4\",      \"year\": 2019},\n    {\"plate\": \"EFG-555\", \"make\": \"Hyundai\", \"model\": \"Elantra\", \"year\": 2020},\n    {\"plate\": \"HIJ-666\", \"make\": \"Subaru\",  \"model\": \"Outback\", \"year\": 2018},\n    {\"plate\": \"KLM-777\", \"make\": \"Ford\",    \"model\": \"Focus\",   \"year\": 2017},\n    {\"plate\": \"NOP-888\", \"make\": \"Mazda\",   \"model\": \"CX-5\",    \"year\": 2021},\n    {\"plate\": \"QRS-999\", \"make\": \"Chevy\",   \"model\": \"Impala\",  \"year\": 2014},\n])\n\n# — Traffic violations (plate → violation) — (20 entries)\nviolations = pd.DataFrame([\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-01\", \"type\": \"speeding\",   \"fine\": 150},\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-07\", \"type\": \"red_light\",  \"fine\": 200},\n    {\"plate\": \"XYZ-999\", \"date\": \"2025-06-11\", \"type\": \"seatbelt\",  \"fine\":  75},\n    {\"plate\": \"JKL-456\", \"date\": \"2025-06-05\", \"type\": \"parking\",    \"fine\":  40},\n    {\"plate\": \"MNO-321\", \"date\": \"2025-06-09\", \"type\": \"speeding\",   \"fine\": 120},\n    {\"plate\": \"PQR-654\", \"date\": \"2025-06-10\", \"type\": \"dui\",         \"fine\": 500},\n    {\"plate\": \"STU-111\", \"date\": \"2025-06-12\", \"type\": \"speeding\",   \"fine\": 130},\n    {\"plate\": \"VWX-222\", \"date\": \"2025-06-14\", \"type\": \"parking\",    \"fine\":  55},\n    {\"plate\": \"YZA-333\", \"date\": \"2025-06-15\", \"type\": \"red_light\",  \"fine\": 250},\n    {\"plate\": \"BCD-444\", \"date\": \"2025-06-16\", \"type\": \"speeding\",   \"fine\": 160},\n    {\"plate\": \"EFG-555\", \"date\": \"2025-06-17\", \"type\": \"parking\",    \"fine\":  35},\n    {\"plate\": \"HIJ-666\", \"date\": \"2025-06-18\", \"type\": \"seatbelt\",  \"fine\":  80},\n    {\"plate\": \"KLM-777\", \"date\": \"2025-06-19\", \"type\": \"speeding\",   \"fine\": 140},\n    {\"plate\": \"NOP-888\", \"date\": \"2025-06-20\", \"type\": \"dui\",         \"fine\": 600},\n    {\"plate\": \"QRS-999\", \"date\": \"2025-06-21\", \"type\": \"parking\",    \"fine\":  45},\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-22\", \"type\": \"speeding\",   \"fine\": 155},\n    {\"plate\": \"ZZZ-000\", \"date\": \"2025-06-12\", \"type\": \"speeding\",   \"fine\": 130}, \n    {\"plate\": \"UNKNOWN\", \"date\": \"2025-06-23\", \"type\": \"red_light\",  \"fine\": 220}, \n    {\"plate\": \"NOP-888\", \"date\": \"2025-06-24\", \"type\": \"seatbelt\",  \"fine\":  85},\n    {\"plate\": \"STU-111\", \"date\": \"2025-06-25\", \"type\": \"parking\",    \"fine\":  50},\n])"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-5-list-comprehension-and-lambda-functions",
    "href": "assignments/assignment-01.html#problem-5-list-comprehension-and-lambda-functions",
    "title": "Assignment 01: Programming",
    "section": "Problem 5: List Comprehension and Lambda Functions",
    "text": "Problem 5: List Comprehension and Lambda Functions\nRead about two advanced Python features we did not cover in class: - List comprehensions - Lambda functions\nThey are very useful for writing concise code, since they allow you to create lists and functions in a single line.\n\nWrite a list comprehension that builds a staircase pattern of the word “Staircases” with 10 steps, letter by letter.\n\nExample output:\n\n['S',\n 'St',\n 'Sta',\n 'Stai',\n 'Stair',\n 'Stairc',\n 'Stairca',\n 'Staircas',\n 'Staircase',\n 'Staircases']\nWrite a lambda function that takes a list of numbers and returns a new list containing only the numbers from the original list that are divisible by 3.\nWrite a lambda function that takes a string and returns the string reversed. (e.g. “shalom” becomes “molahs”). Then, apply this function to the column of names in the drivers table to create a new column called reversed_name. (You can use the apply method on a Pandas DataFrame to apply a function to each element in a column.)"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-6-object-oriented-programming-oop-with-inheritance",
    "href": "assignments/assignment-01.html#problem-6-object-oriented-programming-oop-with-inheritance",
    "title": "Assignment 01: Programming",
    "section": "Problem 6: Object-Oriented Programming (OOP) with Inheritance",
    "text": "Problem 6: Object-Oriented Programming (OOP) with Inheritance\nObjective\nPractice defining base classes, subclasses, method overriding, etc.\nBuild a mini sports‐league system. You’ll model TeamMember, Player, Coach, and Team, then extend with sport‐specific players.\nComplete the class definitions below:\n\nfrom typing import List\n\nclass TeamMember:\n    def __init__(self, name: str, age: int) -&gt; None:\n        self.name = name\n        self.age = age\n\n    def role(self) -&gt; str:\n        \"\"\"Return the member’s role name.\"\"\"\n        ...\n\nclass Player(TeamMember):\n    def __init__(self, name: str, age: int, position: str, number: int) -&gt; None:\n        super().__init__(name, age)\n        self.position = position\n        self.number = number\n\n    def role(self) -&gt; str:\n        ...\n\n    def play(self) -&gt; str:\n        \"\"\"Return a generic “is playing” message.\"\"\"\n        ...\n\nclass Coach(TeamMember):\n    def __init__(self, name: str, age: int, experience_years: int) -&gt; None:\n        super().__init__(name, age)\n        self.experience_years = experience_years\n\n    def role(self) -&gt; str:\n        ...\n\n    def train(self, player: Player) -&gt; str:\n        \"\"\"Return a training message.\"\"\"\n        ...\n\nclass Team:\n    def __init__(self, name: str) -&gt; None:\n        self.name = name\n        self.members: List[TeamMember] = []\n\n    def add_member(self, member: TeamMember) -&gt; None:\n        ...\n\n    def lineup(self) -&gt; List[str]:\n        \"\"\"List ‘Name – Role’ for each member.\"\"\"\n        ...\n\n    def game_day(self) -&gt; None:\n        \"\"\"\n        For each Player, call play();\n        for each Coach, call train() on every Player.\n        Print each message.\n        \"\"\"\n        ...\n\nOnce the methods are all defined, you can create instances of these classes and test the functionality.\nInstantiate two teams, “HaPoel” and “Maccabi”. Try adding players and coaches to each team, and then print out the team lineups with .lineup() and simulate a gameday preparation with .game_day().\nNext: Define two subclasses of Player: BasketballPlayer and FootballPlayer (European football, not American). Each subclass should have a unique play() method that prints a sport-specific message. For example, BasketballPlayer might print “Nailing a three-pointer!” while FootballPlayer might print “Goooooooooal!”."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-7-functional-vs.-object-oriented-programming-for-team-statistics",
    "href": "assignments/assignment-01.html#problem-7-functional-vs.-object-oriented-programming-for-team-statistics",
    "title": "Assignment 01: Programming",
    "section": "Problem 7: Functional vs. Object-Oriented Programming for team statistics",
    "text": "Problem 7: Functional vs. Object-Oriented Programming for team statistics\nThis exervise is designed to emphasize the differences between functional and object-oriented programming paradigms.\nWe’ll define a dataset with recorded game statistics for a team of players, and then implement two different approaches to compute the team’s statistical leaders: one using functional programming and the other using object-oriented programming.\n\ndata = pd.DataFrame({\n    \"game_id\": [\"001\", \"001\", \"002\", \"002\", \"002\"],\n    \"team\": [\"HaPoel\", \"Maccabi\", \"HaPoel\", \"Maccabi\", \"HaPoel\"],\n    \"player\": [\"Messi\", \"Ronaldo\", \"Messi\", \"Wagner\", \"Neymar\"],\n    \"goals\": [1, 2, 1, 0, 1],\n    \"assists\": [0, 1, 0, 2, 1],\n    \"minutes_played\": [90, 90, 90, 90, 90]\n})\n\n# compute the scoring leader for a given team\ndef compute_scoring_leader(data, team: str) -&gt; str:\n    ...\n    \n# compute the scoring leader for a given team using OOP\nclass TeamStats:\n    def __init__(self, data: pd.DataFrame, team: str) -&gt; None:\n        ...\n    \n    def scoring_leader(self) -&gt; str:\n        ..."
  },
  {
    "objectID": "notebooks/lecture-10.html",
    "href": "notebooks/lecture-10.html",
    "title": "Lecture 10: ANOVA",
    "section": "",
    "text": "We want to emphasize that nearly any statistical method with a closed-form solution can be replicated with a simulation.\nTake ANOVA, or analysis of variance, for example. The ANOVA test is used to determine whether there are any statistically significant differences between the means of two or more groups. It is a parametric test that assumes the data is normally distributed and that the variances of the groups are equal.\nThe exact test we compute is the F-test, which compares the variance between groups to the variance within groups. The null hypothesis is that all group means are equal, while the alternative hypothesis is that at least one group mean is different. The F-test relies on the fact that the ratio of two independent chi-squared variables (i.e. a normal variable, squared) follows an F-distribution.\nHere we will simulate the F-test by generating random data from a normal distribution and computing the F-statistic for different group means. We will see that under the null hypothesis (group means are equal), the F-statistic follows an F-distribution with degrees of freedom equal to the number of groups minus one and the total number of observations minus the number of groups.\nIn the next lecture we will compare the F-test to a non-parametric permutation test, which does not make any assumptions about the distribution of the data."
  },
  {
    "objectID": "notebooks/lecture-08.html",
    "href": "notebooks/lecture-08.html",
    "title": "Lecture 08: Linear Regression",
    "section": "",
    "text": "Now that we’ve talked about inference in depth, it is time to talk about prediction. In this lecture, we will build up to the basics of linear regression, which is a powerful tool for making predictions based on data.",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#predictions-from-patterns",
    "href": "notebooks/lecture-08.html#predictions-from-patterns",
    "title": "Lecture 08: Linear Regression",
    "section": "Predictions from patterns",
    "text": "Predictions from patterns\nHow do we make predictions? We typically look for patterns in the data and use those patterns to make informed guesses about future outcomes.\n\n\n\n\n\n\nWhy not memorize?\n\n\n\nWhy not just memorize the data? Think about a dataset like this:\n\n\n\nName\nApples / Day\n\n\n\n\nAlice\n3\n\n\nBob\n5\n\n\nCharlie\n2\n\n\n\nIf you were asked to predict how many apples a new person would eat per day, you could memorize the data and say “Alice eats 3 apples, Bob eats 5 apples, Charlie eats 2 apples.” You would be 100% correct on this dataset! But what if you were asked to predict how many apples a new person would eat? You would have no idea, because your memorized list only contains the data you have seen already. You have no way to make a prediction about a new person like Dave:\n\n\n\nName\nApples / Day\n\n\n\n\nDave\n?\n\n\n\nSo, we need a better strategy!\nMore broadly, this is called generalization: the ability to make predictions about new data based on patterns learned from existing data.\n\n\nConsider the following dataset of the heights (in inches) of a sample of adults. The dataset also records the height of their parents, so we’ll refer to the adults as “children” and their parents as “parents” in this context. 1\n\n\nCode\ngalton = pd.read_csv(\"../data/galton-stata11.tsv\", sep=\"\\t\", dtype={\"male\": bool, \"female\": bool, \"gender\": \"category\", \"family\": \"category\"})\ngalton.head() # first few rows of the dataset\n\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\ngender\nheight\nkids\nmale\nfemale\n\n\n\n\n0\n1\n78.5\n67.0\nM\n73.2\n4\nTrue\nFalse\n\n\n1\n1\n78.5\n67.0\nF\n69.2\n4\nFalse\nTrue\n\n\n2\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n3\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n4\n2\n75.5\n66.5\nM\n73.5\n4\nTrue\nFalse\n\n\n\n\n\n\n\nLet’s say I asked you to predict the height of another child not in the dataset. Assume they’re from the same population as the children in the dataset. You have no other information about the child. How would you do it?\nThe most straightforward thing to do is ask yourself: “What is the typical height of a child in this dataset?” You could then use that typical height as your prediction. This is the simplest kind of pattern recognition: finding the average value in the dataset and using it as a prediction.\n\n\nCode\n# print the mean height of children in the dataset\nmean_height = galton[\"height\"].mean()\nprint(f\"The mean height of children in the Galton dataset is {mean_height:.2f} inches.\")\n\n# plot the distribution of heights\nplt.figure(figsize=(8, 4))\nsns.histplot(galton[\"height\"], stat=\"proportion\", bins=30)\nplt.xlabel(\"Height (inches)\")\nplt.ylabel(\"Proportion of children\")\nplt.title(\"Distribution of Heights of Children\")\nplt.axvline(mean_height, color=\"red\", linestyle=\"--\", label=\"Mean Height\")\nplt.legend()\nplt.show()\n\n\nThe mean height of children in the Galton dataset is 66.76 inches.\n\n\n\n\n\n\n\n\n\nThis isn’t bad as a first guess, and you actually can’t do any better than this if you have no other information about the child.\nBut what if you had more information? For example, say you had other information about the child (besides their own height), such as their sex (male / female) or the heights of their parents. If you think that these factors might be related to the child’s height, they should inform your prediction.\nLet’s start by just plotting the heights of the children in the dataset against the heights of their parents. This will help us visualize the relationship between the two variables.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=True, sharex=True)\n# plot the heights of children against the heights of their fathers\nsns.scatterplot(data=galton, x=\"father\", y=\"height\", ax=ax[0], alpha=0.5)\nax[0].set_xlabel(\"Father's Height (inches)\")\nax[0].set_ylabel(\"Child's Height (inches)\")\nax[0].set_title(\"Child's Height vs Father's Height\")\nax[0].axvline(galton[\"father\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean Father's Height\")\nax[0].axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nax[0].legend(loc='lower right')\n\n# plot the heights of children against the heights of their mothers\nsns.scatterplot(data=galton, x=\"mother\", y=\"height\", ax=ax[1], alpha=0.5)\nax[1].set_xlabel(\"Mother's Height (inches)\")\nax[1].set_ylabel(\"Child's Height (inches)\")\nax[1].set_title(\"Child's Height vs Mother's Height\")\nax[1].axvline(galton[\"mother\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean Mother's Height\")\nax[1].axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nax[1].legend(loc='lower right')\n\n\n\n\n\n\n\n\n\nNotice how the naive prediction we made earlier (the average height of the children), shown as a blue dashed line, totally ignores the heights of the parents. It’s constant across the entire plot, and does not capture any of the variation in the data.\nYou may, based on your own experience and understanding of the world, expect that children with taller parents tend to be taller themselves. It sure looks that way in the plot above – the taller children tend to have taller parents, and the shorter children tend to have shorter parents. It’s not always the case, by any means, but it seems to be a general trend.\nBut how much taller? We want to quantify this relationship, so that we can get a formula for predicting a child’s height based on their parents’ heights. Mathematically, we want to define a function \\(f\\) that takes the heights of the parents as input and returns the predicted height of the child as output.",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#preparing-the-data",
    "href": "notebooks/lecture-08.html#preparing-the-data",
    "title": "Lecture 08: Linear Regression",
    "section": "Preparing the data",
    "text": "Preparing the data\nDealing with two separate parents is cumbersome. Can we combine the heights of the parents into a single number that represents “the height of the parents”? One way to do this is to take the average of the two heights.\nThis is problematic at the moment. Based on the plot it seems that the father’s height has a different relationship with the child’s height than the mother’s height does. The trends are similar, but the plots have different shapes. This is probably because men and women tend to have different average heights (as you can see in the plot above). Also because men are taller on average, they contribute more to the average height of the parents than women do.\nThere is a useful trick we can use to simplify this. We can put the heights of men and women in the dataset on the same scale by standardizing them.\n\\[\n\\begin{align*}\n\\text{standardized female height} = \\frac{\\text{height} - \\text{average of female heights}}{\\text{standard deviation of female heights}} \\\\\n\\text{standardized male height} = \\frac{\\text{height} - \\text{average of male heights}}{\\text{standard deviation of male heights}} \\\\\n\\end{align*}\n\\]\nThis procedure transforms all of the heights into a common scale, where the average is zero and the standard deviation is one. This process is called standardization or z-scoring.\nLet’s do this and plot the standardized heights of the parents against the heights of the children.\n\n\nCode\n# include all female and male heights (parents and children)\nmother_heights = galton[[\"mother\", \"family\"]].drop_duplicates()[\"mother\"].values\nfemale_child_heights = galton[galton[\"gender\"] == \"F\"][\"height\"].values\nfemale_heights = np.concatenate([mother_heights, female_child_heights])\n\nfather_heights = galton[[\"father\", \"family\"]].drop_duplicates()[\"father\"].values\nmale_child_heights = galton[galton[\"gender\"] == \"M\"][\"height\"].values\nmale_heights = np.concatenate([father_heights, male_child_heights])\n\n# calculate the mean and standard deviation of the heights of mothers and fathers\nfemale_mean = female_heights.mean()\nfemale_std = female_heights.std(ddof=1)\nmale_mean = male_heights.mean()\nmale_std = male_heights.std(ddof=1)\n\n# transform the heights to z-scores\ngalton[\"mother_z\"] = (galton[\"mother\"] - female_mean) / female_std\ngalton[\"father_z\"] = (galton[\"father\"] - male_mean) / male_std\ngalton.loc[galton[\"gender\"] == \"F\", \"height_z\"] = (galton.loc[galton[\"gender\"] == \"F\", \"height\"] - female_mean) / female_std\ngalton.loc[galton[\"gender\"] == \"M\", \"height_z\"] = (galton.loc[galton[\"gender\"] == \"M\", \"height\"] - male_mean) / male_std\n\n# plot the z-scores of the heights of children against the z-scores of their fathers\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=True, sharex=True)\nsns.scatterplot(data=galton, x=\"father_z\", y=\"height_z\", ax=ax[0], alpha=0.5)\nax[0].set_xlabel(\"Father's Height (z-score)\")\nax[0].set_ylabel(\"Child's Height (z-score)\")\nax[0].axvline(0, color=\"red\", linestyle=\"--\")\nax[0].axhline(0, color=\"blue\", linestyle=\"--\")\n\n# plot the z-scores of the heights of children against the z-scores of their mothers\nsns.scatterplot(data=galton, x=\"mother_z\", y=\"height_z\", ax=ax[1], alpha=0.5)\nax[1].set_xlabel(\"Mother's Height (z-score)\")\nax[1].set_ylabel(\"Child's Height (z-score)\")\nax[1].axvline(0, color=\"red\", linestyle=\"--\")\nax[1].axhline(0, color=\"blue\", linestyle=\"--\")  \n\n\n\n\n\n\n\n\n\nNow we have put the heights of the parents on the same scale, so we can combine them into a single number that represents “the height of the parents”. We can do this by taking the average of the standardized heights of the parents.\n\n\nCode\ngalton[\"midparent\"] = (galton[\"mother_z\"] + galton[\"father_z\"]) / 2\n\n# plot the z-scores of the heights of children against the midparent z-scores\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height_z\", alpha=0.5)\nplt.xlabel(\"Midparent Height (z-score)\")\nplt.ylabel(\"Child's Height (z-score)\")\nplt.axvline(0, color=\"red\", linestyle=\"--\")\nplt.axhline(0, color=\"blue\", linestyle=\"--\")\nplt.title(\"Child's Height vs Midparent Height (z-score)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNice! It seems like this retains the relationship between the heights of the parents and the heights of the children, but now we have a single number that represents the height of the parents.\nLet’s go back to the units we care about predicting, though. It’s ok to use standardized heights for the parents, but we want to predict the height of the child in inches.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height\", alpha=0.5)\nplt.xlabel(\"Midparent Height (Z-score)\")\nplt.ylabel(\"Child's Height (inches)\")\nplt.axvline(0, color=\"red\", linestyle=\"--\")\nplt.axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nplt.title(\"Child's Height vs Midparent Height (Z-score)\")\n\n\nText(0.5, 1.0, \"Child's Height vs Midparent Height (Z-score)\")",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#quantifying-relationships",
    "href": "notebooks/lecture-08.html#quantifying-relationships",
    "title": "Lecture 08: Linear Regression",
    "section": "Quantifying relationships",
    "text": "Quantifying relationships\nBack to our question: how much taller are the children of tall parents?\nThe simplest way to quantify this relationship is to use a linear function, which is a function of the form \\(f(x) = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept. A linear function has a constant rate of change, which means that for every unit increase in the input, the output increases by a fixed amount. Fitting a linear function to data is called linear regression.\nIn our case, we choose to model the relationship between the heights of the parents and the heights of the children as a linear function.\n\\[\\text{predicted height of child} = \\text{slope} \\times \\text{midparent height} + \\text{intercept}\\]\nNote that our first model, the average height of the children, is a special case of this linear function where the slope is zero and the intercept is the average height of the children.\nWhat should the slope be? Basically, we want it to be whatever value makes the predicted heights of the children as close to the actual heights of the children as possible.\nWe can use a statistical technique called ordinary least squares (OLS) to find the best-fitting line for our data. OLS minimizes the (squared) distance between the predicted heights of the children and the actual heights of the children.\n\n\n\n\n\n\nWhy squared?\n\n\n\n\n\nOLS minimizes \\((\\text{predicted height} - \\text{actual height})^2\\). Why squared? There are two main reasons: 1. Squaring the differences ensures that we don’t have negative values canceling out positive values 2. When you square a number, it becomes larger as the number gets larger. This means that larger errors are penalized more heavily than smaller errors. So you’d rather be off by 1 inch 10 times (squared error of 10) than be off by 10 inches once (squared error of 100). This helps us find a line that is close to all of the points and avoids being too far off on any one point.\nAdvanced note: If you like calculus, the other benefit of squared errors is that they are differentiable (think about a parabola \\(y = x^2\\) - the slope is always changing, but it never has a sharp corner). This is a big difference compared to absolute errors, which have a sharp corner at zero (the slope of \\(|x|\\) is not defined at zero, where it changes from -1 to 1).\n\n\n\nLet’s fit a linear regression model to the data and see what the best-fitting line looks like.\n\n\n\n\n\n\nSpecifying regression models\n\n\n\n\n\nStatistical software packages have built-in functions for fitting many common models to data. There are a variety of packages that do pretty much the same thing, and they all have their own syntax for specifying the model you want to fit.\nSome packages accept a formula as input, which specifies the relationship between the variables. The “formula” is a string that describes the model you want to fit, and has its own syntax. In this case, we want to predict the height of the child based on the midparent height, so we use the formula height ~ midparent. The ~ symbol separates the outcome variable (the variable we want to predict) from the input variable (the variable we are using to make the prediction). The + symbol can be used to add more input variables to the model, but in this case we only have one input variable. The intercept is included by default, so we don’t need to specify it explicitly.\n\n\n\n\n\nCode\n# fit OLS and draw the prediction errors\nfrom statsmodels.formula.api import ols\n# fit a linear regression model to the data\nmodel = ols(\"height ~ midparent\", data=galton).fit()\n# print the summary of the model\npredicted_heights = model.predict(galton[\"midparent\"]).values\nerrors = galton[\"height\"] - predicted_heights\n\nselect_idx = galton[\"family\"].drop_duplicates().sample(20).index.values # avoid overplotting a few non-overlapping families\nremaining_idx = galton.index.difference(select_idx).values\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height\", ax=ax[0])\nsns.lineplot(x=galton[\"midparent\"], y=predicted_heights, color=\"k\", label=\"Predicted Height\", ax=ax[0])\n\nsns.scatterplot(data=galton.iloc[select_idx], x=\"midparent\", y=\"height\", alpha=1., ax=ax[1])\nsns.scatterplot(data=galton.iloc[remaining_idx], x=\"midparent\", y=\"height\", alpha=0.15, color=\"gray\", ax=ax[1])\nplt.xlabel(\"Midparent Height (Z-score)\")\nplt.ylabel(\"Child's Height (inches)\")\nsns.lineplot(x=galton[\"midparent\"], y=predicted_heights, color=\"k\", label=\"Predicted Height\", ax=ax[1])\nfor i in select_idx:\n    # draw a line from the predicted height to the actual height\n    ax[1].plot([galton[\"midparent\"].iloc[i], galton[\"midparent\"].iloc[i]], [predicted_heights[i], galton[\"height\"].iloc[i]], color=\"red\", linestyle=\"--\", alpha=.8)\n\n\n\n\n\n\n\n\n\nIn the second plot, we show the prediction errors for a few datapoints. The red lines show the difference between the predicted height of the child and the actual height of the child. OLS finds the best fit that minimizes the sum of the squared lengths of these red lines.\nSo what’s the slope that answers our question? We can extract the parameters of the model to find out. The slope is the parameter associated with the midparent height variable.\n\n\nCode\nmodel.params\n\n\nIntercept    66.765946\nmidparent     1.656486\ndtype: float64\n\n\nSo what exactly can we say?\nThe slope of a line \\(y = mx + b\\) is the change in \\(y\\) for a one-unit increase in \\(x\\). So, in our case, the slope is the change in the predicted height of the child for a one-unit increase in the midparent height.\nSo, for every one unit increase in the midparent height, the predicted height of the child increases by about 1.7 inches.\nThis probably seems confusing because the midparent heights are standardized and averaged. It means that if the parents’ heights are on average one standard deviation above the mean height, the predicted height of the child is 1.7 inches taller than the average height of the children in the dataset.\nSo how did we do in terms of prediction?\nLet’s take a look at the distribution of the errors.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.histplot(x=errors, alpha=0.5, stat=\"probability\", ax=ax)\nax.set_xlabel(\"Prediction Error (inches)\")\nax.axhline(0, color=\"gray\", linestyle=\"--\")\nax.axvline(0, color=\"gray\", linestyle=\"--\")\nax.set_title(\"Prediction Error vs Midparent Height (Z-score)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe often evaluate our predictions by studying the mean and standard deviation of the prediction errors. We want the mean to be close to zero, which means that our predictions are on average correct. You don’t want to be consistently over- or under-predicting. The standard deviation tells us how much the prediction errors vary.\n\n\nCode\nprint(f\"Mean prediction error: {np.mean(errors):.2e} \\nStandard deviation of prediction error: {np.std(errors):.2f}\")\n\n\nMean prediction error: -2.43e-14 \nStandard deviation of prediction error: 3.39\n\n\nSpecifically, the standard deviation is called the “Root Mean Squared Error” (RMSE), and it tells us how far off our predictions are from the actual values on average: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted height}_i - \\text{actual height}_i)^2}\\] So, in our case the RMSE is about 3.4. This means that on average, our predictions are off by about 3.4 inches.\nLet’s compare this to the RMSE of the naive prediction we made earlier (the average height of the children).\n\n\nCode\nnaive_rmse = np.sqrt(np.mean((galton[\"height\"] - mean_height) ** 2))\nprint(f\"Naive RMSE: {naive_rmse:.2f} inches\")\n\n\nNaive RMSE: 3.58 inches\n\n\nNotice anything else interesting about the residual distribution? It’s centered around zero, and most of the residuals close to that mean – it’s approximately normal!\nWe skipped over this important detail before, but that’s actually a key assumption of the linear regression model. The residuals (or you can think of them as “noise” – variation in the data that we can’t explain with our model) should be normally distributed.\nThis highlights the connection between linear regression and the kind of probabilistic statistical models we have been discussing in this course all along. The regression model treats the variable we are predicting (the height of the child) as a random variable itself. It has an expected value that depends in a linear way on the input variable, but it also has some inherent variability.\nSo the linear regression model predictions can be written as: \\[y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\sigma^2\\) is the variance of the residuals.\nThe consequence of this variability is that, just like our previous models, our data-driven conclusions are subject to uncertainty from sampling. We might just happen to get a sample where the children of tall parents are shorter than average, and get a slope that is negative.\nThis is why we need to be careful about interpreting the results of our regression model. We’ll return to the hypothesis testing framework in the next lecture, and show how it can be applied to regression models.",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#correlation-is-a-special-case-of-regression",
    "href": "notebooks/lecture-08.html#correlation-is-a-special-case-of-regression",
    "title": "Lecture 08: Linear Regression",
    "section": "Correlation is a special case of regression",
    "text": "Correlation is a special case of regression\nYou have probably heard of the term “correlation” before. Correlation is a measure of the strength and direction of the linear relationship between two variables. If two things are correlated, it means that when one variable increases, the other variable tends to increase (or decrease) as well.\nThe correlation coefficient (\\(R\\)) quantifies this relationship, and takes values between -1 and 1. A correlation of 1 means that the two variables are perfectly positively correlated (when one increases, the other increases by a fixed amount), and a correlation of -1 means that the two variables are perfectly negatively correlated (when one increases, the other decreases by a fixed amount). A correlation of 0 means that there is no linear relationship between the two variables. 2\nCorrelation is nice because it is a single number that summarizes the relationship between two variables and tells us how strong that relationship is. Regression coefficients are also single numbers that summarize the relationship between two variables, but they vary depending on the units of the variables.\nHere is the key point we want to make: correlation is a special case of regression. Specifically, the correlation coefficient is the slope of the regression line when both variables are standardized (i.e., when they are \\(z\\)-scored – subtracting off the mean and dividing by the standard deviation).\nLet’s see this in action. We’ll generate some date with different linear relationships, and run a regression before and after standardizing the variables.\n\n\nCode\nimport numpy as np\nfrom scipy import stats\nfrom statsmodels.formula.api import ols\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrng = np.random.default_rng(42)  # for reproducibility\nx_shared = 20 * rng.normal(size=100) + 7\n\ny_1 = 2.5 * x_shared + 10 + rng.normal(size=100, loc=0, scale=15)\ny_2 = -3 * x_shared + 15 + rng.normal(size=100, loc=0, scale=30)\ny_3 = 0.4 * x_shared + 12 + rng.normal(size=100, loc=0, scale=10)\n\ndata = pd.DataFrame({\n    \"x\": x_shared,\n    \"x_norm\": stats.zscore(x_shared),\n    \"y_1\": y_1,\n    \"y_2\": y_2,\n    \"y_3\": y_3,\n    \"y_1_norm\": stats.zscore(y_1),\n    \"y_2_norm\": stats.zscore(y_2),\n    \"y_3_norm\": stats.zscore(y_3)\n})\n\nfig, ax = plt.subplots(3, 2, figsize=(12, 12), sharex=False, sharey=False)\nfor i in range(3):\n    sns.regplot(data=data, x=\"x\", y=f\"y_{i+1}\", ax=ax[i, 0], ci=None, line_kws={\"color\": \"red\"})\n    # calculate the regression coefficient and correlation\n    model = ols(f\"y_{i+1} ~ x\", data=data).fit()\n    slope = model.params[\"x\"]\n    intercept = model.params[\"Intercept\"]\n    r_value, _ = stats.pearsonr(data[\"x\"], data[f\"y_{i+1}\"])\n    # print the regression coefficient and correlation, top center\n    ax[i, 0].annotate(f\"y = {slope:.2f}x + {intercept:.2f}\\nR = {r_value:.2f}\", xy=(0.4, 0.95), xycoords=\"axes fraction\", fontsize=12, ha=\"left\", va=\"top\")\n    sns.regplot(x=stats.zscore(data[\"x\"]), y=stats.zscore(data[f\"y_{i+1}\"]), ax=ax[i, 1], ci=None, line_kws={\"color\": \"red\"})\n    # calculate the regression coefficient and correlation for standardized data\n    model_norm = ols(f\"y_{i+1}_norm ~ x_norm\", data=data).fit()\n    slope_norm = model_norm.params[\"x_norm\"]\n    intercept_norm = model_norm.params[\"Intercept\"]\n    r_value_norm, _ = stats.pearsonr(stats.zscore(data[\"x\"]), stats.zscore(data[f\"y_{i+1}\"]))\n    # print the regression coefficient and correlation, top center\n    ax[i, 1].annotate(f\"y = {slope_norm:.2f}x + {intercept_norm:.2f}\\nR = {r_value_norm:.2f}\", xy=(0.4, 0.95), xycoords=\"axes fraction\", fontsize=12, ha=\"left\", va=\"top\")\n    ax[i, 0].set_title(\"Original Scale\")\n    ax[i, 1].set_title(\"Standardized Scale\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nGoing back to the Galton dataset, we can compute the correlation between the heights of the parents and the heights of the children.\n\n\nCode\nmodel = ols(\"height ~ midparent\", data=galton).fit()\npredicted_heights = model.predict(galton[\"midparent\"]).values\n\ngalton[\"midparent_z\"] = (galton[\"midparent\"] - galton[\"midparent\"].mean()) / galton[\"midparent\"].std(ddof=1)\ngalton[\"child_z\"] = (galton[\"height\"] - galton[\"height\"].mean()) / galton[\"height\"].std(ddof=1)\n\nmodel_z = ols(\"child_z ~ midparent_z\", data=galton).fit()\npredicted_heights_z = model_z.predict(galton[\"midparent_z\"]).values\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=False, sharex=False)\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height\", ax=ax[0], alpha=0.5)\nsns.lineplot(x=galton[\"midparent\"], y=predicted_heights, color=\"k\", label=\"Predicted Height\", ax=ax[0])\nax[0].annotate(f\"Slope: {model.params['midparent']:.2f}\\nR: {np.sqrt(model.rsquared):.2f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\", fontsize=12, ha=\"left\", va=\"top\")\nax[0].set_xlabel(\"Midparent Height (avg of z-scores)\")\nax[0].set_ylabel(\"Child's Height (inches)\")\n\nsns.scatterplot(data=galton, x=\"midparent_z\", y=\"child_z\", ax=ax[1], alpha=0.5)\nsns.lineplot(x=galton[\"midparent_z\"], y=predicted_heights_z, color=\"k\", label=\"Predicted Height\", ax=ax[1])\nax[1].annotate(f\"Slope: {model_z.params['midparent_z']:.2f}\\nR: {np.sqrt(model_z.rsquared):.2f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\", fontsize=12, ha=\"left\", va=\"top\")\nax[1].set_xlabel(\"Midparent Height (z-score)\")\nax[1].set_ylabel(\"Child's Height (z-score)\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#footnotes",
    "href": "notebooks/lecture-08.html#footnotes",
    "title": "Lecture 08: Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Galton dataset is is a classic dataset in statistics, named after Sir Francis Galton, who studied the relationship between the heights of parents and their children. The study is the origin of the term “linear regression” and is murkily involved in the eugenics movement of the early 1900s. Data from here.↩︎\nThere is a super fun online game where you can practice guessing correlations based on scatterplots: Guess the Correlation↩︎",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html",
    "href": "notebooks/lecture-06.html",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "",
    "text": "We have talked about quantifying uncertainty with \\(p\\)-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking “how likely is it that the average is \\(\\mu=0\\)?” we might ask “what is a plausible range of values for the average?”\nThis range is called a confidence interval. Like in hypothesis testing, we can choose a risk level \\(\\alpha\\) (e.g., 5%). The confidence interval, like a hypothesis test, is computed based on a sample – so sometimes it is incorrect!\nMathematically, we the confidence interval for a parameter \\(\\theta\\) is defined as the range of values bounded by \\([L, U]\\) that are likely to contain the true value of the parameter with probability \\(1-\\alpha\\): \\[\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n\\]\nThere is an important distinction to make here: \\(\\theta\\) is not the thing that changes! In fact, we can’t compute \\(\\theta\\) - it is a theoretical property of the population, which we can’t access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\nThe sample mean \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\). Meaning, we can assume that the sample mean is typically close to the population mean.\nThe variance of the sample mean is \\(\\sigma^2/n\\), where \\(\\sigma^2\\) can be estimated from the sample.\nThe distribution of the sample mean is symmetric.\n\n\n\nCode\n# annotated plot of the sample mean distribution\nfix, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-3, 3, 100)\npdf = stats.norm.pdf(x, loc=0, scale=1)\nax.plot(x, pdf, label=\"Sample Mean Distribution\")\nax.fill_between(x, pdf, alpha=0.1, color=\"blue\")\n# ax.axvline(0, color=\"black\", linestyle=\"--\")\n# ax.axvline([1, -1], color=\"red\", linestyle=\"--\", label=r\"$\\mu \\pm \\frac{\\sigma}{\\sqrt{n}}$\")\n# ax.annotate(r\"$\\mu$\", xy=(0, 0.1), xytext=(0.5, 0.2),\n#             arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\"),\n#             fontsize=18, color=\"black\")\nax.set_title(r\"Sample Mean $\\bar{X}$ Distribution\")\nax.set_xlabel(\"Sample Mean\")\nax.set_ylabel(\"Probability Density\")\nax.set_xticks(np.arange(-3, 4, 1))\nax.set_xticklabels([r\"$\\mu-3\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu-2\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu-\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu$\", r\"$\\mu+\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu+2\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu+3\\frac{\\sigma}{\\sqrt{n}}$\"])\nax.grid(True, linestyle=\"--\", axis='x', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within \\(k\\) standard deviations of the mean?\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range \\([\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]\\) as: \\[\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n\\] where we made the substitution \\(u = (x - \\mu) \\sqrt{n}/\\sigma\\).\nWe can set this integral to be equal to the probability \\(1 - \\alpha\\) to find the value of \\(z\\) that corresponds to a given confidence level.\n\\[\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n\\]\nSolving this integral for a chosen value of \\(\\alpha\\) gives us the value of \\(z\\) that corresponds to that confidence level. We denote it as \\(z_{\\alpha/2}\\), because the upper limit corresponds to the \\((1 - \\frac{\\alpha}{2})\\) quantile of the distribution.\n\n\n\n\n\n\nStandard normal distribution quantiles\n\n\n\n\n\nThe standard normal distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So \\((1 - \\alpha)\\) is the probability that a sample from the standard normal distribution falls within \\([-z, z]\\).\nThis is the \\((1 - \\frac{\\alpha}{2})\\) because the standard normal distribution is symmetric around 0. So the probability \\(\\alpha\\) of the sample falling outside of \\([-z, z]\\) is split equally between the two tails of the distribution. So there is a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling below \\(-z\\) and a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling above \\(z\\).\n\n\n\nIf you evaluate this integral, you find that the probability of the sample mean being within \\(z\\) standard errors 1 of the mean is, for various values of \\(z\\):\n\n\n\n\\(z\\)\nProbability\n\n\n\n\n1\n0.6827\n\n\n2\n0.9545\n\n\n3\n0.9973\n\n\n4\n&gt; 0.9999\n\n\n\nWe use this logic to define a confidence interval for the population mean \\(\\mu\\) as: \\[\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#confidence-intervals",
    "href": "notebooks/lecture-06.html#confidence-intervals",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "",
    "text": "We have talked about quantifying uncertainty with \\(p\\)-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking “how likely is it that the average is \\(\\mu=0\\)?” we might ask “what is a plausible range of values for the average?”\nThis range is called a confidence interval. Like in hypothesis testing, we can choose a risk level \\(\\alpha\\) (e.g., 5%). The confidence interval, like a hypothesis test, is computed based on a sample – so sometimes it is incorrect!\nMathematically, we the confidence interval for a parameter \\(\\theta\\) is defined as the range of values bounded by \\([L, U]\\) that are likely to contain the true value of the parameter with probability \\(1-\\alpha\\): \\[\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n\\]\nThere is an important distinction to make here: \\(\\theta\\) is not the thing that changes! In fact, we can’t compute \\(\\theta\\) - it is a theoretical property of the population, which we can’t access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\nThe sample mean \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\). Meaning, we can assume that the sample mean is typically close to the population mean.\nThe variance of the sample mean is \\(\\sigma^2/n\\), where \\(\\sigma^2\\) can be estimated from the sample.\nThe distribution of the sample mean is symmetric.\n\n\n\nCode\n# annotated plot of the sample mean distribution\nfix, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-3, 3, 100)\npdf = stats.norm.pdf(x, loc=0, scale=1)\nax.plot(x, pdf, label=\"Sample Mean Distribution\")\nax.fill_between(x, pdf, alpha=0.1, color=\"blue\")\n# ax.axvline(0, color=\"black\", linestyle=\"--\")\n# ax.axvline([1, -1], color=\"red\", linestyle=\"--\", label=r\"$\\mu \\pm \\frac{\\sigma}{\\sqrt{n}}$\")\n# ax.annotate(r\"$\\mu$\", xy=(0, 0.1), xytext=(0.5, 0.2),\n#             arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\"),\n#             fontsize=18, color=\"black\")\nax.set_title(r\"Sample Mean $\\bar{X}$ Distribution\")\nax.set_xlabel(\"Sample Mean\")\nax.set_ylabel(\"Probability Density\")\nax.set_xticks(np.arange(-3, 4, 1))\nax.set_xticklabels([r\"$\\mu-3\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu-2\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu-\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu$\", r\"$\\mu+\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu+2\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu+3\\frac{\\sigma}{\\sqrt{n}}$\"])\nax.grid(True, linestyle=\"--\", axis='x', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within \\(k\\) standard deviations of the mean?\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range \\([\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]\\) as: \\[\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n\\] where we made the substitution \\(u = (x - \\mu) \\sqrt{n}/\\sigma\\).\nWe can set this integral to be equal to the probability \\(1 - \\alpha\\) to find the value of \\(z\\) that corresponds to a given confidence level.\n\\[\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n\\]\nSolving this integral for a chosen value of \\(\\alpha\\) gives us the value of \\(z\\) that corresponds to that confidence level. We denote it as \\(z_{\\alpha/2}\\), because the upper limit corresponds to the \\((1 - \\frac{\\alpha}{2})\\) quantile of the distribution.\n\n\n\n\n\n\nStandard normal distribution quantiles\n\n\n\n\n\nThe standard normal distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So \\((1 - \\alpha)\\) is the probability that a sample from the standard normal distribution falls within \\([-z, z]\\).\nThis is the \\((1 - \\frac{\\alpha}{2})\\) because the standard normal distribution is symmetric around 0. So the probability \\(\\alpha\\) of the sample falling outside of \\([-z, z]\\) is split equally between the two tails of the distribution. So there is a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling below \\(-z\\) and a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling above \\(z\\).\n\n\n\nIf you evaluate this integral, you find that the probability of the sample mean being within \\(z\\) standard errors 1 of the mean is, for various values of \\(z\\):\n\n\n\n\\(z\\)\nProbability\n\n\n\n\n1\n0.6827\n\n\n2\n0.9545\n\n\n3\n0.9973\n\n\n4\n&gt; 0.9999\n\n\n\nWe use this logic to define a confidence interval for the population mean \\(\\mu\\) as: \\[\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#uncertainty-without-models",
    "href": "notebooks/lecture-06.html#uncertainty-without-models",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Uncertainty without models",
    "text": "Uncertainty without models\nSo far, we’ve emphasized the importance of developing models of data-generating processes in order to quantify randomness and uncertainty. If you know something about the underlying process that generates your data, you can pick an appropriate statistical model and use it to sample from the distribution, simulate new data, and evaluate hypotheses. The CLT is a powerful tool because even when we don’t know what a good model of the data-generating process is, we can often use the normal distribution as an approximation for the distribution of sample means.\nHowever, there are still many situations where we don’t know enough about the DGP to confidently specify a model. In these cases, we can still use statistical techniques to evaluate hypotheses about the data. The idea is that if we don’t have any knowledge about the DGP, we rely on the only information we have: the data itself.\n\nResampling methods\nHow did we get a dataset in the first place? Recall that every dataset is a sample from some underlying distribution.\nWhat is the main difference between a sample and a population? A sample is a subset of the population, and it is usually much smaller than the population. The sample might not cover all the “edge cases” or rare events that are present in the population, or certain outcomes might be overrepresented (you could flip a fair coin 10 times and get 10 heads, even if it is unlikely). But as we have seen, the bigger the sample is the more it tends to represent the population well.\n\n\nCode\n# sample data from a normal distribution\nrng = np.random.default_rng(42)  # for reproducibility\nfig, ax = plt.subplots(1, 3, figsize=(10, 4))\nfor i, n_samples in enumerate([10, 100, 1000]):\n    sample = rng.normal(loc=0, scale=1, size=n_samples)\n    ax[i].hist(sample, bins=30, density=True, alpha=0.5)\n    ax[i].set_title(f\"Sample of {n_samples} from N(0, 1)\")\n    ax[i].set_xlabel(\"Value\")\n    ax[i].set_ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf the sample starts to look like the population… then maybe we can just use the sample itself as a proxy for the population? This is the idea behind resampling methods. When the sample is pretty large, we can sub-sample from the original dataset as though it were the population, and use the sub-samples to estimate properties of the population.\nLet’s try this with the example above. Treating each of the histograms as a proxy for the population, we will sub-sample (with replacement) from the sample 1000 times to create new samples, and see what they look like.\n\n\nCode\nrng = np.random.default_rng(42)  # for reproducibility\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\nfor i, n_samples in enumerate([10, 100, 1000]):\n    sample = rng.normal(loc=0, scale=1, size=n_samples)\n    # choose samples at random with replacement\n    resample = rng.choice(sample, size=10000, replace=True)\n    sns.histplot(resample, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Resampled', ax=ax[i])\n    sns.histplot(sample, bins=30, stat=\"probability\", alpha=0.1, color='blue', label='Original', ax=ax[i])\n    ax[i].set_title(f\"10000 subsamples of {n_samples} samples\", fontsize=12)\n    ax[i].set_xlabel(\"Value\")\n    ax[i].set_ylabel(\"Probability\")\nplt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs it turns out, the distribution of the resampled data is very similar to the original sample. So if the original sample is similar to the true population distribution, the resampled distribution will retain that similarity.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#bootstrapping",
    "href": "notebooks/lecture-06.html#bootstrapping",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping is a specific type of resampling method that allows us to estimate the sampling distribution of any test statistic (e.g., mean, median, variance) by repeatedly resampling with replacement from the observed data. This bootstrapped distribution can then be used to compute confidence intervals or perform hypothesis tests without making any assumptions about the underlying distribution of the data!\nBootstrapping works as follows:\n\nGiven a dataset of size \\(n\\), draw a sample of size \\(n\\) from the original dataset with replacement.\nCompute the test statistic (e.g., mean, median, variance) on the bootstrapped sample.\nRepeat steps 1 and 2 a large number of times (e.g., 1000 times) to create a distribution of the test statistic.\nUse the distribution of the test statistic to compute confidence intervals or perform hypothesis tests.\n\n\n\n\n\n\n\nWhy sample with replacement?\n\n\n\n\n\nThe idea behind the bootstrap is to take independent samples from the original dataset. It is supposed to simulate i.i.d. sampling from the population. Sampling with replacement allows us to create new samples such that taking a sample does not affect the next sample, and every bootstrapped sample comes from the same proxy distribution.\nThink about what would happen if we sampled without replacement: since the sample is the same size as the original dataset, we would end up with the same sample every time! There are only \\(n\\) datapoints in the sample, so if we sample without replacement, we will always get the same sample back.\n\n\n\nLet’s generate a sample from a normal distribution with known parameters \\(\\mu=3.2\\) and \\(\\sigma=1.5\\). Then, we will use bootstrapping to estimate the mean and variance of the population from the sample.\n\n\nCode\nrng = np.random.default_rng(42)\n\nsample_size = 1000\nn_bootstraps = 1000\n\noriginal_sample = rng.normal(loc=3.2, scale=1.5, size=sample_size)\n\nbootstrapped_means = []\nbootstrapped_std = []\nfor _ in range(n_bootstraps):\n    resample = rng.choice(original_sample, size=sample_size, replace=True)\n    bootstrapped_means.append(np.mean(resample))\n    bootstrapped_std.append(np.std(resample, ddof=1))\nbootstrapped_means = np.array(bootstrapped_means)\nbootstrapped_std = np.array(bootstrapped_std)\nprint(f\"Bootstrapped mean: {np.mean(bootstrapped_means):.2f}\")\nprint(f\"Bootstrapped standard deviation: {np.mean(bootstrapped_std):.2f}\")\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(bootstrapped_means, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Bootstrapped Means', ax=ax[0])\nsns.histplot(bootstrapped_std, bins=30, stat=\"probability\", alpha=0.5, color='orange', label='Bootstrapped Std Devs', ax=ax[1])\nax[0].axvline(np.mean(bootstrapped_means), color='red', linestyle='--', label='Mean of Means')\nax[1].axvline(np.mean(bootstrapped_std), color='red', linestyle='--', label='Mean of Std Devs')\nax[0].legend()\nax[1].legend()\nax[0].set_title(\"Bootstrapped Means Distribution\")\nax[1].set_title(\"Bootstrapped Std Devs Distribution\")\nax[0].set_xlabel(\"Mean Value\")\nax[1].set_xlabel(\"Standard Deviation Value\")\nax[0].set_ylabel(\"Probability\")\nax[1].set_ylabel(\"Probability\")\nplt.show()\n\n\nBootstrapped mean: 3.16\nBootstrapped standard deviation: 1.48\n\n\n\n\n\n\n\n\n\nHere is the real power of bootstrapping, though: because we have the whole distribution of the resampled data, we can compute confidence intervals any statistic by looking at at different percentiles of the resampled distribution. For example, we can compute a 95% confidence interval for the mean by taking the 2.5th and 97.5th percentiles of the resampled means.\n\n\nCode\nlower_bound_mean = np.percentile(bootstrapped_means, 2.5)\nupper_bound_mean = np.percentile(bootstrapped_means, 97.5)\nprint(f\"95% Confidence Interval for the Mean: ({lower_bound_mean:.2f}, {upper_bound_mean:.2f})\")\nlower_bound_std = np.percentile(bootstrapped_std, 2.5)\nupper_bound_std = np.percentile(bootstrapped_std, 97.5)\nprint(f\"95% Confidence Interval for the Standard Deviation: ({lower_bound_std:.2f}, {upper_bound_std:.2f})\")\n\n# plot the distribution of bootstrapped means and std with confidence intervals\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\nsns.histplot(bootstrapped_means, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Bootstrapped Means', ax=ax[0])\nax[0].axvline(lower_bound_mean, color='blue', linestyle='--', label='Lower Bound (2.5th Percentile)')\nax[0].axvline(upper_bound_mean, color='green', linestyle='--', label='Upper Bound (97.5th Percentile)')\nax[0].set_title(\"Bootstrapped Means Distribution\")\nax[0].set_xlabel(\"Mean Value\")\nax[0].set_ylabel(\"Probability\")\nax[0].legend()\n\nsns.histplot(bootstrapped_std, bins=30, stat=\"probability\", alpha=0.5, color='orange', label='Bootstrapped Std', ax=ax[1])\nax[1].axvline(lower_bound_std, color='red', linestyle='--', label='Lower Bound (2.5th Percentile)')\nax[1].axvline(upper_bound_std, color='blue', linestyle='--', label='Upper Bound (97.5th Percentile)')\nax[1].set_title(\"Bootstrapped Std Distribution\")\nax[1].set_xlabel(\"Standard Deviation Value\")\nax[1].set_ylabel(\"Probability\")\nax[1].legend()\nplt.show()\n\n\n95% Confidence Interval for the Mean: (3.07, 3.25)\n95% Confidence Interval for the Standard Deviation: (1.41, 1.55)\n\n\n\n\n\n\n\n\n\n\nUnderstanding the meaning of a confidence interval\nThe definition of a confidence interval is a bit confusing, but bootstrapping can actually help us demonstrate what it means.\nThe uncertainty in the confidence interval does NOT come from uncertainty in the value of the parameter we are estimating. In our example, we designed the distribution and we know that the true mean is 3.2, so there is no uncertainty about the value of the parameter. The uncertainty comes from the fact that we are estimating the parameter based on a sample, and the sample is not necessarily representative of the population.\nTo illustrate this, let’s repeatedly sample from our distribution and run the bootstrapping procedure on each sample. In the plot below, each interval represents a 95% confidence interval for the mean of a different sample. The highlighted intervals are the ones that failed to capture the true mean of 3.2.\n\n\nCode\ndef bootstrap_mean_ci(sample, n_iterations=1000, alpha=0.05, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    n = len(sample)\n    means = []\n    for _ in range(n_iterations):\n        resample = rng.choice(sample, size=n, replace=True)\n        means.append(np.mean(resample))\n    lower_bound = np.percentile(means, 100 * alpha / 2)\n    upper_bound = np.percentile(means, 100 * (1 - alpha / 2))\n    return lower_bound, upper_bound\n\n\n## Do a bootstrap on 1000 samples from the same distribution, see what proportion of CIs contain the mean\nrng = np.random.default_rng(42)\nn_sims = 1000\nsample_size = 1000\n# store the intervals (lower, upper)\nci_list = []\n# track which intervals do not contain the true mean\nviolating_ci_idx = []\nfor i in range(n_sims):\n    sample = rng.normal(loc=3.2, scale=1.5, size=sample_size)\n    lower_bound, upper_bound = bootstrap_mean_ci(\n        sample, n_iterations=1000, alpha=0.05, rng=rng\n    )\n    ci_list.append((i, lower_bound, upper_bound))\n    # check if the true mean is within the confidence interval\n    if not (3.2 &gt;= lower_bound and 3.2 &lt;= upper_bound):\n        # print(f\"Iteration {i}: CI does not contain the true mean 3.2\")\n        violating_ci_idx.append(i)\nprint(\n    f\"Proportion of CIs that do not contain the true mean: {len(violating_ci_idx) / n_sims:.4f}\"\n)\n\n# plot the first 5 violating CIs\nfig, ax = plt.subplots(figsize=(10, 6))\nfor i in range(violating_ci_idx[5] + 1):\n    if i in violating_ci_idx:\n        alpha = 1.0\n    else:\n        alpha = 0.2\n    lower_bound, upper_bound = ci_list[i][1], ci_list[i][2]\n    ax.plot([i, i], [lower_bound, upper_bound], color=\"gray\", alpha=alpha, linewidth=2)\n    ax.scatter(\n        i, lower_bound, color=\"blue\", alpha=alpha, label=\"Lower Bound\" if i == 0 else \"\", zorder=3\n    )\n    ax.scatter(\n        i,\n        upper_bound,\n        color=\"green\",\n        alpha=alpha,\n        label=\"Upper Bound\" if i == 0 else \"\",\n        zorder=3\n    )\n\nax.axhline(3.2, color=\"black\", linestyle=\"--\", label=\"True Mean (3.2)\")\nax.set_title(\"Violating Confidence Intervals\")\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"Confidence Interval\")\nax.legend()\nplt.show()\n\n\nProportion of CIs that do not contain the true mean: 0.0580\n\n\n\n\n\n\n\n\n\nNotice that about 5% of the intervals do not capture the true mean, which is exactly what we expect from a 95% confidence interval.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#example-variability-in-scoring-average",
    "href": "notebooks/lecture-06.html#example-variability-in-scoring-average",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Example: variability in scoring average",
    "text": "Example: variability in scoring average\nLet’s return to our analysis of NBA players’ scoring ability.\nWe can use bootstrapping to estimate confidence intervals for each player’s scoring average.\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace({\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan})\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\n# print scoring averages\nprint(f\"Shai Gilgeous-Alexander's scoring average: {compare_df[compare_df['player'] == 'Shai Gilgeous-Alexander']['PTS'].mean():.2f}\")\nprint(f\"Giannis Antetokounmpo's scoring average: {compare_df[compare_df['player'] == 'Giannis Antetokounmpo']['PTS'].mean():.2f}\")\n\n### Bootstrapping confidence intervals for scoring average ###\n\nsga_lower_bound, sga_upper_bound = bootstrap_mean_ci(\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    n_iterations=1000,\n    alpha=0.05,\n    rng=np.random.default_rng(42)\n)\ngiannis_lower_bound, giannis_upper_bound = bootstrap_mean_ci(\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    n_iterations=1000,\n    alpha=0.05,\n    rng=np.random.default_rng(42)\n)\nprint(\"-\"*25)\nprint(f\"Shai Gilgeous-Alexander's 95% CI: ({sga_lower_bound:.2f}, {sga_upper_bound:.2f})\")\nprint(f\"Giannis Antetokounmpo's 95% CI: ({giannis_lower_bound:.2f}, {giannis_upper_bound:.2f})\")\n\n\n\nShai Gilgeous-Alexander's scoring average: 32.68\nGiannis Antetokounmpo's scoring average: 30.39\n-------------------------\nShai Gilgeous-Alexander's 95% CI: (30.97, 34.42)\nGiannis Antetokounmpo's 95% CI: (28.70, 32.09)\n\n\nNotice that Shai’s 95% CI does not overlap with Giannis’ scoring average, but it does overlap with Giannis’ 95% CI!\nThis actually highlights a problem with our previous analysis: we only considered the variability in SGA’s scoring average, and asked how likely it is that his scoring average was equivalent to a specific value (Giannis’ scoring average). But we didn’t consider the variability in Giannis’ scoring average!\nThink about it like this: perhaps SGA’s scoring average was randomly higher than expected, and Giannis’ scoring average was randomly lower than expected. This possibly makes it more likely that the two averages are closer together than it would appear.\nAnother way of stating this is that our null hypothesis was too weak (we chose a baseline equal to Giannis’ scoring average, but Giannis could actually be better or worse than that).\nLet’s fix this mistake! We can use bootstrapping to generate scoring average distributions for both players, and then compare the two distributions to see how much they overlap.\nThe question is: if we resample with replacement from both players’ game logs (i.e., their individual game scores), how often do we find that Giannis scores more on average than SGA?\n\n\nCode\ndef bootstrap_mean_dist(sample, n_iterations=1000, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    n = len(sample)\n    means = []\n    for _ in range(n_iterations):\n        resample = rng.choice(sample, size=n, replace=True)\n        means.append(np.mean(resample))\n    return np.array(means)\n# Generate bootstrapped distributions for both players\nsga_dist = bootstrap_mean_dist(\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    n_iterations=1000,\n    rng=np.random.default_rng(42)\n)\ngiannis_dist = bootstrap_mean_dist(\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    n_iterations=1000,\n    rng=np.random.default_rng(42)\n)\n# compute the proportion of bootstrapped means where Giannis's mean is greater than or equal to SGA's mean\np_value = np.mean(giannis_dist &gt;= sga_dist)\nprint(f\"Proportion of bootstrapped samples where Giannis's mean &gt;= SGA's mean: {p_value:.4f}\")\n\n# Plot the distributions\nfig, ax = plt.subplots(2, 1, figsize=(10, 6))\nsns.histplot(sga_dist, bins=30, stat=\"probability\", alpha=0.5, color='blue', label='Shai Gilgeous-Alexander', ax=ax[0])\nsns.histplot(giannis_dist, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Giannis Antetokounmpo', ax=ax[0])\nax[0].axvline(np.mean(sga_dist), color='blue', linestyle='--')\nax[0].axvline(np.mean(giannis_dist), color='green', linestyle='--')\nax[0].set_title(\"Bootstrapped Scoring Average Distributions\")\nax[0].set_xlabel(\"Scoring Average\")\nax[0].set_ylabel(\"Probability\")\nax[0].legend()\n# Plot the distribution of the difference in means\ndiff_dist = sga_dist - giannis_dist\nsns.histplot(diff_dist, bins=30, stat=\"probability\", alpha=0.5, label='Difference (SGA - Giannis)', ax=ax[1])\nax[1].fill_betweenx(\n    y=ax[1].get_ylim(),\n    x1=ax[1].get_xlim()[0],\n    x2=0,\n    alpha=0.1\n)\nax[1].axvline(0, color='purple', linestyle='--', label=r'$\\mu_{\\text{SGA}} - \\mu_{\\text{Giannis}} = 0$')\nax[1].set_title(\"Bootstrapped Difference in Scoring Averages\")\nax[1].set_xlabel(\"Difference in Scoring Average (SGA - Giannis)\")\nax[1].set_ylabel(\"Probability\")\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nProportion of bootstrapped samples where Giannis's mean &gt;= SGA's mean: 0.0340\n\n\n\n\n\n\n\n\n\nRecall that in the previous analysis, our \\(p\\)-value was 0.004! So our corrected analysis estimates that the probability that SGA’s scoring title is a “fluke” is actually an order of magnitude higher.\nAnother way to look at this is to return to the lens of hypothesis testing. What hypothesis test captures the variability of both players’ scoring? How is our null hypothesis different from the one we used before?\nEarlier we just tested if SGA’s true scoring rate was higher than a constant value (Giannis’ observed scoring average). Now, we are testing if SGA’s true scoring rate is higher than Giannis’ true scoring rate, which is a random variable that can take on different values across different samples.\n\n\\(H_0: ~\\mu_{\\text{SGA}} \\leq \\mu_{\\text{Giannis}}\\)\n\\(H_1: ~\\mu_{\\text{SGA}} &gt; \\mu_{\\text{Giannis}}\\)\n\nThis null hypothesis basically posits that in the best case, SGA’s scoring average is equal to Giannis’ scoring average (if not worse). This would mean that their scoring output is indistinguishable – if the variance of their scoring is on the same scale, then their game logs are actually being sampled from the same distribution! We can simulate this in a bootstrapping procedure by combining the two players’ game logs and resampling from the combined distribution.\nThe question is: if we resample with replacement from the combined game logs (i.e. the players have the exact same scoring distribution), how often do we see such a large advantage for SGA?\n\n\nCode\nnp.random.seed(42)  \n# number of games played by each player\nn_games_sga = len(compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"])\nn_games_giannis = len(compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"])\n# Compute the observed difference in means\nobserved_diff = (\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"].mean()\n    - compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"].mean()\n)\nprint(f\"Observed difference in scoring averages: {observed_diff:.2f} (SGA - Giannis)\")\n\nn_bootstraps = 1000\nbootstrapped_diffs = []\n# Perform bootstrapping to compute the difference in means\nfor i in range(n_bootstraps):\n    # Bootstrap sampling for SGA\n    sga_sample = compare_df[\"PTS\"].sample(n=n_games_sga, replace=True)\n    # Bootstrap sampling for Giannis\n    giannis_sample = compare_df[\"PTS\"].sample(n=n_games_giannis, replace=True)\n    # Compute the means\n    sga_mean = sga_sample.mean()\n    giannis_mean = giannis_sample.mean()\n    diff = sga_mean - giannis_mean\n    bootstrapped_diffs.append(diff)\nbootstrapped_diffs = np.array(bootstrapped_diffs)\n# Compute the p-value\np_value = np.mean(bootstrapped_diffs &gt;= observed_diff)\nprint(f\"Bootstrapped p-value for the difference in scoring averages: {p_value:.4f}\")\n\n\nObserved difference in scoring averages: 2.30 (SGA - Giannis)\nBootstrapped p-value for the difference in scoring averages: 0.0440\n\n\nSimulation and resampling methods give us a powerful way to quantify uncertainty in a way that does not rely so heavily on assumptions about the underlying distribution of the data.\nIn the next lecture, we’ll look at another simulation-based method for hypothesis testing: permutation tests.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#footnotes",
    "href": "notebooks/lecture-06.html#footnotes",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. the standard deviation of the sampling distribution↩︎",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html",
    "href": "notebooks/lecture-04.html",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#data-generating-processes",
    "href": "notebooks/lecture-04.html#data-generating-processes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#statistical-models",
    "href": "notebooks/lecture-04.html#statistical-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\nLet’s return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a Bernoulli distribution, which describes the probability of each outcome. Specifically,\n\\[P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}\\]\nwhere (X) is the outcome of the coin flip, (p) is the probability of heads, and (1 - p) is the probability of tails. If we assume a fair coin, then (p = 0.5).\nNow clearly, this model does not capture the complexity of a real-world coin flip, which is of course influenced by many factors such as the weight of the coin, the force of the flip, air resistance, etc. Statistical models are always reductive in this sense. But the important thing is that a Bernoulli distribution really does do a good job of describing the outcomes of a coin flip. As long as that is the case, we can use the model to make predictions about the data generated by the process.\nTo see this, consider the following process: you roll a fair die 100 times. For each roll, you record whether the number you rolled was even or odd. Since there are three even numbers (2, 4, 6) and three odd numbers (1, 3, 5), and each number has an equal probability of being rolled, the probability of rolling an even number is 0.5 and the probability of rolling an odd number is also 0.5. This is statistically identical to flipping a fair coin! So we can use the same Bernoulli distribution to model the outcomes of this process, even though it completely ignore the details of the die rolling process itself. We lose information about the specific numbers rolled, but as long as we accurately capture the probability of the outcomes we care about, it doesn’t matter.\n\n\n\nCode\ndef dice_even_odd(n=1000):\n    \"\"\"\n    Simulate rolling a die n times and return the results.\n    \n    Parameters:\n    n (int): Number of rolls to simulate.\n    \n    Returns:\n    pd.DataFrame: DataFrame with columns 'roll' and 'even_odd'.\n    \"\"\"\n    rolls = np.random.randint(1, 7, size=n)\n    is_even = rolls % 2 == 0\n    return pd.DataFrame({\"roll\": rolls, \"is_even\": is_even, \"even_odd\": np.where(is_even, \"even\", \"odd\")})\n\ndice_rolls = dice_even_odd(1000)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nsns.histplot(dice_rolls[\"roll\"], bins=np.arange(1, 8) - 0.5, discrete=True, ax=ax[0])\nax[0].set_xlabel(\"Die Roll\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Die Rolls\")\nax[0].set_xticks(np.arange(1, 7))\nax[0].grid(axis='y')\n\nsns.histplot(dice_rolls[\"even_odd\"], discrete=True, shrink=0.8, ax=ax[1])\nax[1].set_xlabel(\"Even or Odd\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Even and Odd Rolls\")\nax[1].grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\n\nStatistical modeling generally uses this kind of trick. We attempt to find a simple model that captures the essential features of the data generating process.\nWe don’t always know as much about the data generating process as we do for a coin flip or a die roll. In fact, often we don’t know anything about it at all! In these cases, the approach is a little more “guess and check”. We start with a simple model, and see how well it describes the data. If it doesn’t work, we try a more complex model or a different model altogether.\n\nChallenges with finite samples\nIt can be hard to tell if you have a good model or not. With any finite dataset, it is quite likely that the data will not perfectly match the model. This is because the model describes probabilistic tendencies of the data. But in any small sample we will likely see deviations from the idealized outcomes described by the model.\nThink about this in the extreme: if you flip a coin only once, you will either get heads or tails. So your observed proportion of heads will be either 0 or 1, which is very different from the expected proportion of 0.5. In fact it is impossible to observe the expected proportion of heads in a single flip! This is true for a single observation of basically any random variable that takes on more than one value – it is close to impossible to learn anything about the variable’s tendencies from just one observation.\nEven with a small number of observations, it is quite difficult to tell if a model is a good fit for the data.\nLet’s say you and your roommate are always arguing about who should take out the trash. You decide to flip a coin to decide who takes it out (you are notorious for always choosing tails, because you think it’s good luck). You decide to do a best-of-ten series, so you flip the coin 10 times and record the number of heads. It turns out that you get 3 heads and 7 tails. Your roommate is furious: “That’s not fair! You always get tails! I bet you rigged the coin!”\nIs your roommate justified in being suspicious? What is the probability of getting 3 heads in 10 flips of a fair coin? What is the probability of getting 3 heads in 10 flips of a biased coin that has a 25% chance of landing on heads?\n\n\n\n\n\n\n\n\nAs this exercise shows, with a small number of observations you have some information that can help you distinguish between the two models, but it is not enough to be confident in your conclusion.\nWith more data though, suddenly the evidence becomes much stronger. The probability of getting a badly imbalanced distribution of heads and tails becomes tiny as the number of flips increases.\nIn general this is great news – the more data you have, the more confident you can be in your conclusions. It’s probably not worth flipping a coin 1000 times to decide who takes out the trash, but if you did, you would be able to tell with much more certainty whether the coin is fair or not.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "href": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Convergence for large sample sizes",
    "text": "Convergence for large sample sizes\nWhen the size of a dataset is large enough, we get some guarantees.\n\nLaw of Large Numbers\nThe Law of Large Numbers (LLN) states that as the sample size increases, the sample mean will converge to the population mean. In other words, if we take enough samples, the average of those samples will be close to the true average of the population.\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with expected value \\(\\mathbb{E}[X]\\), then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges to \\(\\mathbb{E}[X]\\) as \\(n\\) approaches infinity: \\[\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1\\]\nPrecisely, this states that the probability that the sample mean converges to the population mean is 1. 1\n\n\n\n1  This is technically the Strong Law of Large Numbers, stating that the sample mean converges almost surely to the population mean. There is also a Weak Law of Large Numbers which states that the sample mean converges in probability to the population mean, which is a weaker condition.You don’t just have to take this for granted – let’s see the LLN in action.\nWe will simulate flipping a biased coin \\(n\\) times, and plot the proportion of heads as we increase the number of flips. We will see that the proportion converges to 0.25 as the number of flips increases.\n\n\nCode\n# set seed\nnp.random.seed(56)\n# flip a fair coin n times\nsamples_sizes = [10, 100, 1000, 1e4, 1e5, 1e6]\nproportion_heads = []\nfor n in samples_sizes:\n    # Simulate flipping a fair coin n times\n    flips = np.random.binomial(1, 0.25, size=int(n))\n    proportion_heads.append(np.mean(flips))\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.lineplot(x=samples_sizes, y=proportion_heads, marker='o', ax=ax)\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nax.set_title(\"Proportion of Heads in Coin Flips\")\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s just one experiment (flip \\(n\\) coins a single time). But this is a random process, so we can simulate it many times and see how the proportion of heads changes.\n\n\nCode\nnp.random.seed(56)\nsamples_sizes = [10, 100, 1000, 1e4, 1e5]\nsim_results = []\n\nfor n in samples_sizes: # outer loop is over sample size\n    for _ in range(1000):  # inner loop runs the simulation 1000 times, for each sample size\n        # sample n flips from a fair coin\n        flips = np.random.binomial(1, 0.25, size=int(n))\n        # calculate the proportion of heads\n        proportion_heads = np.mean(flips)\n        # store the result\n        sim_results.append(pd.Series({\"n\": n, \"proportion_heads\": proportion_heads}))\n# gather all the results (a list of Series) into a DataFrame\nsim_results = pd.concat(sim_results, axis=1).T\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# plot the proportion from individual simulations as points\nsns.stripplot(data=sim_results, \n              x=\"n\", y=\"proportion_heads\", \n              jitter=False, alpha=0.3, ax=ax, native_scale=True, marker='o', size=4)\n# plot the mean proportion of heads across simulations\nsns.lineplot(data=sim_results, \n             x=\"n\", y=\"proportion_heads\", \n             errorbar=\"sd\", marker='o', ax=ax, \n             label='Mean Proportion of Heads')\n# make ticks nicer\nax.set_xticks([10, 100, 1000, 10000, 100000])\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# histogram of the proportion of heads for each sample size\nsns.FacetGrid(sim_results, col=\"n\", col_wrap=2, height=4, aspect=1.2, sharex=False, sharey=False) \\\n    .map(sns.histplot, \"proportion_heads\", bins=np.linspace(0, 1, 41)) \\\n    .set_axis_labels(\"Proportion of Heads\", \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the variability across simulations decreases as the number of flips increases. This is what we mean when we say that the sample mean converges to the population mean. It’s not just close to the true mean on average, it is also more consistent across different samples/simulations.\nWe’re going to take advantage of this fact all the time. Basically any time you compare groups, or competing hypotheses, or evaluate predictive models, you end up taking the average of some quantity. The Law of Large Numbers tells us that as the sample size increases, the average will converge to the true value.\nAgain, what does this mean for your argument with your roommate? As you gather more and more data (coin flips), the proportion of heads (which is itself a sample mean) will converge to the true proportion of heads (the population mean). With enough flips, it becomes very clear whether the coin is rigged or not.\n\n\n\n\n\n\nUnbiased Estimators\n\n\n\nNotice in the simulation above that even when the sample mean was highly variable in small samples, it was always still centered around the true population mean. This is a crucial property for any estimator: it is said to be unbiased if the expected value of the estimator is equal to the true value of the parameter being estimated. In other words, on average, the estimator will give you the correct answer.\nThe LLN gives you a different guarantee: that as the sample size increases, the variance of the estimator decreases, so it becomes more and more likely that the estimator will be close to the true value.\n\n\n\n\nCentral Limit Theorem\nThe Central Limit Theorem states that the sample mean of \\(n\\) independent and identically distributed random variables will converge to a normal distribution, regardless of the distribution of the individual variables. This means that even if a variable is not normally distributed, if you average a bunch of them together, the sample mean will be approximately normally distributed.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges in distribution to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity: \\[\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\n\n\nConsider rolling a die – there is an equal chance (1/6) of getting each of the values between 1 and 6.\nNote that the expected value of each roll of the die is 3.5 – we can compute this as \\[E\\left[X\\right]=1\\left(\\frac{1}{6}\\right)+2\\left(\\frac{1}{6}\\right)+3\\left(\\frac{1}{6}\\right)+4\\left(\\frac{1}{6}\\right)+5\\left(\\frac{1}{6}\\right)+6\\left(\\frac{1}{6}\\right)=3.5\\]\nSo the mean of the distribution of dice values should be 3.5, but as you can see below it’s definitely not normal – values are distributed evenly from 1 to 6 rather than clustering closer to the mean.\n\n\nCode\n# plot histogram of 100 rolls of a die\nnp.random.seed(42)  # for reproducibility\nrolls = pd.DataFrame({'roll': np.random.randint(1, 7, 100)})\n\nsns.histplot(data=rolls, x='roll', bins=6, discrete=True, edgecolor=\"black\", stat='proportion')\nplt.axvline(x=rolls['roll'].mean(), color='red', linestyle='--', label='Mean')\nplt.title(\"Histogram of 100 dice rolls\")\nplt.xlabel(\"Roll\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBut what happens if we average the values of the 100 dice rolls in our sample? We can simulate this process many times (roll a dice 100 times, take the average, repeat) to get a distribution of the average.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) * np.sqrt(n)  # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\sqrt{n}(\\bar{X} - \\mathbb{E}[X])$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see the distribution of the sum is approximately normal! It has the mean we would expect (3.5) and clearly values are clustered symmetrically around the mean.\nFor the purposes of the class it’s not necessary to understand why this happens, just that it happens. To give a bit of intuition, though: with more samples, averages tend to cluster around the true mean of the distribution.\nWhy is this useful? Well, the normal distribution is a well studied distribution with many useful properties. It is symmetric, easy to work with mathematically, and is useful for approximating the distribution of many real-world processes. The normal distribution is decently good description of most data that clusters around its mean.\nThe CLT tells us that even if we don’t know the distribution of the individual variables, if we average enough of them together, we magically know the distribution of the sample mean.\nRemember earlier in the lecture, we talked about how statistical models are useful as long as they are a faithful description of the outcome probabilities for a DGP? Well, the CLT gives us a guarantee that for a huge class of DGPs, we can use the normal distribution to model the sample mean.\n\nStandard errors\nThe CLT tells us that the sample mean will converge to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity. This means that the standard deviation of the sample mean (called the standard error) is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\).\nWhat does it mean for the standard error to decrease as the sample size increases? It means that as we collect more data, our estimate of the mean becomes more precise. It’s basically the same point as the Law of Large Numbers!\nIn the above example we multiplied the deviations of the sample mean (\\(\\bar{X}-\\mathbb{E}[X]\\)) by \\(\\sqrt{n}\\) to put the distributions on the same scale, which makes it easier to visualize their shapes (so we can see that they become increasingly “normal”). But without that scaling, the distributions get increasingly narrow as \\(n\\) increases, which is exactly what the standard error describes.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\bar{X} - \\mathbb{E}[X]$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html",
    "href": "notebooks/lecture-02.html",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n\\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability",
    "href": "notebooks/lecture-02.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\\]\nBy definition this quantity cannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and it must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs).\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)). If we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like saying “there’s a 100% chance that something will happen”.\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events \\(H\\) and \\(T\\) cover 100% of the possible outcomes. So we can write: \\[\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\\]\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event \\(A\\), the event either happens or it doesn’t. So we can compute the probability of the event not occurring as: \\[\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\\]\n\nProbability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\).\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows: \\[\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\\]\n\n\nAddition and multiplication rules (and / or)\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that \\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as \\(B\\) above, so we can use the same calculation: \\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\).\n\n\n\n\n\n\nNote on notation\n\n\n\n\n\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively. The notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively. So we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads. Technically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs).\n\n\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events \\(A\\) and \\(B\\), the following rules hold:\n\nAddition rule: For any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\n\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram. \n\n\n\n\nMultiplication rule: For any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset.\n\n\n\n\n\n\n\nConditional probability\n\n\n\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”. The conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day.\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\nSo the conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\\]\n\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”\n\n\n\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n\nIndependence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\nHow does this relate to the multiplication rule? If \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\). That is, knowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply \\(\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\).\n\n\nComplicated counting\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes.\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let’s think about the case where the first flip is heads. The probability of getting a head on the first flip is \\(\\frac{1}{2}\\), and the probability of getting tails on the 4 other flips is also \\(\\frac{1}{2}\\) each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips: \\[\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}\\]\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting “exactly one head”. For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions: \\[\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16\\]\nThere are two common types of outcomes we want to count: permutations and combinations.\nA combination is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are choosing an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\\[\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nIt is clear here that there are 5 possible “slots” where we can place a head.\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still applies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\nLet’s take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices. \\[\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nNow let’s consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\\[\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n\\]\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads. This gives us a total of \\(5 \\cdot 4 = 20\\) ways to choose 2 flips to be heads out of 5 flips.\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order – where flip 1 was heads first).\nThis illustrates the key distinction between combinations and permutations. A permutation is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. If you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting.\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign “slots” in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length \\(n\\), the number of permutations is: \\[\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n\\]\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\nNamely, the number of combinations of \\(k\\) items from a set of \\(n\\) items is given by the formula: \\[\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n\\]\nThis formula counts all of the possible permutations of the sequence, and then divides by the number of ways to arrange the \\(k\\) items that are selected (which is \\(k!\\)) and the number of ways to arrange the remaining \\(n-k\\) items (which is \\((n-k)!\\)).\nIn our example, we have \\(n = 5\\) (the number of flips) and \\(k = 2\\) (the number of heads). So the number of combinations of 2 heads from 5 flips is: \\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-functions",
    "href": "notebooks/lecture-02.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\nHowever, it is often more convenient to work with probability functions. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails).\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\n\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values.\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n\n\n\n\n\n\n\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#random-variables",
    "href": "notebooks/lecture-02.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\).\nWe use probability functions to describe the probabilities associated with random variables. Specifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nBernoulli random variable\n\n\n\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin.\n\n\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUniform random variable\n\n\n\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\).\n\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "href": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nWe call the probability function for a random variable a probability distribution, which describes how the probabilities are distributed across the possible values of the random variable.\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point.\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. We can use a bar chart to visualize the counts of each value.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.figure(figsize=(8, 5))\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\", rwidth=0.8)\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let’s check it out:\n\n# load in the dice rolls data\ndice_rolls_df = pd.read_csv(\"../data/dice_rolls.csv\")\nprint(\"Total number of dice rolls:\", len(dice_rolls_df))\ndice_rolls_df.head(10)\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(dice_rolls_df['rolls'], bins=np.arange(0.5, 7.5, 1), density=True, rwidth=0.8)\nplt.title('Histogram of Dice Rolls')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.xticks(np.arange(1, 7))\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\n\n\n\n\n\nNotice that when the \\(y\\)-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a histogram to visualize the distribution of the data.\nThe histogram is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins.\n\ndf = pd.read_csv(\"../data/airbnb.csv\")\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.dropna(subset=[\"price\"])\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb['price'], bins=50, stat=\"proportion\", edgecolor='black')\nplt.title('Histogram of Airbnb Prices')\nplt.xlabel('Price (USD)')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea under a probability distribution\n\n\n\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. This is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1.\nFor discrete: \\[ \\sum_{x} f(x) = 1 \\] For continuous: \\[ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\]\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable \\(X\\) falls between \\(a\\) and \\(b\\), we can compute the area under the PDF from \\(a\\) to \\(b\\): \\[ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx \\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#expectation",
    "href": "notebooks/lecture-02.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play.\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you’re not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero.\n\n\n\n\n\n\nGambling warning: the house always wins\n\n\n\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n\n\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\mathbb{E}[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\nSo in our roulette example, you can either lose $1 (with 90% probability) or win $9 (with 10% probability). The expectation would be: \\[\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win $10 on one game and lose $1 on the other 9 games. The average outcome is: \\[\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as \\(\\bar{X}\\), and referred to as the sample mean. \\[\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nThis approximation becomes more accurate as the number of samples \\(n\\) increases. We will talk about this more in a future lecture.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "href": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe average is a useful summary of a random variable’s central tendency, but it does not tell us anything about how spread out the values are.\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game – the average amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\n\n\n\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can’t afford to lose all of it on a single game.\nWe need a statistic that captures the typical distance between the values of the random variable and the average value.\n\n\n\n\n\n\nWhy distance from the average?\n\n\n\n\n\nLet’s imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what – the only question is how much you win. Let’s take an example where the payouts still differ by $10: you get $5 if you “lose” and $15 if you “win”.\nIn this case, the expected value is: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n\\] So you can expect to win $6 per game on average.\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\nThe answer is simple: we subtract the average from each value of \\(X\\): \\[X' = X - \\mathbb{E}[X]\\] This gives us a new random variable \\(X'\\) that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\\[\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0\\]\nor more generally: \\[\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0\\]\n\n\n\nSo let’s compute exactly that - the distance from the average. The formula for distance between two vectors \\(x\\) and \\(y\\) is: \\[\nd^2 = \\sum_{i} (x_i - y_i)^2\n\\] where \\(x_i\\) and \\(y_i\\) are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange (\\(a^2 + b^2 = c^2\\)).\nIn our case, we want to compute the distance between the values of the random variable \\(X\\) and the average value \\(\\mathbb{E}[X]\\). \\[\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n\\]\nNow we’re getting somewhere! However, this is adding up all of the squared distances – that means that the more values we have, the larger the distance will be. This is not quite right – instead we want to compute the average distance from the mean in order to get a sense of how spread out the values typically are.\nSo we need to divide by the number of values: \\[\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n\\]\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. If we replace the average with the expectation, we get the formula for the variance of a random variable \\(X\\): \\[\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n\\]\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if \\(X\\) represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the standard deviation:\n\\[\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n\\]\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset: \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\]\n\n\n\n\n\n\nSample variance vs. population variance\n\n\n\n\n\nTechnically, the formula above is an imperfect estimate of the population standard deviation. It’s in general a little bit too small, because the sample mean \\(\\bar{X}\\) does not perfectly represent the population mean \\(\\mathbb{E}[X]\\). We can correct for this by dividing by \\(n-1\\) instead of \\(n\\): \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\] This is called the sample standard deviation.\nWhy is the initial estimate too small? In a small dataset, the sample mean “overfits” the data, meaning it is closer to the individual data points than the true population mean. Let’s think about this in terms of coin flips. If we flip a coin once, the sample mean is either \\(\\hat{X}=0\\) or 1, depending on whether we got heads or tails. But the true population mean is \\(\\mathbb{E}[X]=\\frac{1}{2}\\). If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either \\((1-1)^2\\) or \\((0-0)^2\\)).\nBy contrast, the true (population) standard deviation is \\(\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\\), which is larger than the estimate using the sample mean.\nThis bias in computing the standard deviation gets smaller as the sample size \\(n\\) increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the \\(n-1\\) correction to get a more accurate estimate of the population standard deviation.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#common-probability-distributions",
    "href": "notebooks/lecture-02.html#common-probability-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Common probability distributions",
    "text": "Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. It is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nProbability Function\nParameters\nMean\nVariance\n\n\n\n\nBernoulli\nDiscrete\n\\(f(x) = p^x (1-p)^{1-x}\\)\n\\(p \\in [0, 1]\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\nDiscrete\n\\(f(x) = \\binom{n}{x} p^x (1-p)^{n-x}\\)\n\\(n \\in \\mathbb{N}, p \\in [0, 1]\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nPoisson\nDiscrete\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\nUniform\nContinuous\n\\(f(x) = \\frac{1}{b-a}\\)\n\\(a &lt; b\\)\n\\(\\frac{a+b}{2}\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\n\nNormal\nContinuous\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\nExponential\nContinuous\n\\(f(x) = \\lambda e^{-\\lambda x}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\n\nThe plots below show the probability functions for each of these distributions.\n\nfig, ax = plt.subplots(3, 2, figsize=(12, 12), sharex=False, sharey=True)\nax = ax.flatten()\n# Bernoulli Distribution\nx = np.arange(0, 2)\np = 0.5\nax[0].bar(x, stats.bernoulli.pmf(x, p), width=0.4, color='blue', alpha=0.7)\nax[0].set_title('Bernoulli Distribution (p=0.5)')\nax[0].set_xticks(x)\nax[0].set_xticklabels(['0', '1'])\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Probability')\n\n# Binomial Distribution\nn = 10\nx = np.arange(0, n + 1)\np = 0.5\nax[1].bar(x, stats.binom.pmf(x, n, p), width=0.4, color='green', alpha=0.7)\nax[1].set_title('Binomial Distribution (n=10, p=0.5)')\nax[1].set_xticks(x)\nax[1].set_xlabel('Number of Successes')\nax[1].set_ylabel('Probability')\n\n# Poisson Distribution\nlambda_ = 5\nx = np.arange(0, 15)\nax[2].bar(x, stats.poisson.pmf(x, lambda_), width=0.4, color='orange', alpha=0.7)\nax[2].set_title('Poisson Distribution (λ=5)')\nax[2].set_xticks(x)\nax[2].set_xlabel('Number of Events')\nax[2].set_ylabel('Probability') \n\n# Uniform Distribution\na, b = 0, 10\nx = np.linspace(a, b, 1000)\nax[3].plot(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan')\nax[3].set_title('Uniform Distribution (a=0, b=10)')\nax[3].set_xlabel('Value')\nax[3].set_ylabel('Probability Density')\nax[3].fill_between(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan', alpha=0.2)\n\n# Normal Distribution\nmu, sigma = 0, 1\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nax[4].plot(x, stats.norm.pdf(x, mu, sigma), color='purple')\nax[4].set_title('Normal Distribution (μ=0, σ=1)')\nax[4].set_xlabel('Value')\nax[4].set_ylabel('Probability Density')\nax[4].fill_between(x, stats.norm.pdf(x, mu, sigma), color='purple', alpha=0.2)\n\n# Exponential Distribution\nlambda_ = 1\nx = np.linspace(0, 10, 1000)\nax[5].plot(x, stats.expon.pdf(x, scale=1/lambda_), color='red')\nax[5].set_title('Exponential Distribution (λ=1)')\nax[5].set_xlabel('Value')\nax[5].set_ylabel('Probability Density')\nax[5].fill_between(x, stats.expon.pdf(x, scale=1/lambda_), color='red', alpha=0.2)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#summary",
    "href": "notebooks/lecture-02.html#summary",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. Probability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\nIn particular, we covered:\n\nThe basic definition of probability and how to compute it for simple events.\nThe addition and multiplication rules for calculating probabilities of multiple events.\nThe concept of independence and how it affects probabilities.\nRandom variables and their probability distributions\nThe expectation (or expected value) of a random variable\nVariance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html",
    "href": "notebooks/lecture-00.html",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#welcome",
    "href": "notebooks/lecture-00.html#welcome",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#why-statistics",
    "href": "notebooks/lecture-00.html#why-statistics",
    "title": "Lecture 00",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#description",
    "href": "notebooks/lecture-00.html#description",
    "title": "Lecture 00",
    "section": "Description",
    "text": "Description\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features.\n\n\nCode\n# from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.rename(columns={\"neighbourhood_group\": \"borough\"})\nairbnb = airbnb.dropna(subset=[\"borough\", \"price\", \"long\", \"lat\"])\nairbnb[\"borough\"] = airbnb[\"borough\"].str.lower()\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"manhatan\", \"manhattan\")\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"brookln\", \"brooklyn\")\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n# print the first 5 rows\nairbnb[:5]\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n3\n1002755\nNaN\n85098326012\nunconfirmed\nGarry\nbrooklyn\nClinton Hill\n40.68514\n-73.95976\nUnited States\n...\n$74\n30.0\n270.0\n7/5/2019\n4.64\n4.0\n1.0\n322.0\nNaN\nNaN\n\n\n4\n1003689\nEntire Apt: Spacious Studio/Loft by central park\n92037596077\nverified\nLyndon\nmanhattan\nEast Harlem\n40.79851\n-73.94399\nUnited States\n...\n$41\n10.0\n9.0\n11/19/2018\n0.10\n3.0\n1.0\n289.0\nPlease no smoking in the house, porch or on th...\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\nNow there’s a lot you can do, but let’s start by visualizing the prices of listings.\n\n\nCode\n# plot a histogram of the price column\nplt.figure(figsize=(10, 5))\nplt.hist(airbnb['price'], bins=50)\nplt.title('Prices of Airbnb Listings in NYC')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\nCode\nairbnb[\"price\"].describe()\n\n\ncount    102316.000000\nmean        625.291665\nstd         331.677344\nmin          50.000000\n25%         340.000000\n50%         624.000000\n75%         913.000000\nmax        1200.000000\nName: price, dtype: float64\n\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the geopandas library to plot the locations of listings on a map of New York City.\n\n\nCode\nimport geopandas as gpd\nfrom geodatasets import get_path\n# load the shapefile of NYC neighborhoods\nnyc_neighborhoods = gpd.read_file(get_path('nybb'))\nnyc_neighborhoods = nyc_neighborhoods.to_crs(epsg=4326)  # convert to WGS84\n# plot the neighborhoods with airbnb listings\nnyc_neighborhoods.plot(figsize=(8, 8), color='white', edgecolor='black')\n# plot the airbnb listings on top of the neighborhoods\n# use the 'long' and 'lat' columns to create a GeoDataFrame\nairbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb['long'], airbnb['lat']), crs='EPSG:4326')\n# set the coordinate reference system to WGS84\nairbnb_gdf.plot(ax=plt.gca(), column=\"price\", markersize=3, alpha=0.05, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\nplt.title('Airbnb Listings in NYC')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics only describe the data.\nWhy is this limiting? After all, we like data – it tells us things about the world and it’s objective and quantifiable.\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\nLet’s look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small “sample” or subset of the data?\n\n\nCode\n# separately plot 3 samples of airbnb listings\nfig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(4):\n    # sample 1000 listings\n    sample = airbnb.sample(100, random_state=i)\n    # plot the neighborhoods with airbnb listings\n    nyc_neighborhoods.plot(ax=ax[i], color='white', edgecolor='black')\n    # plot the airbnb listings on top of the neighborhoods\n    # use the 'long' and 'lat' columns to create a GeoDataFrame\n    airbnb_gdf_sample = gpd.GeoDataFrame(sample, geometry=gpd.points_from_xy(sample['long'], sample['lat']), crs='EPSG:4326')\n    # set the coordinate reference system to WGS84\n    airbnb_gdf_sample.plot(ax=ax[i], column=\"price\", markersize=3, alpha=0.8, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\n    ax[i].set_title(f'Airbnb Listings in NYC (Sample {i+1})\\n Average Price: ${sample[\"price\"].mean():.2f}')\n    ax[i].set_xlabel('Longitude')\n    ax[i].set_ylabel('Latitude')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\n\n\n\n\n\nSample vs. Population\n\n\n\n\n\nA population is the entire set of data that you are interested in. A sample is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.\nNote that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.\nOften, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#inference",
    "href": "notebooks/lecture-00.html#inference",
    "title": "Lecture 00",
    "section": "Inference",
    "text": "Inference\nSo what if we want to answer questions about a population based on a sample? This is where inference comes in. Specifically, we want to use the given sample to infer something about the population.\nHow do we do this if we can’t ever see the entire population? The answer is that we need a link which connects the sample to the population – specifically, we can explicitly treat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\nThere is always a DGP\n\n\n\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.\nThere has to be a DGP, even if we don’t know what it is. The DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\nThis is where the model comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess – a guess about where your data come from.\nFor example, we might assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. (So the probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.)\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\n\nCode\n# number of listings per borough\nborough_counts = airbnb['borough'].value_counts().rename_axis('borough').reset_index(name='count')\nborough_counts['borough'] = borough_counts['borough'].str.title()  # capitalize\n# normalize the counts to proportions\nborough_counts['proportion'] = borough_counts['count'] / borough_counts['count'].sum()\n\nplt.figure(figsize=(8, 5))\nplt.bar(borough_counts['borough'], borough_counts['proportion'])\nplt.axhline(y=1/5, color='r', linestyle='--', label='Equal Proportion (20%)')\nplt.title('Proportion of Airbnb Listings per Borough')\nplt.xlabel('Borough', fontsize=14)\nplt.ylabel('Proportion', fontsize=14)\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe inference question we want to ask is roughly:\n“If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\nNotice that this is a question about the probability of the sample, given a certain model of the DGP. In our Airbnb example above, it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\nWhat should we do now? Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. After all, the model is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\n\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n\n\n\n\n\nDon’t try this at home!\n\n\n\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. This requires domain knowledge and subject matter expertise. In the Airbnb example, you would want to know something about the five boroughs of New York City before starting your analysis. Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption (Manhattan sees vastly more tourism than the other boroughs, and Brooklyn and Queens have by far the most residents according to recent census data).\nStatistical inference is a powerful tool, but it is not a substitute for understanding the data and the context in which it was collected. Modeling might be guesswork, but it is best if it is informed guesswork.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#prediction",
    "href": "notebooks/lecture-00.html#prediction",
    "title": "Lecture 00",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data, we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. We won’t go into the details of the model we use here, but the basic idea is that we assume the features of the listing (e.g., price) is related to the probability of it being in a certain borough (perhaps more expensive listings are more likely to be in Manhattan, for example).\n\n\n\n\n\n\nFitting a model\n\n\n\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nThe model for a coin flip, for example, has a single parameter: the probability of landing on heads. If you turn the knob to 0.5, you get a fair coin; if you turn it to 1.0, you get a coin that always lands on heads; if you turn it to 0.0, you get a coin that always lands on tails.\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data.\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport seaborn as sns\n\n# prepare the data for linear regression\nfeature_columns = [\"price\", \"room_type\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"review_rate_number\"]\nairbnb_clean = airbnb.dropna(subset=feature_columns + [\"borough\"])\nX = airbnb_clean[feature_columns]\n# convert categorical variables to lowercase\nX.loc[:, \"room_type\"] = X[\"room_type\"].str.lower()\ny = airbnb_clean[\"borough\"].values.reshape(-1)\n# convert categorical variables to dummy variables\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\nX[\"rating_interaction\"] = X[\"reviews_per_month\"] * X[\"review_rate_number\"]\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n# fit the logistic regression model\nmodel = LogisticRegression(max_iter=1000, solver=\"newton-cholesky\")\nmodel.fit(X_train, y_train)\n# make predictions on the test set\ny_pred = model.predict(X_test)\n# calculate the prediction accuracy\naccuracy = np.mean(y_pred == y_test)\nprint(f\"{40*'='}\")\nprint(f'Prediction Accuracy: {accuracy:.2%}')\nprint(f\"{40*'='}\")\n# calculate the confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=airbnb['borough'].unique())\n# confusion_matrix = [confusion_matrix[i].T for i in range(len(confusion_matrix))] \nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\nax = ax.flatten()\nax[-1].axis('off')  # turn off the last subplot since we have only 5 boroughs\n# plot the confusion matrix for each borough\nfor i, borough in enumerate(airbnb['borough'].unique()):\n    sns.heatmap(confusion_matrix[i], annot=True, fmt='d', cmap='Blues', ax=ax[i], cbar=False)\n    ax[i].set_xlabel('Predicted', fontsize=10)\n    ax[i].set_ylabel('Actual', fontsize=10)\n    ax[i].set_xticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\n    ax[i].set_yticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\nplt.tight_layout()\n_ = plt.show()\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================\n\n\n\n\n\n\n\n\n\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\n\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. For now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. For example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance.\n\n\n\nNow let’s take a look at the distribution of the model’s predictions.\n\n\nCode\n# plot the counts of predicted listings per borough side by side with the actual counts\nimport seaborn as sns\nplot_df = pd.DataFrame({\n    'predicted': pd.Series(y_pred.flatten()),\n    'actual': pd.Series(y_test.flatten())\n})\nplot_df = plot_df.melt(var_name='type', value_name='borough')\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.countplot(x='borough', hue='type', data=plot_df, ax=ax, alpha=0.7)\nsns.move_legend(ax, loc=\"center right\", title=None)\nplt.title('Predicted vs Actual Airbnb Listings per Borough')\nplt.xlabel('Borough')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n\n\n\n\n\nPrediction and inference can interact\n\n\n\n\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#summary",
    "href": "notebooks/lecture-00.html#summary",
    "title": "Lecture 00",
    "section": "Summary",
    "text": "Summary\nIn this introductory lecture we talked about the 3 objectives of data analysis: description, inference, and prediction. Hopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. Don’t worry, we’ll get there!\nNext up, we’ll cover some of the basic programming concepts that are important for data science. After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. From there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learned any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "slides/lecture-09-slides.html#recap-linear-regression",
    "href": "slides/lecture-09-slides.html#recap-linear-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Recap: Linear Regression",
    "text": "Recap: Linear Regression\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): slope\n\\(\\epsilon\\): error term (random noise)\n\n\nWe use data to estimate \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)"
  },
  {
    "objectID": "slides/lecture-09-slides.html#sampling-variability-in-regression",
    "href": "slides/lecture-09-slides.html#sampling-variability-in-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Sampling Variability in Regression",
    "text": "Sampling Variability in Regression\nDifferent samples → different estimates of \\(\\beta\\)\n\nThis is the same sampling variability we’ve discussed throughout the course!"
  },
  {
    "objectID": "slides/lecture-09-slides.html#hypothesis-testing-in-regression",
    "href": "slides/lecture-09-slides.html#hypothesis-testing-in-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Hypothesis Testing in Regression",
    "text": "Hypothesis Testing in Regression\nCommon question: Is there a relationship between \\(X\\) and \\(Y\\)?\n\nHypotheses:\n\n\\(H_0\\): \\(\\beta_1 = 0\\) (no relationship)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\) (there is a relationship)"
  },
  {
    "objectID": "slides/lecture-09-slides.html#simulating-the-null-distribution",
    "href": "slides/lecture-09-slides.html#simulating-the-null-distribution",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Simulating the Null Distribution",
    "text": "Simulating the Null Distribution\nHow can we generate \\(\\hat{\\beta}_1\\) values under \\(H_0\\)?\n\nPermutation test: Shuffle \\(Y\\) values to break the \\(X\\)-\\(Y\\) relationship"
  },
  {
    "objectID": "slides/lecture-09-slides.html#permutation-test-for-regression",
    "href": "slides/lecture-09-slides.html#permutation-test-for-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Permutation Test for Regression",
    "text": "Permutation Test for Regression\n\n\nCode\ndef permutation_test_regression(data, x_col, y_col, n_permutations=1000, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    \n    # Observed slope\n    model = smf.ols(f'{y_col} ~ {x_col}', data=data).fit()\n    observed_beta = model.params[x_col]\n    \n    # Permutation distribution\n    perm_betas = []\n    for _ in range(n_permutations):\n        y_perm = rng.permutation(data[y_col])\n        perm_model = smf.ols(f'{y_col} ~ {x_col}', \n                              data=data.assign(**{y_col: y_perm})).fit()\n        perm_betas.append(perm_model.params[x_col])\n    \n    perm_betas = np.array(perm_betas)\n    p_value = np.mean(np.abs(perm_betas) &gt;= np.abs(observed_beta))\n    \n    return observed_beta, perm_betas, p_value"
  },
  {
    "objectID": "slides/lecture-09-slides.html#example",
    "href": "slides/lecture-09-slides.html#example",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Example",
    "text": "Example\n\n\nObserved slope: 0.5596\nPermutation p-value: 0.0000"
  },
  {
    "objectID": "slides/lecture-09-slides.html#permutation-vs-parametric",
    "href": "slides/lecture-09-slides.html#permutation-vs-parametric",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Permutation vs Parametric",
    "text": "Permutation vs Parametric"
  },
  {
    "objectID": "slides/lecture-09-slides.html#parametric-results-match",
    "href": "slides/lecture-09-slides.html#parametric-results-match",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Parametric Results Match!",
    "text": "Parametric Results Match!\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.443\nModel:                            OLS   Adj. R-squared:                  0.437\nNo. Observations:                 100   F-statistic:                     77.87\nCovariance Type:            nonrobust   Prob (F-statistic):           4.29e-14\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0023      0.049     -0.047      0.962      -0.100       0.095\nx              0.5596      0.063      8.824      0.000       0.434       0.685\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides/lecture-09-slides.html#confidence-intervals-for-regression",
    "href": "slides/lecture-09-slides.html#confidence-intervals-for-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Confidence Intervals for Regression",
    "text": "Confidence Intervals for Regression\nUse bootstrap to estimate CI for slope:\n\n\nCode\ndef bootstrap_regression_ci(data, x_col, y_col, n_bootstraps=1000, alpha=0.05, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    \n    n = len(data)\n    coefs = []\n    for _ in range(n_bootstraps):\n        sample = data.sample(n, replace=True)\n        model = smf.ols(f\"{y_col} ~ {x_col}\", data=sample).fit()\n        coefs.append(model.params[x_col])\n    \n    coefs = np.array(coefs)\n    lower = np.percentile(coefs, 100 * alpha / 2)\n    upper = np.percentile(coefs, 100 * (1 - alpha / 2))\n    return coefs, lower, upper"
  },
  {
    "objectID": "slides/lecture-09-slides.html#bootstrap-ci-example",
    "href": "slides/lecture-09-slides.html#bootstrap-ci-example",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Bootstrap CI Example",
    "text": "Bootstrap CI Example\n\n\nBootstrap 95% CI for slope: [1.362, 1.758]\nTrue slope: 1.5\n\nParametric 95% CI: [1.352, 1.761]"
  },
  {
    "objectID": "slides/lecture-09-slides.html#bootstrap-distribution",
    "href": "slides/lecture-09-slides.html#bootstrap-distribution",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution"
  },
  {
    "objectID": "slides/lecture-09-slides.html#multiple-regression",
    "href": "slides/lecture-09-slides.html#multiple-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWhat if we have multiple predictors?\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon\\]\n\nThis is multiple linear regression"
  },
  {
    "objectID": "slides/lecture-09-slides.html#example-student-performance",
    "href": "slides/lecture-09-slides.html#example-student-performance",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Example: Student Performance",
    "text": "Example: Student Performance\nPredicting student grades from various factors:\n\nParent education\nStudy time\nFamily relationships\nAnd more…"
  },
  {
    "objectID": "slides/lecture-09-slides.html#the-data",
    "href": "slides/lecture-09-slides.html#the-data",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "The Data",
    "text": "The Data\n\n\n\n\n\n\n\n\n\nMedu\nFedu\nstudytime\nG3\n\n\n\n\n0\n4\n4\n2\n6\n\n\n1\n1\n1\n2\n6\n\n\n2\n1\n1\n2\n10\n\n\n3\n4\n2\n3\n15\n\n\n4\n3\n3\n2\n10"
  },
  {
    "objectID": "slides/lecture-09-slides.html#simple-regression-fathers-education",
    "href": "slides/lecture-09-slides.html#simple-regression-fathers-education",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Simple Regression: Father’s Education",
    "text": "Simple Regression: Father’s Education\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     G3   R-squared:                       0.023\nModel:                            OLS   Adj. R-squared:                  0.021\nNo. Observations:                 395   F-statistic:                     9.352\nCovariance Type:            nonrobust   Prob (F-statistic):            0.00238\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.7967      0.576     15.264      0.000       7.664       9.930\nFedu           0.6419      0.210      3.058      0.002       0.229       1.055\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides/lecture-09-slides.html#but-wait",
    "href": "slides/lecture-09-slides.html#but-wait",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "But Wait…",
    "text": "But Wait…\nWhat about mother’s education?\n\nLet’s look at how these variables relate:\n\n\n\n\n\n\n\n\n\nFedu\nMedu\nG3\n\n\n\n\nFedu\n1.000\n0.623\n0.152\n\n\nMedu\n0.623\n1.000\n0.217\n\n\nG3\n0.152\n0.217\n1.000"
  },
  {
    "objectID": "slides/lecture-09-slides.html#the-problem-multicollinearity",
    "href": "slides/lecture-09-slides.html#the-problem-multicollinearity",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "The Problem: Multicollinearity",
    "text": "The Problem: Multicollinearity\nFather’s and mother’s education are correlated!\n\nStudents with educated fathers often have educated mothers\nHard to separate individual effects"
  },
  {
    "objectID": "slides/lecture-09-slides.html#multiple-regression-to-the-rescue",
    "href": "slides/lecture-09-slides.html#multiple-regression-to-the-rescue",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Multiple Regression to the Rescue",
    "text": "Multiple Regression to the Rescue\nInclude both predictors:\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     G3   R-squared:                       0.048\nModel:                            OLS   Adj. R-squared:                  0.043\nNo. Observations:                 395   F-statistic:                     9.802\nCovariance Type:            nonrobust   Prob (F-statistic):           7.01e-05\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.8205      0.648     12.073      0.000       6.547       9.094\nFedu           0.1176      0.265      0.443      0.658      -0.404       0.639\nMedu           0.8359      0.264      3.168      0.002       0.317       1.355\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides/lecture-09-slides.html#interpretation",
    "href": "slides/lecture-09-slides.html#interpretation",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Interpretation",
    "text": "Interpretation\nBefore (Fedu only): significant effect\nAfter (Fedu + Medu): Fedu effect shrinks, Medu dominates\n\nMultiple regression controls for other variables!"
  },
  {
    "objectID": "slides/lecture-09-slides.html#what-multiple-regression-does",
    "href": "slides/lecture-09-slides.html#what-multiple-regression-does",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "What Multiple Regression Does",
    "text": "What Multiple Regression Does\nFor Fedu coefficient:\n\nPredict G3 from Medu → get residuals (part of G3 not explained by Medu)\nPredict Fedu from Medu → get residuals (part of Fedu not explained by Medu)\n\nRegress G3 residuals on Fedu residuals\n\n\nThe coefficient shows Fedu’s effect after accounting for Medu"
  },
  {
    "objectID": "slides/lecture-09-slides.html#visualizing-the-process",
    "href": "slides/lecture-09-slides.html#visualizing-the-process",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Visualizing the Process",
    "text": "Visualizing the Process"
  },
  {
    "objectID": "slides/lecture-09-slides.html#causality-warning",
    "href": "slides/lecture-09-slides.html#causality-warning",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Causality Warning!",
    "text": "Causality Warning!\nCorrelation (even in regression) ≠ Causation\n\nTwo main issues:\n\nConfounding variables: A third variable causes both X and Y\nReverse causation: Y might cause X, not vice versa"
  },
  {
    "objectID": "slides/lecture-09-slides.html#confounding-example",
    "href": "slides/lecture-09-slides.html#confounding-example",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Confounding Example",
    "text": "Confounding Example\nDoes mother’s education cause better grades?\n\nOr do they share a common cause (e.g., family wealth)?\n\nWealth → can afford education for mom\nWealth → can afford tutoring for student"
  },
  {
    "objectID": "slides/lecture-09-slides.html#the-reverse-regression",
    "href": "slides/lecture-09-slides.html#the-reverse-regression",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "The “Reverse” Regression",
    "text": "The “Reverse” Regression\nWhat if we regress Medu on G3?\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Medu   R-squared:                       0.404\nModel:                            OLS   Adj. R-squared:                  0.401\nNo. Observations:                 395   F-statistic:                     132.8\nCovariance Type:            nonrobust   Prob (F-statistic):           9.00e-45\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.9051      0.136      6.658      0.000       0.638       1.172\nFedu           0.6080      0.040     15.319      0.000       0.530       0.686\nG3             0.0299      0.009      3.168      0.002       0.011       0.048\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThis suggests G3 “causes” Medu — obviously wrong!"
  },
  {
    "objectID": "slides/lecture-09-slides.html#solutions-for-causality",
    "href": "slides/lecture-09-slides.html#solutions-for-causality",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Solutions for Causality",
    "text": "Solutions for Causality\n\nInclude confounders in the model (if you can measure them)\nRandomized controlled trials (gold standard)\nAdvanced causal inference methods (beyond this course)"
  },
  {
    "objectID": "slides/lecture-09-slides.html#summary",
    "href": "slides/lecture-09-slides.html#summary",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Summary",
    "text": "Summary\nRegression Inference\n\nUse permutation tests or bootstrap for hypothesis testing\nSame principles as before, applied to regression\n\n\nMultiple Regression\n\nInclude multiple predictors simultaneously\nControls for confounding between predictors\n\n\n\nCausality\n\nRegression shows association, not causation\nNeed additional assumptions or experimental design"
  },
  {
    "objectID": "slides/lecture-09-slides.html#course-summary",
    "href": "slides/lecture-09-slides.html#course-summary",
    "title": "Lecture 09: Regression Inference and Multiple Regression",
    "section": "Course Summary",
    "text": "Course Summary\nWe’ve covered a complete statistical toolkit:\n\nProbability — foundation for reasoning about uncertainty\nSampling & Simulation — generating data from models\nStatistical Models — DGPs, LLN, CLT\nHypothesis Testing — p-values, significance\nConfidence Intervals — quantifying uncertainty\nBootstrapping — no distributional assumptions\nPermutation Tests — the universal test\nRegression — prediction and inference\n\n\nKey theme: Quantifying and communicating uncertainty!"
  },
  {
    "objectID": "slides/lecture-07-slides.html#recap",
    "href": "slides/lecture-07-slides.html#recap",
    "title": "Lecture 07: Permutation Tests",
    "section": "Recap",
    "text": "Recap\nBootstrap:\n\nResample with replacement from data\nEstimate distribution of any statistic\nBuild confidence intervals\n\n\nToday: Permutation tests — another powerful resampling method"
  },
  {
    "objectID": "slides/lecture-07-slides.html#permutation-tests",
    "href": "slides/lecture-07-slides.html#permutation-tests",
    "title": "Lecture 07: Permutation Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nLike bootstrap, permutation tests are non-parametric:\n\nNo assumptions about underlying distribution\nRely on resampling\n\n\nKey difference: resample without replacement"
  },
  {
    "objectID": "slides/lecture-07-slides.html#the-setup",
    "href": "slides/lecture-07-slides.html#the-setup",
    "title": "Lecture 07: Permutation Tests",
    "section": "The Setup",
    "text": "The Setup\nWe have two samples \\(X\\) and \\(Y\\).\nQuestion: Do they come from different distributions?\n\nNull hypothesis (\\(H_0\\)): Both samples come from the same distribution."
  },
  {
    "objectID": "slides/lecture-07-slides.html#the-logic",
    "href": "slides/lecture-07-slides.html#the-logic",
    "title": "Lecture 07: Permutation Tests",
    "section": "The Logic",
    "text": "The Logic\nIf \\(H_0\\) is true:\n\n\\(X\\) and \\(Y\\) are drawn from the same distribution\nWe can combine them: \\(Z = X \\cup Y\\)\nAny random split of \\(Z\\) is just as valid as the original\n\n\nKey insight: Under \\(H_0\\), the labels (X vs Y) are exchangeable!"
  },
  {
    "objectID": "slides/lecture-07-slides.html#permutation-test-algorithm",
    "href": "slides/lecture-07-slides.html#permutation-test-algorithm",
    "title": "Lecture 07: Permutation Tests",
    "section": "Permutation Test Algorithm",
    "text": "Permutation Test Algorithm\n\nCompute observed test statistic (e.g., difference in means)\nCombine samples: \\(Z = X \\cup Y\\)\nRandomly permute (shuffle) \\(Z\\)\nSplit into new “X” and “Y” of original sizes\nCompute test statistic on shuffled data\nRepeat steps 3-5 many times\np-value = proportion of permuted stats ≥ observed"
  },
  {
    "objectID": "slides/lecture-07-slides.html#permutation-test-code",
    "href": "slides/lecture-07-slides.html#permutation-test-code",
    "title": "Lecture 07: Permutation Tests",
    "section": "Permutation Test Code",
    "text": "Permutation Test Code\n\n\nCode\ndef permutation_test(test_func, x, y, num_permutations=10000, rng=None, one_sided=True):\n    observed_stat = test_func(x, y)\n    combined = np.concatenate([x, y])\n    count = 0\n    if rng is None:\n        rng = np.random.default_rng()\n    \n    for _ in range(num_permutations):\n        permuted = rng.permutation(combined)\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n        permuted_stat = test_func(x_perm, y_perm)\n        \n        if one_sided:\n            if permuted_stat &gt;= observed_stat:\n                count += 1\n        else:\n            if np.abs(permuted_stat) &gt;= np.abs(observed_stat):\n                count += 1\n    \n    return count / num_permutations"
  },
  {
    "objectID": "slides/lecture-07-slides.html#application-nba-scoring",
    "href": "slides/lecture-07-slides.html#application-nba-scoring",
    "title": "Lecture 07: Permutation Tests",
    "section": "Application: NBA Scoring",
    "text": "Application: NBA Scoring\nIs SGA a better scorer than Giannis?\n\n\nPermutation test p-value: 0.0336"
  },
  {
    "objectID": "slides/lecture-07-slides.html#bootstrap-vs-permutation",
    "href": "slides/lecture-07-slides.html#bootstrap-vs-permutation",
    "title": "Lecture 07: Permutation Tests",
    "section": "Bootstrap vs Permutation",
    "text": "Bootstrap vs Permutation\nBoth work, but permutation tests are often more powerful.\n\nPower = probability of correctly rejecting \\(H_0\\) when it’s false\n\n\nA more powerful test detects true effects more reliably."
  },
  {
    "objectID": "slides/lecture-07-slides.html#there-is-really-only-one-test",
    "href": "slides/lecture-07-slides.html#there-is-really-only-one-test",
    "title": "Lecture 07: Permutation Tests",
    "section": "There Is Really Only One Test!",
    "text": "There Is Really Only One Test!\nAll statistical tests follow the same pattern:\n\nCompute a test statistic on observed data\nChoose a null hypothesis / model\nGenerate null distribution (analytically or by simulation)\nCompare observed statistic to null distribution → p-value"
  },
  {
    "objectID": "slides/lecture-07-slides.html#named-tests-are-special-cases",
    "href": "slides/lecture-07-slides.html#named-tests-are-special-cases",
    "title": "Lecture 07: Permutation Tests",
    "section": "Named Tests Are Special Cases",
    "text": "Named Tests Are Special Cases\nMost “named” tests (t-test, chi-square, ANOVA, etc.) are just:\n\nSpecific test statistics\nSpecific null distributions\nOften derived analytically (before computers!)\n\n\nSimulation-based methods (bootstrap, permutation) are more flexible!"
  },
  {
    "objectID": "slides/lecture-07-slides.html#example-two-sample-t-test",
    "href": "slides/lecture-07-slides.html#example-two-sample-t-test",
    "title": "Lecture 07: Permutation Tests",
    "section": "Example: Two-Sample t-test",
    "text": "Example: Two-Sample t-test\nThe t-test compares means of two groups.\n\nAssumption: Errors are normally distributed\n\n\nUnder this assumption, the test statistic follows a t-distribution."
  },
  {
    "objectID": "slides/lecture-07-slides.html#t-distribution-vs-normal",
    "href": "slides/lecture-07-slides.html#t-distribution-vs-normal",
    "title": "Lecture 07: Permutation Tests",
    "section": "t-Distribution vs Normal",
    "text": "t-Distribution vs Normal"
  },
  {
    "objectID": "slides/lecture-07-slides.html#t-distribution-properties",
    "href": "slides/lecture-07-slides.html#t-distribution-properties",
    "title": "Lecture 07: Permutation Tests",
    "section": "t-Distribution Properties",
    "text": "t-Distribution Properties\n\nHeavier tails than normal (more extreme values)\nConverges to normal as sample size increases\nDegrees of freedom (df) = \\(n_1 + n_2 - 2\\) for two samples"
  },
  {
    "objectID": "slides/lecture-07-slides.html#comparing-methods",
    "href": "slides/lecture-07-slides.html#comparing-methods",
    "title": "Lecture 07: Permutation Tests",
    "section": "Comparing Methods",
    "text": "Comparing Methods\nLet’s compare:\n\nParametric t-test (using scipy)\nManual calculation\nSampling from t-distribution\nPermutation test"
  },
  {
    "objectID": "slides/lecture-07-slides.html#the-comparison",
    "href": "slides/lecture-07-slides.html#the-comparison",
    "title": "Lecture 07: Permutation Tests",
    "section": "The Comparison",
    "text": "The Comparison\n\n\nCode\nrng = np.random.default_rng(43)\nsamples_a = rng.uniform(low=-1, high=3, size=10)\nsamples_b = rng.uniform(low=-3, high=1, size=10)\n\n# 1. Parametric t-test\nt_result = stats.ttest_ind(samples_a, samples_b, equal_var=True)\nprint(f\"Parametric t-test p-value: {t_result.pvalue:.6f}\")\n\n# 2. Manual t-statistic\ndiff_means = np.mean(samples_a) - np.mean(samples_b)\npooled_var = (np.var(samples_a, ddof=1) + np.var(samples_b, ddof=1))\npooled_std_error = np.sqrt(pooled_var / len(samples_a))\nt_stat = diff_means / pooled_std_error\nt_abs = np.abs(t_stat)\ndf = len(samples_a) + len(samples_b) - 2\np_value_pdf = 2 * (1 - stats.t.cdf(t_abs, df=df))\nprint(f\"Manual calculation p-value: {p_value_pdf:.6f}\")\n\n# 3. Sampling from t-distribution\nt_samples = rng.standard_t(df=df, size=10000)\np_value_sim = np.mean(np.abs(t_samples) &gt;= t_abs)\nprint(f\"Simulated t-distribution p-value: {p_value_sim:.6f}\")\n\n\nParametric t-test p-value: 0.014783\nManual calculation p-value: 0.014783\nSimulated t-distribution p-value: 0.014400"
  },
  {
    "objectID": "slides/lecture-07-slides.html#permutation-t-test",
    "href": "slides/lecture-07-slides.html#permutation-t-test",
    "title": "Lecture 07: Permutation Tests",
    "section": "Permutation t-Test",
    "text": "Permutation t-Test\n\n\nCode\ndef permutation_test_t(x, y, num_permutations=10000, rng=None):\n    observed_stat = stats.ttest_ind(x, y, equal_var=True).statistic\n    combined = np.concatenate([x, y])\n    if rng is None:\n        rng = np.random.default_rng()\n    \n    permuted_stats = []\n    for _ in range(num_permutations):\n        permuted = rng.permutation(combined)\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n        permuted_stat = stats.ttest_ind(x_perm, y_perm, equal_var=True).statistic\n        permuted_stats.append(permuted_stat)\n    \n    permuted_stats = np.array(permuted_stats)\n    p_value = np.mean(np.abs(permuted_stats) &gt;= np.abs(observed_stat))\n    return permuted_stats, p_value\n\npermuted_stats, p_value_perm = permutation_test_t(samples_a, samples_b, rng=rng)\nprint(f\"Permutation test p-value: {p_value_perm:.6f}\")\n\n\nPermutation test p-value: 0.015700"
  },
  {
    "objectID": "slides/lecture-07-slides.html#all-methods-agree",
    "href": "slides/lecture-07-slides.html#all-methods-agree",
    "title": "Lecture 07: Permutation Tests",
    "section": "All Methods Agree!",
    "text": "All Methods Agree!\n\n\n\nMethod\np-value\n\n\n\n\nParametric t-test\n0.0148\n\n\nManual calculation\n0.0148\n\n\nSimulated t-distribution\n~0.014\n\n\nPermutation test\n~0.016\n\n\n\n\nThey’re all the same test, just computed differently!"
  },
  {
    "objectID": "slides/lecture-07-slides.html#visualizing-the-agreement",
    "href": "slides/lecture-07-slides.html#visualizing-the-agreement",
    "title": "Lecture 07: Permutation Tests",
    "section": "Visualizing the Agreement",
    "text": "Visualizing the Agreement"
  },
  {
    "objectID": "slides/lecture-07-slides.html#why-this-matters",
    "href": "slides/lecture-07-slides.html#why-this-matters",
    "title": "Lecture 07: Permutation Tests",
    "section": "Why This Matters",
    "text": "Why This Matters\nThe null distributions are nearly identical!\n\nThis is why all the tests give similar p-values.\n\n\nKey insight: Named tests are just specific implementations of the general hypothesis testing framework."
  },
  {
    "objectID": "slides/lecture-07-slides.html#when-to-use-what",
    "href": "slides/lecture-07-slides.html#when-to-use-what",
    "title": "Lecture 07: Permutation Tests",
    "section": "When to Use What?",
    "text": "When to Use What?\nParametric tests (t-test, etc.):\n\nFast (no simulation needed)\nWell-understood mathematically\nRequire distributional assumptions\n\n\nSimulation-based tests (permutation, bootstrap):\n\nMore flexible\nNo distributional assumptions\nComputationally intensive\nUseful for complex statistics"
  },
  {
    "objectID": "slides/lecture-07-slides.html#summary",
    "href": "slides/lecture-07-slides.html#summary",
    "title": "Lecture 07: Permutation Tests",
    "section": "Summary",
    "text": "Summary\nPermutation Tests\n\nShuffle labels under null hypothesis\nCompare observed statistic to permuted distribution\nNon-parametric and powerful\n\n\nThe Unified Framework\n\nTest statistic\nNull distribution (analytical or simulated)\np-value\n\n\n\nAll statistical tests are variations of this pattern!"
  },
  {
    "objectID": "slides/lecture-07-slides.html#next-time",
    "href": "slides/lecture-07-slides.html#next-time",
    "title": "Lecture 07: Permutation Tests",
    "section": "Next Time",
    "text": "Next Time\nLinear Regression\n\nPredicting outcomes from features\nQuantifying relationships in data\nConnecting inference to prediction"
  },
  {
    "objectID": "slides/lecture-05-slides.html#recap",
    "href": "slides/lecture-05-slides.html#recap",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Recap",
    "text": "Recap\n\nAll data comes from a data generating process (DGP)\nStatistical models describe the probability distribution of data\nLLN: Sample means converge to true means\nCLT: Sample means become normally distributed\n\n\nToday: How to use these ideas to test claims about data"
  },
  {
    "objectID": "slides/lecture-05-slides.html#the-role-of-inference",
    "href": "slides/lecture-05-slides.html#the-role-of-inference",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "The Role of Inference",
    "text": "The Role of Inference\nDrawing general conclusions (about a population) from specific observations (a sample)\n\nNow that we have the building blocks, we can start making statistical inferences."
  },
  {
    "objectID": "slides/lecture-05-slides.html#the-burden-of-proof",
    "href": "slides/lecture-05-slides.html#the-burden-of-proof",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "The Burden of Proof",
    "text": "The Burden of Proof\nHow can we use statistics to “prove” something?\n\nPeople often cite data as evidence:\n\n“Unemployment is lower than 4 years ago”\n“Player X has the highest scoring average”\n\n\n\nThese statements can sound iron-clad, but they often:\n\nIgnore complexity and uncertainty\nCherry-pick supporting data\nHide how data was collected/analyzed"
  },
  {
    "objectID": "slides/lecture-05-slides.html#example-unemployment-data",
    "href": "slides/lecture-05-slides.html#example-unemployment-data",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Example: Unemployment Data",
    "text": "Example: Unemployment Data\nA politician claims their policies reduced unemployment.\nEvidence: “Unemployment rate is lower than 4 years ago”"
  },
  {
    "objectID": "slides/lecture-05-slides.html#unemployment-short-view",
    "href": "slides/lecture-05-slides.html#unemployment-short-view",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Unemployment: Short View",
    "text": "Unemployment: Short View"
  },
  {
    "objectID": "slides/lecture-05-slides.html#unemployment-the-bigger-picture",
    "href": "slides/lecture-05-slides.html#unemployment-the-bigger-picture",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Unemployment: The Bigger Picture",
    "text": "Unemployment: The Bigger Picture"
  },
  {
    "objectID": "slides/lecture-05-slides.html#what-the-data-really-shows",
    "href": "slides/lecture-05-slides.html#what-the-data-really-shows",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "What the Data Really Shows",
    "text": "What the Data Really Shows\nThe politician’s claim is misleading:\n\nUnemployment spiked in 2020 (COVID-19 pandemic)\nBy 2021, it wasn’t back to pre-pandemic levels\nThe “decrease” is recovery, not policy success\n\n\nKey lesson: Always be skeptical about data presented as evidence"
  },
  {
    "objectID": "slides/lecture-05-slides.html#sources-of-uncertainty",
    "href": "slides/lecture-05-slides.html#sources-of-uncertainty",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Sources of Uncertainty",
    "text": "Sources of Uncertainty\n\nAlternative explanations: External factors (pandemic) vs. claimed cause (policy)\nSampling variability: Data is collected from samples, not whole populations\nCherry-picking: Different time periods → different conclusions\n\n\nHow do we know if a change is real or just random fluctuation?"
  },
  {
    "objectID": "slides/lecture-05-slides.html#the-burden-of-proof-1",
    "href": "slides/lecture-05-slides.html#the-burden-of-proof-1",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "The Burden of Proof",
    "text": "The Burden of Proof\nThe person making the claim must provide evidence.\n\nIn statistics, “beyond reasonable doubt” means:\n\nAlternative explanations are so unlikely\nA reasonable person would ignore them\nBut this is always a subjective judgment!"
  },
  {
    "objectID": "slides/lecture-05-slides.html#formalizing-hypotheses",
    "href": "slides/lecture-05-slides.html#formalizing-hypotheses",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Formalizing Hypotheses",
    "text": "Formalizing Hypotheses\nMake claims specific and testable:\n\nNull hypothesis (\\(H_0\\))\n\nThe status quo or baseline assumption\nOften a statement of “no effect” or “no difference”\n\n\n\nAlternative hypothesis (\\(H_1\\))\n\nThe claim we want to test\nLogical complement of \\(H_0\\) (true when \\(H_0\\) is false)"
  },
  {
    "objectID": "slides/lecture-05-slides.html#the-p-value",
    "href": "slides/lecture-05-slides.html#the-p-value",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "The p-value",
    "text": "The p-value\nDefinition: The probability of observing data at least as extreme as what we observed, assuming \\(H_0\\) is true.\n\nIf \\(p\\) is very small:\n\nData is unlikely under \\(H_0\\)\nWe reject \\(H_0\\) in favor of \\(H_1\\)"
  },
  {
    "objectID": "slides/lecture-05-slides.html#visualizing-the-p-value",
    "href": "slides/lecture-05-slides.html#visualizing-the-p-value",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value"
  },
  {
    "objectID": "slides/lecture-05-slides.html#statistical-significance",
    "href": "slides/lecture-05-slides.html#statistical-significance",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe p-value is compared to a threshold \\(\\alpha\\) (your risk tolerance)\n\n\nIf \\(p &lt; \\alpha\\): result is statistically significant\nIf \\(p \\geq \\alpha\\): result is not statistically significant\n\n\n\nCommon choice: \\(\\alpha = 0.05\\) (5% chance of wrongly rejecting \\(H_0\\))\n\n\nBetter practice: Report the actual p-value, not just “significant” or “not”"
  },
  {
    "objectID": "slides/lecture-05-slides.html#important-notes",
    "href": "slides/lecture-05-slides.html#important-notes",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Important Notes",
    "text": "Important Notes\n\n\\(H_0\\) and \\(H_1\\) must be mutually exclusive (one true → other false)\n\\(H_0\\) and \\(H_1\\) must be exhaustive (one must be true)\n\n\nHypothesis testing does NOT provide absolute certainty:\n\nFailing to reject \\(H_0\\) ≠ \\(H_0\\) is true\nRejecting \\(H_0\\) ≠ \\(H_1\\) is definitely true"
  },
  {
    "objectID": "slides/lecture-05-slides.html#worked-example-the-rigged-coin",
    "href": "slides/lecture-05-slides.html#worked-example-the-rigged-coin",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Worked Example: The Rigged Coin",
    "text": "Worked Example: The Rigged Coin\nYour roommate accuses you of using a rigged coin.\n\nYou always choose tails\nResult: 3 heads, 7 tails in 10 flips\n\n\nCan we test this claim?"
  },
  {
    "objectID": "slides/lecture-05-slides.html#setting-up-the-hypotheses",
    "href": "slides/lecture-05-slides.html#setting-up-the-hypotheses",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Setting Up the Hypotheses",
    "text": "Setting Up the Hypotheses\n\n\\(H_0\\) (null): The coin is fair (\\(p \\geq 0.5\\))\n\\(H_1\\) (alternative): The coin is rigged toward tails (\\(p &lt; 0.5\\))\n\n\nThis is a one-sided test — we only care if the coin favors tails."
  },
  {
    "objectID": "slides/lecture-05-slides.html#simulating-the-null-distribution",
    "href": "slides/lecture-05-slides.html#simulating-the-null-distribution",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Simulating the Null Distribution",
    "text": "Simulating the Null Distribution\nStrategy: Simulate flipping a fair coin many times, see how often we get results as extreme as observed.\n\nThis gives us a sampling distribution under \\(H_0\\)."
  },
  {
    "objectID": "slides/lecture-05-slides.html#simulation-code",
    "href": "slides/lecture-05-slides.html#simulation-code",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Simulation Code",
    "text": "Simulation Code\n\n\nCode\nrng = np.random.default_rng(56)\nn_sims = 10000\nn_flips = 10\nprob_heads = 0.5\nproportions = []\n\nfor i in range(n_sims):\n    flips = rng.binomial(n=n_flips, p=prob_heads)\n    proportions.append(flips / n_flips)\n\nproportions = np.array(proportions)\np_value = np.mean(proportions &lt;= 0.3)  # proportion ≤ observed\nprint(f\"p-value: {p_value:.4f}\")\n\n\np-value: 0.1702"
  },
  {
    "objectID": "slides/lecture-05-slides.html#visualizing-the-result",
    "href": "slides/lecture-05-slides.html#visualizing-the-result",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Visualizing the Result",
    "text": "Visualizing the Result"
  },
  {
    "objectID": "slides/lecture-05-slides.html#conclusion",
    "href": "slides/lecture-05-slides.html#conclusion",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Conclusion",
    "text": "Conclusion\np-value ≈ 0.17 (17%)\n\nInterpretation: Getting 3 heads in 10 flips happens ~17% of the time with a fair coin.\n\n\nVerdict: Not enough evidence to support the roommate’s claim!"
  },
  {
    "objectID": "slides/lecture-05-slides.html#a-more-complex-example-nba-mvp",
    "href": "slides/lecture-05-slides.html#a-more-complex-example-nba-mvp",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "A More Complex Example: NBA MVP",
    "text": "A More Complex Example: NBA MVP\n“Shai Gilgeous-Alexander (SGA) is the best scorer in the NBA”\n\nThis is too vague to test! Let’s make it specific:\n\nFocus on points per game (PPG)\nCompare to next-best scorer (Giannis Antetokounmpo)"
  },
  {
    "objectID": "slides/lecture-05-slides.html#formalizing-the-hypothesis",
    "href": "slides/lecture-05-slides.html#formalizing-the-hypothesis",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Formalizing the Hypothesis",
    "text": "Formalizing the Hypothesis\nAssumptions:\n\nEach player’s scoring follows a distribution with mean \\(\\mu\\)\nBy CLT, the average PPG is approximately normal\n\n\nHypotheses:\n\n\\(H_0\\): \\(\\mu_{\\text{SGA}} \\leq \\mu_{\\text{Giannis}}\\)\n\\(H_1\\): \\(\\mu_{\\text{SGA}} &gt; \\mu_{\\text{Giannis}}\\)"
  },
  {
    "objectID": "slides/lecture-05-slides.html#the-data",
    "href": "slides/lecture-05-slides.html#the-data",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "The Data",
    "text": "The Data\n\n\nSGA: 32.68 PPG (SD: 7.54)\nGiannis: 30.39 PPG"
  },
  {
    "objectID": "slides/lecture-05-slides.html#scoring-distributions",
    "href": "slides/lecture-05-slides.html#scoring-distributions",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Scoring Distributions",
    "text": "Scoring Distributions"
  },
  {
    "objectID": "slides/lecture-05-slides.html#using-the-clt",
    "href": "slides/lecture-05-slides.html#using-the-clt",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Using the CLT",
    "text": "Using the CLT\nUnder \\(H_0\\): SGA’s true scoring rate = Giannis’s observed PPG (30.4)\n\nQuestion: What’s the probability SGA scores ≥32.7 PPG if his true ability is 30.4?\n\n\np-value from CLT: 0.003910"
  },
  {
    "objectID": "slides/lecture-05-slides.html#visualizing-the-null-distribution",
    "href": "slides/lecture-05-slides.html#visualizing-the-null-distribution",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Visualizing the Null Distribution",
    "text": "Visualizing the Null Distribution"
  },
  {
    "objectID": "slides/lecture-05-slides.html#simulation-approach",
    "href": "slides/lecture-05-slides.html#simulation-approach",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Simulation Approach",
    "text": "Simulation Approach\nWe can also simulate without assuming normality!\n\n\nCode\nrng = np.random.default_rng(42)\nn_sims = 5000\nshai_simulated_ppg = []\n\nfor _ in range(n_sims):\n    low_minus_high = np.sqrt(12 * shai_sample_std**2)\n    simulated_scores = rng.uniform(low=30.4 - low_minus_high/2,\n                                   high=30.4 + low_minus_high/2, size=76)\n    simulated_ppg = np.mean(simulated_scores)\n    shai_simulated_ppg.append(simulated_ppg)\n\np_value_simulated = np.mean(np.array(shai_simulated_ppg) &gt;= 32.7)\nprint(f\"p-value from simulation: {p_value_simulated:.4f}\")\n\n\np-value from simulation: 0.0040"
  },
  {
    "objectID": "slides/lecture-05-slides.html#both-methods-agree",
    "href": "slides/lecture-05-slides.html#both-methods-agree",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Both Methods Agree!",
    "text": "Both Methods Agree!\n\nCLT p-value: ~0.004\nSimulation p-value: ~0.004\n\n\nConclusion: Very unlikely SGA would score 32.7 PPG if his true ability was only 30.4 PPG."
  },
  {
    "objectID": "slides/lecture-05-slides.html#errors-in-hypothesis-testing",
    "href": "slides/lecture-05-slides.html#errors-in-hypothesis-testing",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Errors in Hypothesis Testing",
    "text": "Errors in Hypothesis Testing\nTwo types of errors:\n\n\n\n\nReject \\(H_0\\)\nDon’t Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) true\nType I (false positive)\n✓ Correct\n\n\n\\(H_0\\) false\n✓ Correct\nType II (false negative)\n\n\n\n\n\nType I error rate = \\(\\alpha\\) (our significance threshold)\nType II error rate = \\(\\beta\\)"
  },
  {
    "objectID": "slides/lecture-05-slides.html#summary",
    "href": "slides/lecture-05-slides.html#summary",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nHypothesis Testing Framework\n\nDefine null and alternative hypotheses\nCollect data\nCompute test statistic\nFind p-value (probability of data under \\(H_0\\))\nCompare to significance threshold \\(\\alpha\\)\n\n\nKey Points\n\np-values quantify evidence against \\(H_0\\)\nSimulation can generate sampling distributions\nAlways report p-values, not just “significant/not”"
  },
  {
    "objectID": "slides/lecture-05-slides.html#next-time",
    "href": "slides/lecture-05-slides.html#next-time",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Next Time",
    "text": "Next Time\nConfidence Intervals and Bootstrapping\n\nQuantifying uncertainty differently\nResampling methods\nNo assumptions about distributions needed!"
  },
  {
    "objectID": "slides/lecture-03-slides.html#recap",
    "href": "slides/lecture-03-slides.html#recap",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Recap",
    "text": "Recap\nIn the last lecture, we introduced some basic concepts of probability and random variables. We learned about:\n\nProbability\nConditional probability\nIndependence\nProbability functions\nRandom variables\nExpectation (or expected value)\nVariance\nProbability distributions\n\nNow that we have a good understanding of the basics of probability, we can start to explore how we deal with randomness computationally."
  },
  {
    "objectID": "slides/lecture-03-slides.html#sampling-from-probability-distributions",
    "href": "slides/lecture-03-slides.html#sampling-from-probability-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from probability distributions",
    "text": "Sampling from probability distributions\nA sample is a subset of data drawn from a more general population. That population can be thought of as a probability distribution – this distribution essentially describes how likely you are to observe different values when you sample from it.\nWe will quickly review some important concepts related to sampling."
  },
  {
    "objectID": "slides/lecture-03-slides.html#iid-sampling",
    "href": "slides/lecture-03-slides.html#iid-sampling",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "IID sampling",
    "text": "IID sampling\nIndependent and identically distributed (IID) sampling\nWhen we sample from a probability distribution, we often assume that the samples are independent and identically distributed (IID). This means that each sample is drawn from the same distribution and that the samples do not influence each other.\nCoin flips are a good example of IID sampling. If you flip a fair coin multiple times, each flip has the same probability of being heads or tails (this is the “identically distributed” part), and the outcome of one flip does not affect the outcome of another (this is the “independent” part). The same is true for rolling a die!"
  },
  {
    "objectID": "slides/lecture-03-slides.html#iid-in-practice",
    "href": "slides/lecture-03-slides.html#iid-in-practice",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "IID in practice",
    "text": "IID in practice\nWe often apply this concept to more complex random processes as well, where we do not have such a clear understanding of the underlying process. For example, if we are sampling the heights of people in a city, we might assume that each person’s height is drawn from the same distribution (the distribution of heights in that city) and that one person’s height does not affect another’s.\nWhether or not the IID assumption holds in practice is an important question to consider when analyzing data – for example, do you think that the heights of people in a family are independent of each other?"
  },
  {
    "objectID": "slides/lecture-03-slides.html#sampling-with-and-without-replacement",
    "href": "slides/lecture-03-slides.html#sampling-with-and-without-replacement",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling with and without replacement",
    "text": "Sampling with and without replacement\nAnother important concept in sampling is the distinction between sampling with replacement and sampling without replacement.\n\nSampling with replacement means that after we draw a sample from the population, we put it back before drawing the next sample. This means that the same object / instance can be selected multiple times.\nSampling without replacement means that once we draw a sample, we do not put it back before drawing the next sample. This means that each individual can only be selected once. This can introduce dependencies between samples, as the population changes after each draw."
  },
  {
    "objectID": "slides/lecture-03-slides.html#quiz-sampling",
    "href": "slides/lecture-03-slides.html#quiz-sampling",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Quiz: Sampling",
    "text": "Quiz: Sampling"
  },
  {
    "objectID": "slides/lecture-03-slides.html#simulating-a-random-sample",
    "href": "slides/lecture-03-slides.html#simulating-a-random-sample",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating a random sample",
    "text": "Simulating a random sample\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code."
  },
  {
    "objectID": "slides/lecture-03-slides.html#random-sampling-in-python",
    "href": "slides/lecture-03-slides.html#random-sampling-in-python",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Random sampling in Python",
    "text": "Random sampling in Python\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\n\nNormal distribution\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean)."
  },
  {
    "objectID": "slides/lecture-03-slides.html#sampling-from-a-normal-distribution",
    "href": "slides/lecture-03-slides.html#sampling-from-a-normal-distribution",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from a normal distribution",
    "text": "Sampling from a normal distribution\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\nrng = np.random.default_rng(seed=42)  # Create a random number generator with a fixed seed\nsamples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\nplt.figure(figsize=(8, 5))\nplt.hist(samples, bins=10, density=True)\nplt.show()\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]"
  },
  {
    "objectID": "slides/lecture-03-slides.html#sampling-from-a-dataset",
    "href": "slides/lecture-03-slides.html#sampling-from-a-dataset",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from a dataset",
    "text": "Sampling from a dataset\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\n# Sampling without replacement\nrng = np.random.default_rng(seed=42)\nsubsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n# another way to sample; note RNG can be different in different packages\npd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10, replace=False, random_state=42)\n\n\nSample:\n [-1.32269961 -1.13328721 -0.01680116 -1.68286977  0.54315427 -1.68733443\n  0.85797588 -0.85304393 -0.04992591 -0.36305385]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n83\n-0.381738\n\n\n53\n-0.319671\n\n\n70\n-1.275686\n\n\n45\n0.218689\n\n\n44\n0.116686\n\n\n39\n0.650593\n\n\n22\n1.222541\n\n\n80\n0.456775\n\n\n10\n0.879398\n\n\n0\n0.304717"
  },
  {
    "objectID": "slides/lecture-03-slides.html#custom-distributions",
    "href": "slides/lecture-03-slides.html#custom-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Custom distributions",
    "text": "Custom distributions\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\npossible_rolls = [1, 2, 3, 4, 5, 6]\nprobabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\nrng.choice(possible_rolls, p=probabilities, size=100)\n\n\narray([4, 6, 6, 6, 5, 3, 6, 1, 6, 6, 6, 4, 6, 6, 6, 2, 5, 1, 2, 6, 6, 6,\n       4, 4, 5, 2, 2, 5, 3, 6, 5, 6, 6, 4, 6, 6, 4, 3, 6, 2, 2, 1, 6, 6,\n       6, 6, 5, 6, 2, 2, 6, 5, 6, 6, 6, 6, 6, 4, 1, 5, 3, 5, 6, 3, 1, 3,\n       3, 6, 6, 6, 6, 5, 6, 2, 1, 1, 6, 5, 2, 6, 2, 6, 5, 4, 4, 6, 4, 1,\n       2, 6, 6, 6, 3, 6, 6, 6, 5, 3, 1, 6])"
  },
  {
    "objectID": "slides/lecture-03-slides.html#simulating-more-complex-processes",
    "href": "slides/lecture-03-slides.html#simulating-more-complex-processes",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating more complex processes",
    "text": "Simulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nConsider dealing poker hands from a standard deck of cards. When you deal a hand, you draw cards one at a time, and each card drawn affects the next card that can be drawn (because you do not put the card back into the deck)."
  },
  {
    "objectID": "slides/lecture-03-slides.html#dealing-cards-example",
    "href": "slides/lecture-03-slides.html#dealing-cards-example",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Dealing cards example",
    "text": "Dealing cards example\n\n\nCode\n# Make a deck of cards (Ace is 1, King is 13)\ndeck = np.arange(1, 14).repeat(4)  # 4 suits, each with cards 1 to 13\nprint(\"Deck of cards before shuffling:\\n\", deck)\ndeck = np.random.permutation(deck)\nprint(\"Deck of cards after shuffling:\\n\", deck)\n# deal 2 cards to each of 4 players\nrng = np.random.default_rng(seed=21)\n# Get flat indices of 8 cards from deck\n# we want to know exactly which cards are dealt\nchosen_indices = rng.choice(len(deck), size=8, replace=False)\nhands = deck[chosen_indices].reshape(4, 2)\nprint(\"Hands dealt to players:\\n\", hands)\n# remove the dealt cards from the deck\nremaining_deck = np.delete(deck, chosen_indices)\nprint(\"Remaining cards in the deck:\\n\", remaining_deck)\nboard = np.random.choice(remaining_deck, size=5, replace=False)\nprint(\"Community cards on the board:\\n\", board)\n\n\nDeck of cards before shuffling:\n [ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12\n 13 13 13 13]\nDeck of cards after shuffling:\n [13 12  2 12  4  1  3  6  7 10  2  3  9  5  9 13  9  6  9  1  2 12 11  8\n  7  8  5  7  7 13  4 11 11  1 10  4  3  3  5  6  8  8  5  4  6 11  1 13\n 10 12  2 10]\nHands dealt to players:\n [[12  4]\n [ 4  5]\n [ 6 13]\n [11  9]]\nRemaining cards in the deck:\n [13 12  2 12  1  3  6  7 10  2  3  9  9 13  9  1  2 12  8  7  8  5  7  7\n  4 11 11  1 10  3  3  5  6  8  8  5  4  6 11  1 13 10  2 10]\nCommunity cards on the board:\n [12 11 13 11 12]"
  },
  {
    "objectID": "slides/lecture-03-slides.html#multi-step-simulations",
    "href": "slides/lecture-03-slides.html#multi-step-simulations",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Multi-step simulations",
    "text": "Multi-step simulations\nBut it can get even more complex than that. In many real-world scenarios, the process of generating data involves multiple steps or conditions that affect the outcome.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code). We then tend to write loops that simulate the process step by step, keeping track of the state of things as we go along."
  },
  {
    "objectID": "slides/lecture-03-slides.html#busker-example",
    "href": "slides/lecture-03-slides.html#busker-example",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Busker example",
    "text": "Busker example\nLet’s consider an example of a musician busking for money in Rittenhouse Square. The musician’s earnings might depend on various factors like the weather and and the number of passersby. To keep it simple, let’s assume that the musician earns $3 for every passerby who stops to listen. Of course, not every passerby will stop – let’s pretend every passerby has the same 20% chance of stopping.\nThe musician might want to know how much money they can expect to earn in a day of busking. We can simulate this process by generating a random number of passersby and then calculating the earnings based on the stopping probability."
  },
  {
    "objectID": "slides/lecture-03-slides.html#poisson-distribution",
    "href": "slides/lecture-03-slides.html#poisson-distribution",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\n\n\n\n\n\nPoisson distribution\n\n\nThe Poisson distribution is commonly used to model the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It assumes that the events occur independently and at a constant average rate. In our example, we can use the Poisson distribution to model the number of passersby in a given time period (e.g., one hour of busking)."
  },
  {
    "objectID": "slides/lecture-03-slides.html#busker-simulation-code",
    "href": "slides/lecture-03-slides.html#busker-simulation-code",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Busker simulation code",
    "text": "Busker simulation code\n\n\nCode\nn_days = 5\n# simulate whether it rains each day\nrng = np.random.default_rng(seed=42)\nrain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\ntotal_earnings = 0 # initialize a variable to keep track of total earnings\nfor day in range(n_days):\n    # For each day, decide if it rains based on the probability\n    did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n    print(f\"Day {day + 1} ({rain_probabilities[day]:.2%} chance): {'Rain' if did_it_rain else 'No rain'}\")\n    # Based on the outcome, the number of passersby changes\n    if did_it_rain:\n        passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n    else:\n        passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n    print(f\"\\t Number of passersby: {passersby}\")\n    # Simulate the number who stop to listen to the busker\n    listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n    print(f\"\\t Number of listeners: {listeners}\")\n    # Compute the busker's daily earnings\n    earnings = 3 * listeners  # $3 per listener\n    print(f\"\\t Daily earnings: ${earnings}\")\n    print(\"-\" * 40)\n\n    total_earnings += earnings\n\nprint(f\"Total earnings over {n_days} days: ${total_earnings}\")\n\n\nDay 1 (54.18% chance): No rain\n     Number of passersby: 211\n     Number of listeners: 41\n     Daily earnings: $123\n----------------------------------------\nDay 2 (30.72% chance): No rain\n     Number of passersby: 226\n     Number of listeners: 54\n     Daily earnings: $162\n----------------------------------------\nDay 3 (60.10% chance): Rain\n     Number of passersby: 51\n     Number of listeners: 13\n     Daily earnings: $39\n----------------------------------------\nDay 4 (48.82% chance): Rain\n     Number of passersby: 56\n     Number of listeners: 17\n     Daily earnings: $51\n----------------------------------------\nDay 5 (6.59% chance): No rain\n     Number of passersby: 212\n     Number of listeners: 34\n     Daily earnings: $102\n----------------------------------------\nTotal earnings over 5 days: $477"
  },
  {
    "objectID": "slides/lecture-03-slides.html#exercise-simulate-weekly-earnings",
    "href": "slides/lecture-03-slides.html#exercise-simulate-weekly-earnings",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Exercise: Simulate Weekly Earnings",
    "text": "Exercise: Simulate Weekly Earnings\nTask: Simulate the expected earnings of a busker over a week. Run the simulation 1000 times and calculate the average."
  },
  {
    "objectID": "slides/lecture-03-slides.html#solution",
    "href": "slides/lecture-03-slides.html#solution",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Solution",
    "text": "Solution\n\n\n\n\n\n\n\nSolution\n\n\nimport numpy as np\n\ndef busk_one_week(rng, n_days=7):\n    \"\"\"Simulate weekly earnings of a busker.\"\"\"\n    rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n    total_earnings = 0\n    \n    for day in range(n_days):\n        did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n        if did_it_rain:\n            passersby = rng.poisson(lam=50)\n        else:\n            passersby = rng.poisson(lam=200)\n        listeners = rng.binomial(n=passersby, p=0.2)\n        total_earnings += 3 * listeners\n    \n    return total_earnings\n\n# Run 1000 simulations\nrng = np.random.default_rng(seed=33)\nearnings_by_week = [busk_one_week(rng) for _ in range(1000)]\naverage_earnings = np.mean(earnings_by_week)\nprint(f\"Average weekly earnings: ${average_earnings:.2f}\")"
  },
  {
    "objectID": "slides/lecture-01-slides.html#overview",
    "href": "slides/lecture-01-slides.html#overview",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Overview",
    "text": "Overview\nThis lecture will be, intentionally, a bit of a whirlwind. That’s because with the advent of large language models (LLMs) like ChatGPT, Claude, Gemini, etc. knowing how to program in specific languages like Python is becoming less important. You don’t need that much practice or to focus on the syntax of a specific language.\nInstead, the important thing is to understand the core concepts involved in programming, which are largely universal across languages. This high-level understanding will allow you to use LLMs effectively to write code in any language, including Python. If you don’t understand the concepts, you won’t be able to identify when the LLM is making mistakes or producing suboptimal code."
  },
  {
    "objectID": "slides/lecture-01-slides.html#variables",
    "href": "slides/lecture-01-slides.html#variables",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables",
    "text": "Variables\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations."
  },
  {
    "objectID": "slides/lecture-01-slides.html#aside-functions",
    "href": "slides/lecture-01-slides.html#aside-functions",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Aside: Functions",
    "text": "Aside: Functions\n\n\n\n\n\n\n\nFunctions act on variables\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable."
  },
  {
    "objectID": "slides/lecture-01-slides.html#creating-variables",
    "href": "slides/lecture-01-slides.html#creating-variables",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Creating Variables",
    "text": "Creating Variables\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8"
  },
  {
    "objectID": "slides/lecture-01-slides.html#variable-types",
    "href": "slides/lecture-01-slides.html#variable-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variable Types",
    "text": "Variable Types\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger (int)\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat (float)\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString (str)\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList (list)\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary (dict)\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean (bool)\nTrue or False values, e.g., True, False"
  },
  {
    "objectID": "slides/lecture-01-slides.html#variables-are-objects",
    "href": "slides/lecture-01-slides.html#variables-are-objects",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables are Objects",
    "text": "Variables are Objects\nIn Python, everything is an object.\n\nThis means that even basic data types like integers and strings are treated as objects with methods and properties.\nFor example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\n\nSee the later section on Object-Oriented Programming for more details."
  },
  {
    "objectID": "slides/lecture-01-slides.html#typecasting",
    "href": "slides/lecture-01-slides.html#typecasting",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Typecasting",
    "text": "Typecasting\nTypecasting is the process of converting a variable from one type to another. Not all type conversions are allowed, but some common ones include:\n\n\n\n\n\n\n\n\nFrom Type\nTo Type\nExample\n\n\n\n\nInteger\nFloat\nfloat(5) → 5.0\n\n\nFloat\nInteger\nint(3.14) → 3\n\n\nString\nInteger\nint('42') → 42\n\n\nString\nFloat\nfloat('3.14') → 3.14\n\n\nList\nString\n''.join(['S', 'h', 'a', 'l', 'o', 'm']) → 'Shalom'\n\n\nString\nList\nlist('Shalom') → ['S', 'h', 'a', 'l', 'o', 'm']\n\n\nBoolean\nInteger\nint(True) → 1, int(False) → 0\n\n\nInteger\nBoolean\nbool(1) → True, bool(0) → False, bool(-1) → True"
  },
  {
    "objectID": "slides/lecture-01-slides.html#implicit-type-conversions",
    "href": "slides/lecture-01-slides.html#implicit-type-conversions",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Implicit Type Conversions",
    "text": "Implicit Type Conversions\nThere are also some implicit type conversions that happen automatically in Python, such as when you perform arithmetic operations between integers and floats. For example:\n\nx = 5        # Integer\ny = 2.0      # Float\nresult = x + y  # Implicit conversion to float\nresult, type(result)  # result is now a float\n\n(7.0, float)"
  },
  {
    "objectID": "slides/lecture-01-slides.html#lists",
    "href": "slides/lecture-01-slides.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\n\nWe often need to store multiple values together.\nThe most basic way to achieve this is with a list.\nA list is an ordered collection of items that can be of any type, including other lists.\n\n“Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\n\n\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\nprint(my_list[0]) \n\n1"
  },
  {
    "objectID": "slides/lecture-01-slides.html#list-indexing",
    "href": "slides/lecture-01-slides.html#list-indexing",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "List Indexing",
    "text": "List Indexing\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\n[1, 2, 3]\n['apple', 'banana']\n[1, 3, 'banana']\n['banana', 'apple', 3, 2, 1]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#modifying-lists",
    "href": "slides/lecture-01-slides.html#modifying-lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Modifying Lists",
    "text": "Modifying Lists\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']"
  },
  {
    "objectID": "slides/lecture-01-slides.html#arrays-numpy",
    "href": "slides/lecture-01-slides.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\n\nPackages\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array()."
  },
  {
    "objectID": "slides/lecture-01-slides.html#creating-arrays",
    "href": "slides/lecture-01-slides.html#creating-arrays",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Creating Arrays",
    "text": "Creating Arrays\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#array-operations",
    "href": "slides/lecture-01-slides.html#array-operations",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Array Operations",
    "text": "Array Operations\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#array-type-enforcement",
    "href": "slides/lecture-01-slides.html#array-type-enforcement",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Array Type Enforcement",
    "text": "Array Type Enforcement\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\nTypecasting works in NumPy arrays as well via the astype() method.\n\nnp.array([1, 2, 3, 4.1], dtype=float).astype(int)  # Convert float array to int array\n\narray([1, 2, 3, 4])"
  },
  {
    "objectID": "slides/lecture-01-slides.html#advanced-indexing",
    "href": "slides/lecture-01-slides.html#advanced-indexing",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Advanced indexing",
    "text": "Advanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#boolean-indexing",
    "href": "slides/lecture-01-slides.html#boolean-indexing",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Boolean indexing",
    "text": "Boolean indexing\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [ 1  2  3  4  5  6  7  8  9 10]\nBoolean mask: [False False  True  True  True  True  True  True  True  True]\nFiltered array: [ 3  4  5  6  7  8  9 10]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#dictionaries",
    "href": "slides/lecture-01-slides.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}"
  },
  {
    "objectID": "slides/lecture-01-slides.html#accessing-dictionary-values",
    "href": "slides/lecture-01-slides.html#accessing-dictionary-values",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Accessing Dictionary Values",
    "text": "Accessing Dictionary Values\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25"
  },
  {
    "objectID": "slides/lecture-01-slides.html#nested-dictionaries",
    "href": "slides/lecture-01-slides.html#nested-dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Nested Dictionaries",
    "text": "Nested Dictionaries\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}"
  },
  {
    "objectID": "slides/lecture-01-slides.html#dataframes",
    "href": "slides/lecture-01-slides.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns).\nThink of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago"
  },
  {
    "objectID": "slides/lecture-01-slides.html#dataframe-column-types",
    "href": "slides/lecture-01-slides.html#dataframe-column-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "DataFrame Column Types",
    "text": "DataFrame Column Types\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object"
  },
  {
    "objectID": "slides/lecture-01-slides.html#conditional-logic",
    "href": "slides/lecture-01-slides.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false."
  },
  {
    "objectID": "slides/lecture-01-slides.html#if-elif-else-example",
    "href": "slides/lecture-01-slides.html#if-elif-else-example",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "If-elif-else example",
    "text": "If-elif-else example\nHere’s an example of how to use these statements:\nage = 25  # Set your age here\nif age &lt; 18:\n    print(\"You are a minor.\")\nelif age &lt; 65:\n    print(\"You are an adult.\")\nelif age &gt;= 120:\n    print(\"You've done your time, haven't you?\")\nelse:\n    print(\"You are a senior citizen.\")\n# Output: \"You are an adult.\"\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false."
  },
  {
    "objectID": "slides/lecture-01-slides.html#boolean-expressions",
    "href": "slides/lecture-01-slides.html#boolean-expressions",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Boolean expressions",
    "text": "Boolean expressions\n\n\n\n\n\n\n\nBoolean expressions\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT"
  },
  {
    "objectID": "slides/lecture-01-slides.html#loops",
    "href": "slides/lecture-01-slides.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops."
  },
  {
    "objectID": "slides/lecture-01-slides.html#for-loops",
    "href": "slides/lecture-01-slides.html#for-loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "For Loops",
    "text": "For Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one."
  },
  {
    "objectID": "slides/lecture-01-slides.html#useful-python-functions-range-and-enumerate",
    "href": "slides/lecture-01-slides.html#useful-python-functions-range-and-enumerate",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Useful Python functions: range() and enumerate()",
    "text": "Useful Python functions: range() and enumerate()\n\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4.\nThe enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")"
  },
  {
    "objectID": "slides/lecture-01-slides.html#while-loops",
    "href": "slides/lecture-01-slides.html#while-loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "While Loops",
    "text": "While Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true."
  },
  {
    "objectID": "slides/lecture-01-slides.html#functions-and-functional-programming",
    "href": "slides/lecture-01-slides.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result).\nYou can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters."
  },
  {
    "objectID": "slides/lecture-01-slides.html#function-example",
    "href": "slides/lecture-01-slides.html#function-example",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Function example",
    "text": "Function example\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!"
  },
  {
    "objectID": "slides/lecture-01-slides.html#functional-programming",
    "href": "slides/lecture-01-slides.html#functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functional programming",
    "text": "Functional programming\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\nProbably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nTechnically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing."
  },
  {
    "objectID": "slides/lecture-01-slides.html#avoiding-side-effects",
    "href": "slides/lecture-01-slides.html#avoiding-side-effects",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Avoiding side effects",
    "text": "Avoiding side effects\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#side-effects-cont.",
    "href": "slides/lecture-01-slides.html#side-effects-cont.",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Side effects (cont.)",
    "text": "Side effects (cont.)\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones."
  },
  {
    "objectID": "slides/lecture-01-slides.html#object-oriented-programming",
    "href": "slides/lecture-01-slides.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created."
  },
  {
    "objectID": "slides/lecture-01-slides.html#class-example",
    "href": "slides/lecture-01-slides.html#class-example",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Class example",
    "text": "Class example\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse"
  },
  {
    "objectID": "slides/lecture-01-slides.html#oop-advantages",
    "href": "slides/lecture-01-slides.html#oop-advantages",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "OOP advantages",
    "text": "OOP advantages\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality."
  },
  {
    "objectID": "slides/lecture-01-slides.html#inheritance-example",
    "href": "slides/lecture-01-slides.html#inheritance-example",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Inheritance example",
    "text": "Inheritance example\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        return f\"{self.year}-{self.month:02d}-{self.day:02d}\"\n\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name  # Add new attribute\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nchannukah = HolidayDate(2025, 12, 7, \"Channukah\")\nprint(channukah)  # Output: 2025-12-07\nchannukah.print_holiday()  # Output: Channukah is on 2025-12-07."
  },
  {
    "objectID": "slides/lecture-01-slides.html#oop-in-data-science",
    "href": "slides/lecture-01-slides.html#oop-in-data-science",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "OOP in data science",
    "text": "OOP in data science\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class."
  },
  {
    "objectID": "slides/lecture-01-slides.html#summary",
    "href": "slides/lecture-01-slides.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "understanding-uncertainty",
    "section": "",
    "text": "This website contains course materials for the Understanding Uncertainty course, which is a part of the US-Israel Academic Bridge Fellowship hosted at the University of Pennsylvania."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "understanding-uncertainty",
    "section": "About the course",
    "text": "About the course\nThis course provides an introduction to statistics with an emphasis on building intuition about uncertainty in data analysis and decision-making. The course is structured around thinking critically about data generating processes (i.e. the random, real-world phenomena that produce data) and how to model them computationally."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "understanding-uncertainty",
    "section": "About the author",
    "text": "About the author\nJoey Rudoler is a PhD student in the Department of Statistics and Data Science at the Wharton School of the University of Pennsylvania. His research focuses on statistical aspects of machine learning, in particular theory for deep learning. He has a background in physics and neuroscience, and remains interested in the intersection of statistics, machine learning, and neuroscience."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nDescription\nDue Date\n\n\n\n\nAssignment 0\n\n\n\n\nAssignment 1\n\n\n\n\nAssignment 2\n\n\n\n\nAssignment 3"
  },
  {
    "objectID": "slides/lecture-00-slides.html#course-goals",
    "href": "slides/lecture-00-slides.html#course-goals",
    "title": "Why Statistics?",
    "section": "Course Goals",
    "text": "Course Goals\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#why-statistics",
    "href": "slides/lecture-00-slides.html#why-statistics",
    "title": "Why Statistics?",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data:\n\ndescription\ninference\nprediction"
  },
  {
    "objectID": "slides/lecture-00-slides.html#description",
    "href": "slides/lecture-00-slides.html#description",
    "title": "Why Statistics?",
    "section": "Description",
    "text": "Description\n\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\n\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features."
  },
  {
    "objectID": "slides/lecture-00-slides.html#description-1",
    "href": "slides/lecture-00-slides.html#description-1",
    "title": "Why Statistics?",
    "section": "Description",
    "text": "Description\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n\n\n3 rows × 26 columns"
  },
  {
    "objectID": "slides/lecture-00-slides.html#sample-population",
    "href": "slides/lecture-00-slides.html#sample-population",
    "title": "Why Statistics?",
    "section": "Sample ≠ Population",
    "text": "Sample ≠ Population\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\nPopulation\nthe entire set of data that you are interested in.\nSample\na subset of a population.\nA random sample is a sample that is selected randomly from the population.\n\n\n\n\n\n\n\n\nExample: Airbnb listings in New York City\n\n\nWe want to know the average price of Airbnb listings in New York City.\n\npopulation: all Airbnb listings in New York City\nsample: a smaller subset of those listings, which may or may not be representative of the entire population."
  },
  {
    "objectID": "slides/lecture-00-slides.html#what-is-the-population",
    "href": "slides/lecture-00-slides.html#what-is-the-population",
    "title": "Why Statistics?",
    "section": "What is the population?",
    "text": "What is the population?\nFlexible definition:\n\nAverage price of all short-term rentals in New York City? Population: all rentals (not just Airbnb listings) in New York City.\n\n\nOften, the population is actually more abstract or theoretical\n\nAverage price of all possible Airbnb listings in New York City? Population: all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more."
  },
  {
    "objectID": "slides/lecture-00-slides.html#quiz-restaurant-survey",
    "href": "slides/lecture-00-slides.html#quiz-restaurant-survey",
    "title": "Why Statistics?",
    "section": "Quiz: restaurant survey",
    "text": "Quiz: restaurant survey"
  },
  {
    "objectID": "slides/lecture-00-slides.html#inference",
    "href": "slides/lecture-00-slides.html#inference",
    "title": "Why Statistics?",
    "section": "Inference",
    "text": "Inference\nWhat if we want to answer questions about a population based on a sample?\nThis is where inference comes in.\n\nUse the given sample to infer something about the population.\n\n\nHow do we do this if we can’t ever see the entire population?\n\nNeed a link which connects the sample to the population\nTreat the sample as the outcome of a data-generating process (DGP)."
  },
  {
    "objectID": "slides/lecture-00-slides.html#data-generating-process",
    "href": "slides/lecture-00-slides.html#data-generating-process",
    "title": "Why Statistics?",
    "section": "Data-Generating Process",
    "text": "Data-Generating Process\n\n\n\n\n\n\n\nThere is always a DGP\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population.\n\nEncompasses all the factors that influence the data (incl. the mechanisms and relationships between variables).\nThere has to be a DGP, even if we don’t know what it is.\nThe DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown.\n\nWe can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\n\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#statistical-models",
    "href": "slides/lecture-00-slides.html#statistical-models",
    "title": "Why Statistics?",
    "section": "Statistical models",
    "text": "Statistical models\nWhen the full DGP is too complicated / unknown, we use a model\n\nsimplified mathematical representation of the DGP\nallows us to make inferences about the population based on the sample\nultimately sort of a guess – about where your data come from."
  },
  {
    "objectID": "slides/lecture-00-slides.html#example-airbnb-listings.",
    "href": "slides/lecture-00-slides.html#example-airbnb-listings.",
    "title": "Why Statistics?",
    "section": "Example: Airbnb listings.",
    "text": "Example: Airbnb listings.\n\nAssume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs.\nProbability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.\n\n\nThen we can look at the actual sample of listings and see if it matches our assumption:"
  },
  {
    "objectID": "slides/lecture-00-slides.html#unlikely-data-or-unlikely-model",
    "href": "slides/lecture-00-slides.html#unlikely-data-or-unlikely-model",
    "title": "Why Statistics?",
    "section": "Unlikely data or unlikely model?",
    "text": "Unlikely data or unlikely model?\nQuestion: “If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\n\nquestion about the probability of the sample, given a certain model of the DGP\nit intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings."
  },
  {
    "objectID": "slides/lecture-00-slides.html#evaluating-models",
    "href": "slides/lecture-00-slides.html#evaluating-models",
    "title": "Why Statistics?",
    "section": "Evaluating models",
    "text": "Evaluating models\nWhat should we do now?\n\nNow that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model.\nModel is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous."
  },
  {
    "objectID": "slides/lecture-00-slides.html#inference-requires-domain-knowledge",
    "href": "slides/lecture-00-slides.html#inference-requires-domain-knowledge",
    "title": "Why Statistics?",
    "section": "Inference requires domain knowledge",
    "text": "Inference requires domain knowledge\n\n\n\n\n\n\n\nDon’t try this at home!\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data.\nThis requires domain knowledge and subject matter expertise.\n\n\n\n\nIn the Airbnb example:\n\nAssuming that all boroughs are equally likely to produce listings is a pretty bad assumption\n\nManhattan sees vastly more tourism than the other boroughs\nBrooklyn and Queens have by far the most residents according to recent census data."
  },
  {
    "objectID": "slides/lecture-00-slides.html#prediction",
    "href": "slides/lecture-00-slides.html#prediction",
    "title": "Why Statistics?",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data: we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. Basic idea of the model:\n\nwe assume the features of the listing (e.g., price) are related to the probability of it being in a certain borough\n\ne.g., perhaps more expensive listings are more likely to be in Manhattan"
  },
  {
    "objectID": "slides/lecture-00-slides.html#fitting-a-model",
    "href": "slides/lecture-00-slides.html#fitting-a-model",
    "title": "Why Statistics?",
    "section": "Fitting a model",
    "text": "Fitting a model\n\n\n\n\n\n\n\nFitting a model\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nCoin flip has a single parameter: the probability of landing on heads.\n\nIf you turn the knob to 0.5, you get a fair coin;\nif you turn it to 1.0, you get a coin that always lands on heads;\nif you turn it to 0.0, you get a coin that always lands on tails.\n\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data."
  },
  {
    "objectID": "slides/lecture-00-slides.html#predicting-the-borough-of-a-listing",
    "href": "slides/lecture-00-slides.html#predicting-the-borough-of-a-listing",
    "title": "Why Statistics?",
    "section": "Predicting the borough of a listing",
    "text": "Predicting the borough of a listing\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================"
  },
  {
    "objectID": "slides/lecture-00-slides.html#evaluating-predictions",
    "href": "slides/lecture-00-slides.html#evaluating-predictions",
    "title": "Why Statistics?",
    "section": "Evaluating predictions",
    "text": "Evaluating predictions\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\n\n\n\n\n\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously.\nFor now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be.\nFor example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance."
  },
  {
    "objectID": "slides/lecture-00-slides.html#model-predictions-distribution",
    "href": "slides/lecture-00-slides.html#model-predictions-distribution",
    "title": "Why Statistics?",
    "section": "Model predictions (distribution)",
    "text": "Model predictions (distribution)\nNow let’s take a look at the distribution of the model’s predictions."
  },
  {
    "objectID": "slides/lecture-00-slides.html#model-predictions-cont.",
    "href": "slides/lecture-00-slides.html#model-predictions-cont.",
    "title": "Why Statistics?",
    "section": "Model predictions (cont.)",
    "text": "Model predictions (cont.)\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP."
  },
  {
    "objectID": "slides/lecture-00-slides.html#summary",
    "href": "slides/lecture-00-slides.html#summary",
    "title": "Why Statistics?",
    "section": "Summary",
    "text": "Summary\n3 objectives of data analysis: description, inference, and prediction.\nHopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. (Don’t worry, we’ll get there!)\nUp next:\n\nbasic programming concepts that are important for data science.\nAfter that we will learn some foundational concepts in probability that will help us think about data and models more rigorously.\n\n\nFrom there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learned any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability",
    "href": "slides/lecture-02-slides.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\n\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\n\\P(A) = \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}}\n\\]\n\n\nBy definition this quantity:\n\ncannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and\nit must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#example-coin-flip",
    "href": "slides/lecture-02-slides.html#example-coin-flip",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Example: Coin Flip",
    "text": "Example: Coin Flip\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)).\nIf we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\n\\P(\\text{H}) = \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} = \\frac{1}{2}\n\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads."
  },
  {
    "objectID": "slides/lecture-02-slides.html#properties-of-probability-sum-to-1",
    "href": "slides/lecture-02-slides.html#properties-of-probability-sum-to-1",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Properties of Probability: Sum to 1",
    "text": "Properties of Probability: Sum to 1\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say “there’s a 100% chance that something will happen”.\n\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events \\(H\\) and \\(T\\) cover 100% of the possible outcomes. So we can write: \\[\n\\P(H) + \\P(T) = 1 \\quad \\Rightarrow \\quad \\frac{1}{2} + \\frac{1}{2} = 1\n\\]\n\n\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities.\n\n\nFor example, for any event \\(A\\), the event either happens or it doesn’t. So we can compute the probability of the event not occurring as: \\[\n\\P(\\text{not}~ A) = 1 - \\P(A)\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-of-multiple-events",
    "href": "slides/lecture-02-slides.html#probability-of-multiple-events",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability of multiple events",
    "text": "Probability of multiple events\nBut what if we flip the coin twice?\nNow there are four possible outcomes:\n\n\\(HH\\)\n\\(HT\\)\n\\(TH\\)\n\\(TT\\)\n\n\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows:\n\n\n\\[\n\\P(B) = \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} = \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} = \\frac{3}{4}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-of-multiple-events-and-vs-or",
    "href": "slides/lecture-02-slides.html#probability-of-multiple-events-and-vs-or",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability of multiple events: AND vs OR",
    "text": "Probability of multiple events: AND vs OR\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\n\nThere is only one outcome where both flips are heads\nStill four total outcomes\n\\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\n\n\nWhat about the probability of getting heads on the first flip OR the second flip?\n\n\nThis is actually the same event as \\(B\\) before (at least 1 heads), so we can use the same calculation:\n\\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#set-notation-and-or",
    "href": "slides/lecture-02-slides.html#set-notation-and-or",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Set notation (AND / OR)",
    "text": "Set notation (AND / OR)\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively.\nThe notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively.\nSo we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads.\nTechnically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#addition-rule",
    "href": "slides/lecture-02-slides.html#addition-rule",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Addition rule",
    "text": "Addition rule\nFor any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#visualizing-sets-of-events",
    "href": "slides/lecture-02-slides.html#visualizing-sets-of-events",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Visualizing sets of events",
    "text": "Visualizing sets of events\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram."
  },
  {
    "objectID": "slides/lecture-02-slides.html#multiplication-rule",
    "href": "slides/lecture-02-slides.html#multiplication-rule",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Multiplication rule",
    "text": "Multiplication rule\nFor any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset."
  },
  {
    "objectID": "slides/lecture-02-slides.html#conditional-probability",
    "href": "slides/lecture-02-slides.html#conditional-probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Conditional probability",
    "text": "Conditional probability\n\n\n\n\n\n\n\nConditional probability\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time.\n\nFor example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”.\nThe conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day."
  },
  {
    "objectID": "slides/lecture-02-slides.html#conditional-probability-example",
    "href": "slides/lecture-02-slides.html#conditional-probability-example",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Conditional probability example",
    "text": "Conditional probability example\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\n\nThe conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\n\\P(H_2 | H_1) = \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} = \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} = \\frac{1}{2}\n\\]\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#problem-drawing-cards",
    "href": "slides/lecture-02-slides.html#problem-drawing-cards",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Problem: Drawing Cards",
    "text": "Problem: Drawing Cards\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”"
  },
  {
    "objectID": "slides/lecture-02-slides.html#independence",
    "href": "slides/lecture-02-slides.html#independence",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Independence",
    "text": "Independence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\n\nHow does this relate to the multiplication rule?\n\nIf \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\).\n\nKnowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\n\n\n\n\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#independence-example",
    "href": "slides/lecture-02-slides.html#independence-example",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Independence example",
    "text": "Independence example\nOur coin flip example illustrates this nicely.\n\nIf we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip.\nTherefore, the two events (the first flip being heads and the second flip being heads) are independent.\n\nSo the probability of both flips being heads is simply\n\n\\[\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#complicated-counting",
    "href": "slides/lecture-02-slides.html#complicated-counting",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Complicated counting",
    "text": "Complicated counting\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes.\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times.\nLet’s think about the case where the first flip is heads.\n\nThe probability of getting a head on the first flip is \\(\\frac{1}{2}\\), and the probability of getting tails on the 4 other flips is also \\(\\frac{1}{2}\\) each.\nBecause the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips: \\[\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#complicated-counting-cont.",
    "href": "slides/lecture-02-slides.html#complicated-counting-cont.",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Complicated counting (cont.)",
    "text": "Complicated counting (cont.)\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips.\n\nBut there are many other sequences that would also meet the conditions of getting “exactly one head”\n\nFor example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\n\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions: \\[\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#counting-outcomes",
    "href": "slides/lecture-02-slides.html#counting-outcomes",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Counting Outcomes",
    "text": "Counting Outcomes\nThere are two common types of outcomes we want to count: permutations and combinations.\n\nA combination is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads.\nAn important and intuitive way to think about combinations is that we are choosing an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\\[\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nIt is clear here that there are 5 possible “slots” where we can place a head."
  },
  {
    "objectID": "slides/lecture-02-slides.html#choosing-2-heads-from-5-flips",
    "href": "slides/lecture-02-slides.html#choosing-2-heads-from-5-flips",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Choosing 2 heads from 5 flips",
    "text": "Choosing 2 heads from 5 flips\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still applies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\n\nLet’s take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices. \\[\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#counting-continued",
    "href": "slides/lecture-02-slides.html#counting-continued",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Counting continued",
    "text": "Counting continued\nNow let’s consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\\[\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n\\]\n\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads.\nThis gives us a total of \\(5 \\cdot 4 = 20\\) ways to choose 2 flips to be heads out of 5 flips.\n\n\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order – where flip 1 was heads first)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#permutations-vs-combinations",
    "href": "slides/lecture-02-slides.html#permutations-vs-combinations",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Permutations vs Combinations",
    "text": "Permutations vs Combinations\nThis illustrates the key distinction between combinations and permutations. A permutation is an arrangement of items or events in a specific order.\n\nEvery possible combination of heads can be arranged in different ways, leading to different sequences of flips.\nIf you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting.\n\n\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign “slots” in a sequence to each of the items you are arranging.\n\n\nEach time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length \\(n\\), the number of permutations is: \\[\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#combination-formula",
    "href": "slides/lecture-02-slides.html#combination-formula",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Combination formula",
    "text": "Combination formula\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\nNamely, the number of combinations of \\(k\\) items from a set of \\(n\\) items is given by the formula: \\[\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n\\]\n\nThis formula:\n\ncounts all of the possible permutations of the sequence (\\(n!\\))\ndivides by the number of ways to arrange the \\(k\\) items that are selected (which is \\(k!\\))\ndivides by the number of ways to arrange the remaining \\(n-k\\) items (which is \\((n-k)!\\))."
  },
  {
    "objectID": "slides/lecture-02-slides.html#combination-example",
    "href": "slides/lecture-02-slides.html#combination-example",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Combination example",
    "text": "Combination example\nIn our example, we have \\(n = 5\\) (the number of flips) and \\(k = 2\\) (the number of heads). So the number of combinations of 2 heads from 5 flips is: \\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#end-of-part-1",
    "href": "slides/lecture-02-slides.html#end-of-part-1",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "End of Part 1",
    "text": "End of Part 1\nThis concludes the first part of our lecture on probability. We have covered the basic concepts of probability, how to compute probabilities for single and multiple events, and the distinction between combinations and permutations. Next we will talk about probability functions, which allow us to concisely describe the probability of outcomes that have many possible values."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-functions",
    "href": "slides/lecture-02-slides.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\n\nHowever, it is often more convenient to work with probability functions.\n\nassigns a probability to each possible outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n  \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n  \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-functions-are-concise",
    "href": "slides/lecture-02-slides.html#probability-functions-are-concise",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions are concise",
    "text": "Probability functions are concise\n\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format.\nBut probability functions are much more concise!"
  },
  {
    "objectID": "slides/lecture-02-slides.html#die-roll-example",
    "href": "slides/lecture-02-slides.html#die-roll-example",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Die roll example",
    "text": "Die roll example\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides."
  },
  {
    "objectID": "slides/lecture-02-slides.html#python-implementation",
    "href": "slides/lecture-02-slides.html#python-implementation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Python implementation",
    "text": "Python implementation\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\n# Example usage:\nprint(f(1, 6))  # Expected output: 0.166..."
  },
  {
    "objectID": "slides/lecture-02-slides.html#random-variables",
    "href": "slides/lecture-02-slides.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that\n\ncan take on different values based on the outcome of a random event.\nit might be a discrete variable (like the outcome of a coin flip)\nit might be a continuous variable (like the height of a person).\nbasically it is any quantity that has randomness associated with it.\n\nWe denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-functions-for-random-variables",
    "href": "slides/lecture-02-slides.html#probability-functions-for-random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability Functions for Random Variables",
    "text": "Probability Functions for Random Variables\nWe use probability functions to describe the probabilities associated with random variables.\n\nSpecifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\n\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#bernoulli-random-variable",
    "href": "slides/lecture-02-slides.html#bernoulli-random-variable",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Bernoulli random variable",
    "text": "Bernoulli random variable\n\n\n\n\n\n\n\nBernoulli random variable\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin."
  },
  {
    "objectID": "slides/lecture-02-slides.html#continuous-random-variables",
    "href": "slides/lecture-02-slides.html#continuous-random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Continuous random variables",
    "text": "Continuous random variables\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\nUniform random variable\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\)."
  },
  {
    "objectID": "slides/lecture-02-slides.html#random-variables-for-random-processes",
    "href": "slides/lecture-02-slides.html#random-variables-for-random-processes",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables for random processes",
    "text": "Random Variables for random processes\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-distributions",
    "href": "slides/lecture-02-slides.html#probability-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions",
    "text": "Probability distributions\nWe call the probability function for a random variable a probability distribution\n\ndescribes how the probabilities are distributed across the possible values of the random variable.\n\n\nDistributions can be discrete or continuous, depending on the type of random variable.\n\nfor discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value.\nfor continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point."
  },
  {
    "objectID": "slides/lecture-02-slides.html#visualizing-random-variables-and-distributions",
    "href": "slides/lecture-02-slides.html#visualizing-random-variables-and-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Visualizing Random Variables and Distributions",
    "text": "Visualizing Random Variables and Distributions\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. We can use a bar chart to visualize the counts of each value.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.figure(figsize=(8, 5))\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\", rwidth=0.8)\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')"
  },
  {
    "objectID": "slides/lecture-02-slides.html#example-dice-rolls",
    "href": "slides/lecture-02-slides.html#example-dice-rolls",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Example: Dice Rolls",
    "text": "Example: Dice Rolls\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times.\nIf we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let’s check it out:\n\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1"
  },
  {
    "objectID": "slides/lecture-02-slides.html#dice-rolls-plot",
    "href": "slides/lecture-02-slides.html#dice-rolls-plot",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Dice Rolls Plot",
    "text": "Dice Rolls Plot\n\n\n\n\n\n\n\n\n\nNotice that when the \\(y\\)-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!"
  },
  {
    "objectID": "slides/lecture-02-slides.html#histograms-for-continuous-random-variables",
    "href": "slides/lecture-02-slides.html#histograms-for-continuous-random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Histograms for Continuous Random Variables",
    "text": "Histograms for Continuous Random Variables\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values.\n\nInstead, we can use a histogram to visualize the distribution of the data.\n\n\nThe histogram is a graphical representation that summarizes the distribution of a dataset.\n\ndivides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin.\nthe height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin.\nif the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin."
  },
  {
    "objectID": "slides/lecture-02-slides.html#airbnb-prices-histogram",
    "href": "slides/lecture-02-slides.html#airbnb-prices-histogram",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Airbnb prices histogram",
    "text": "Airbnb prices histogram\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins."
  },
  {
    "objectID": "slides/lecture-02-slides.html#area-under-a-probability-distribution",
    "href": "slides/lecture-02-slides.html#area-under-a-probability-distribution",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Area under a probability distribution",
    "text": "Area under a probability distribution\n\n\n\n\n\n\n\nArea under a probability distribution\n\n\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. This is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1.\nFor discrete: \\[ \\sum_{x} f(x) = 1 \\] For continuous: \\[ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\]\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable \\(X\\) falls between \\(a\\) and \\(b\\), we can compute the area under the PDF from \\(a\\) to \\(b\\): \\[ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx \\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#expectation",
    "href": "slides/lecture-02-slides.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable.\n\nFor example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\n\n\nWhy do we need an average?\n\nSince a random variable can take on many different values, a single sample does not give you a lot of information.\nYou might win hundreds of dollars on one game, but this does not mean you will win that much every time you play."
  },
  {
    "objectID": "slides/lecture-02-slides.html#thinking-about-averages",
    "href": "slides/lecture-02-slides.html#thinking-about-averages",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Thinking about averages",
    "text": "Thinking about averages\nInstead, think about what would happen if we repeated the random process many times and took the average.\n\nValues that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact.\nFor example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you’re not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero."
  },
  {
    "objectID": "slides/lecture-02-slides.html#gambling-warning",
    "href": "slides/lecture-02-slides.html#gambling-warning",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Gambling warning",
    "text": "Gambling warning\n\n\n\n\n\n\n\nGambling warning: the house always wins\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses."
  },
  {
    "objectID": "slides/lecture-02-slides.html#expectation-formal-definition",
    "href": "slides/lecture-02-slides.html#expectation-formal-definition",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation (formal definition)",
    "text": "Expectation (formal definition)\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\mathbb{E}[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#expectation-as-weighted-average",
    "href": "slides/lecture-02-slides.html#expectation-as-weighted-average",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation as weighted average",
    "text": "Expectation as weighted average\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\n\nSo in our roulette example, you can either lose $1 (with 90% probability) or win $9 (with 10% probability). The expectation would be: \\[\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#sample-mean",
    "href": "slides/lecture-02-slides.html#sample-mean",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Sample mean",
    "text": "Sample mean\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win $10 on one game and lose $1 on the other 9 games. The average outcome is: \\[\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\n\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as \\(\\bar{X}\\), and referred to as the sample mean. \\[\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nThis approximation becomes more accurate as the number of samples \\(n\\) increases. We will talk about this more in a future lecture."
  },
  {
    "objectID": "slides/lecture-02-slides.html#variance-and-standard-deviation",
    "href": "slides/lecture-02-slides.html#variance-and-standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe average is a useful summary of a random variable’s central tendency, but it does not tell us anything about how spread out the values are.\n\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game – the average amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\n\n\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet."
  },
  {
    "objectID": "slides/lecture-02-slides.html#roulette-winnings-visualization",
    "href": "slides/lecture-02-slides.html#roulette-winnings-visualization",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Roulette winnings visualization",
    "text": "Roulette winnings visualization"
  },
  {
    "objectID": "slides/lecture-02-slides.html#variance-intuition",
    "href": "slides/lecture-02-slides.html#variance-intuition",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance (intuition)",
    "text": "Variance (intuition)\nHow can we quantify this spread?\n\nwe want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1.\n\nmaybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout.\nmaybe you only have $10 in your pocket, so you can’t afford to lose all of it on a single game.\n\n\n\nWe need a statistic that captures the typical distance between the values of the random variable and the average value."
  },
  {
    "objectID": "slides/lecture-02-slides.html#computing-distance-from-average",
    "href": "slides/lecture-02-slides.html#computing-distance-from-average",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Computing distance from average",
    "text": "Computing distance from average\nSo let’s compute exactly that - the distance from the average. The formula for distance between two vectors \\(x\\) and \\(y\\) is: \\[\nd^2 = \\sum_{i} (x_i - y_i)^2\n\\] where \\(x_i\\) and \\(y_i\\) are the elements of the two vectors.\n\nThis is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange (\\(a^2 + b^2 = c^2\\)).\n\n\nIn our case, we want to compute the distance between the values of the random variable \\(X\\) and the average value \\(\\mathbb{E}[X]\\). \\[\nd^2 = \\sum_x (x - \\mathbb{E}[X])^2\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#average-distance",
    "href": "slides/lecture-02-slides.html#average-distance",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Average distance",
    "text": "Average distance\nNow we’re getting somewhere! However, this is adding up all of the squared distances – that means that the more values we have, the larger the distance will be.\nThis is not quite right – instead we want to compute the average distance from the mean in order to get a sense of how spread out the values typically are.\n\nSo we need to divide by the number of values: \\[\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n\\]\nSomething should feel familiar about this expression. Recall that the average is related to the expectation."
  },
  {
    "objectID": "slides/lecture-02-slides.html#variance-formula",
    "href": "slides/lecture-02-slides.html#variance-formula",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance formula",
    "text": "Variance formula\nIf we replace the average with the expectation, we get the formula for the variance of a random variable \\(X\\): \\[\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n\\]\n\nThe variance tells us how spread out the values of a random variable are around the average.\n\na larger variance means that the values are more spread out\na smaller variance means that the values are closer to the average."
  },
  {
    "objectID": "slides/lecture-02-slides.html#standard-deviation",
    "href": "slides/lecture-02-slides.html#standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if \\(X\\) represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret.\n\nSo we often take the square root of the variance to get the standard deviation: \\[\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n\\]\n\n\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset: \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#sample-variance-vs.-population-variance",
    "href": "slides/lecture-02-slides.html#sample-variance-vs.-population-variance",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Sample variance vs. population variance",
    "text": "Sample variance vs. population variance\n\n\n\n\n\n\n\nSample variance vs. population variance\n\n\nTechnically, the formula above is an imperfect estimate of the population standard deviation. It’s in general a little bit too small, because the sample mean \\(\\bar{X}\\) does not perfectly represent the population mean \\(\\mathbb{E}[X]\\). We can correct for this by dividing by \\(n-1\\) instead of \\(n\\): \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\] This is called the sample standard deviation.\nWhy is the initial estimate too small? In a small dataset, the sample mean “overfits” the data, meaning it is closer to the individual data points than the true population mean."
  },
  {
    "objectID": "slides/lecture-02-slides.html#common-probability-distributions",
    "href": "slides/lecture-02-slides.html#common-probability-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Common probability distributions",
    "text": "Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. It is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nParameters\nMean\nVariance\n\n\n\n\nBernoulli\nDiscrete\n\\(p \\in [0, 1]\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\nDiscrete\n\\(n \\in \\mathbb{N}, p \\in [0, 1]\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nPoisson\nDiscrete\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\nUniform\nContinuous\n\\(a &lt; b\\)\n\\(\\frac{a+b}{2}\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\n\nNormal\nContinuous\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\nExponential\nContinuous\n\\(\\lambda &gt; 0\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)"
  },
  {
    "objectID": "slides/lecture-02-slides.html#visualizing-distributions",
    "href": "slides/lecture-02-slides.html#visualizing-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions"
  },
  {
    "objectID": "slides/lecture-02-slides.html#summary",
    "href": "slides/lecture-02-slides.html#summary",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. Probability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\nIn particular, we covered:\n\nThe basic definition of probability and how to compute it for simple events.\nThe addition and multiplication rules for calculating probabilities of multiple events.\nThe concept of independence and how it affects probabilities.\nRandom variables and their probability distributions\nThe expectation (or expected value) of a random variable\nVariance and standard deviation\n\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!"
  },
  {
    "objectID": "slides/lecture-04-slides.html#recap-sampling-and-simulation",
    "href": "slides/lecture-04-slides.html#recap-sampling-and-simulation",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Recap: Sampling and Simulation",
    "text": "Recap: Sampling and Simulation\nLast lecture we learned:\n\nHow to sample from probability distributions\nIID sampling and sampling with/without replacement\nHow to simulate complex random processes step by step\n\n\nToday we formalize these ideas with data generating processes and statistical models."
  },
  {
    "objectID": "slides/lecture-04-slides.html#data-generating-processes",
    "href": "slides/lecture-04-slides.html#data-generating-processes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Data Generating Processes",
    "text": "Data Generating Processes\nA key insight: all data is generated by some underlying process\n\nThe process can be simple or complex\nDeterministic or stochastic\nObserved or unobserved\n\n\nThinking about data generating processes (DGPs) is fundamental to analyzing data."
  },
  {
    "objectID": "slides/lecture-04-slides.html#statistical-models",
    "href": "slides/lecture-04-slides.html#statistical-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model is a formal mathematical representation of a DGP.\n\nDescribes the probability distribution of the data\nAllows precise statements about generated data\nTells us likelihood of certain values, expected values, etc."
  },
  {
    "objectID": "slides/lecture-04-slides.html#example-coin-flips",
    "href": "slides/lecture-04-slides.html#example-coin-flips",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Example: Coin Flips",
    "text": "Example: Coin Flips\nDGP: Flipping a coin (heads or tails)\nStatistical Model: Bernoulli distribution\n\\[P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{(heads)} \\\\\n1 - p & \\text{if } X = 0 ~\\text{(tails)}\n\\end{cases}\\]\n\nFor a fair coin: \\(p = 0.5\\)"
  },
  {
    "objectID": "slides/lecture-04-slides.html#models-are-reductive",
    "href": "slides/lecture-04-slides.html#models-are-reductive",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Models are Reductive",
    "text": "Models are Reductive\nThe Bernoulli model ignores many factors:\n\nWeight of the coin\nForce of the flip\nAir resistance\n\n\nBut it accurately describes the outcomes — and that’s what matters!"
  },
  {
    "objectID": "slides/lecture-04-slides.html#example-dice-coin",
    "href": "slides/lecture-04-slides.html#example-dice-coin",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Example: Dice → Coin",
    "text": "Example: Dice → Coin\nRoll a die 100 times. Record whether each roll is even or odd.\n\n3 even numbers (2, 4, 6)\n3 odd numbers (1, 3, 5)\nEach outcome has probability 1/6\n\n\nResult: P(even) = P(odd) = 0.5\nThis is statistically identical to flipping a fair coin!"
  },
  {
    "objectID": "slides/lecture-04-slides.html#dice-coin-visualization",
    "href": "slides/lecture-04-slides.html#dice-coin-visualization",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Dice → Coin Visualization",
    "text": "Dice → Coin Visualization"
  },
  {
    "objectID": "slides/lecture-04-slides.html#the-goal-of-statistical-modeling",
    "href": "slides/lecture-04-slides.html#the-goal-of-statistical-modeling",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "The Goal of Statistical Modeling",
    "text": "The Goal of Statistical Modeling\nFind a simple model that captures essential features of the DGP.\n\nWe don’t always know the true DGP\nOften we don’t know anything about it!\nApproach: “guess and check”\n\nStart simple\nSee how well it describes the data\nTry more complex models if needed"
  },
  {
    "objectID": "slides/lecture-04-slides.html#challenges-with-finite-samples",
    "href": "slides/lecture-04-slides.html#challenges-with-finite-samples",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Challenges with Finite Samples",
    "text": "Challenges with Finite Samples\nHard to tell if you have a good model with limited data.\n\nExtreme example: Flip a coin once\n\nObserved proportion of heads: 0 or 1\nExpected proportion: 0.5\nImpossible to observe the expected proportion!\n\n\n\nYou can’t learn much about tendencies from a single observation."
  },
  {
    "objectID": "slides/lecture-04-slides.html#the-roommate-problem",
    "href": "slides/lecture-04-slides.html#the-roommate-problem",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "The Roommate Problem",
    "text": "The Roommate Problem\nScenario: You and your roommate flip a coin to decide who takes out the trash.\n\nYou always choose tails\nBest of 10 series\nResult: 3 heads, 7 tails\n\n\nYour roommate: “That’s not fair! You rigged the coin!”\nIs your roommate justified?"
  },
  {
    "objectID": "slides/lecture-04-slides.html#distinguishing-models",
    "href": "slides/lecture-04-slides.html#distinguishing-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Distinguishing Models",
    "text": "Distinguishing Models\n\n\n\nScenario\nP(3 heads in 10 flips)\n\n\n\n\nFair coin (p = 0.5)\n~11.7%\n\n\nBiased coin (p = 0.25)\n~25.0%\n\n\n\n\nWith 10 flips, you have some information but not enough to be confident."
  },
  {
    "objectID": "slides/lecture-04-slides.html#distinguishing-models-more-data",
    "href": "slides/lecture-04-slides.html#distinguishing-models-more-data",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Distinguishing Models (More Data)",
    "text": "Distinguishing Models (More Data)\n\n\n\nScenario\nP(30 heads in 100 flips)\n\n\n\n\nFair coin (p = 0.5)\n~0.002%\n\n\nBiased coin (p = 0.25)\n~4.6%\n\n\n\n\nWith more data, evidence becomes much stronger!"
  },
  {
    "objectID": "slides/lecture-04-slides.html#law-of-large-numbers",
    "href": "slides/lecture-04-slides.html#law-of-large-numbers",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nAs sample size increases, the sample mean converges to the population mean.\n\nFormally: Let \\(X_1, X_2, \\ldots, X_n\\) be i.i.d. random variables with expected value \\(\\mathbb{E}[X]\\)\n\\[\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1\\]\n\n\nThe sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges to the true mean."
  },
  {
    "objectID": "slides/lecture-04-slides.html#lln-in-action",
    "href": "slides/lecture-04-slides.html#lln-in-action",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "LLN in Action",
    "text": "LLN in Action"
  },
  {
    "objectID": "slides/lecture-04-slides.html#lln-multiple-simulations",
    "href": "slides/lecture-04-slides.html#lln-multiple-simulations",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "LLN: Multiple Simulations",
    "text": "LLN: Multiple Simulations"
  },
  {
    "objectID": "slides/lecture-04-slides.html#key-insight-from-lln",
    "href": "slides/lecture-04-slides.html#key-insight-from-lln",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Key Insight from LLN",
    "text": "Key Insight from LLN\nAs sample size increases:\n\nSample mean gets closer to true mean (on average)\nVariability decreases — estimates become more consistent\n\n\nPractical implication: More data → more confident conclusions"
  },
  {
    "objectID": "slides/lecture-04-slides.html#unbiased-estimators",
    "href": "slides/lecture-04-slides.html#unbiased-estimators",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Unbiased Estimators",
    "text": "Unbiased Estimators\nNotice: even with high variability in small samples, the sample mean is centered around the true mean.\n\nAn estimator is unbiased if its expected value equals the true parameter.\n\\[\\mathbb{E}[\\bar{X}] = \\mu\\]\n\n\nThe LLN tells us that as \\(n \\to \\infty\\), the variance of the estimator decreases."
  },
  {
    "objectID": "slides/lecture-04-slides.html#central-limit-theorem",
    "href": "slides/lecture-04-slides.html#central-limit-theorem",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe sample mean converges to a normal distribution, regardless of the original distribution!\n\nFormally: Let \\(X_1, \\ldots, X_n\\) be i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\n\\[\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\n\nEven if \\(X\\) is not normal, the average of many \\(X\\)’s is approximately normal!"
  },
  {
    "objectID": "slides/lecture-04-slides.html#clt-example-dice-rolls",
    "href": "slides/lecture-04-slides.html#clt-example-dice-rolls",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "CLT Example: Dice Rolls",
    "text": "CLT Example: Dice Rolls\nRolling a die: uniform distribution from 1 to 6\n\nExpected value: \\(E[X] = 3.5\\)\nClearly not normal\n\n\nBut what about the average of many dice rolls?"
  },
  {
    "objectID": "slides/lecture-04-slides.html#clt-example-single-die",
    "href": "slides/lecture-04-slides.html#clt-example-single-die",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "CLT Example: Single Die",
    "text": "CLT Example: Single Die"
  },
  {
    "objectID": "slides/lecture-04-slides.html#clt-in-action",
    "href": "slides/lecture-04-slides.html#clt-in-action",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "CLT in Action",
    "text": "CLT in Action"
  },
  {
    "objectID": "slides/lecture-04-slides.html#why-clt-matters",
    "href": "slides/lecture-04-slides.html#why-clt-matters",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Why CLT Matters",
    "text": "Why CLT Matters\nEven when we don’t know the original distribution:\n\nWe can use the normal distribution to model the sample mean\nThis works for a huge class of DGPs\n\n\nThe normal distribution is:\n\nSymmetric\nWell-studied\nEasy to work with mathematically"
  },
  {
    "objectID": "slides/lecture-04-slides.html#standard-errors",
    "href": "slides/lecture-04-slides.html#standard-errors",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Standard Errors",
    "text": "Standard Errors\nThe CLT tells us the sample mean has variance \\(\\frac{\\sigma^2}{n}\\)\n\nStandard error = standard deviation of the sample mean:\n\\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\n\n\nAs \\(n\\) increases, SE decreases → estimates become more precise"
  },
  {
    "objectID": "slides/lecture-04-slides.html#standard-errors-visualized",
    "href": "slides/lecture-04-slides.html#standard-errors-visualized",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Standard Errors Visualized",
    "text": "Standard Errors Visualized"
  },
  {
    "objectID": "slides/lecture-04-slides.html#summary",
    "href": "slides/lecture-04-slides.html#summary",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Summary",
    "text": "Summary\nData Generating Processes\n\nAll data comes from some underlying process\nStatistical models are simplified representations of DGPs\n\n\nLaw of Large Numbers\n\nSample means converge to true means as \\(n \\to \\infty\\)\nMore data → more confident conclusions\n\n\n\nCentral Limit Theorem\n\nSample means are approximately normal (for large \\(n\\))\nStandard error decreases as \\(\\frac{1}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides/lecture-04-slides.html#next-time",
    "href": "slides/lecture-04-slides.html#next-time",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Next Time",
    "text": "Next Time\nHypothesis Testing\n\nHow to formally test claims about data\nUsing sampling distributions to quantify uncertainty\nMaking decisions under uncertainty"
  },
  {
    "objectID": "slides/lecture-06-slides.html#recap",
    "href": "slides/lecture-06-slides.html#recap",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Recap",
    "text": "Recap\n\nHypothesis testing: quantify evidence against \\(H_0\\) with p-values\np-value: probability of observing data as extreme as ours, under \\(H_0\\)\n\n\nToday: A different way to quantify uncertainty — confidence intervals"
  },
  {
    "objectID": "slides/lecture-06-slides.html#confidence-intervals",
    "href": "slides/lecture-06-slides.html#confidence-intervals",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nInstead of asking “is \\(\\mu = 0\\)?”, ask:\n“What is a plausible range of values for \\(\\mu\\)?”\n\nA confidence interval gives a range \\([L, U]\\) that likely contains the true parameter:\n\\[\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\\]"
  },
  {
    "objectID": "slides/lecture-06-slides.html#important-distinction",
    "href": "slides/lecture-06-slides.html#important-distinction",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Important Distinction",
    "text": "Important Distinction\nThe parameter \\(\\theta\\) is fixed (but unknown)!\n\nWhat varies is the sample we draw.\n\nDifferent samples → different confidence intervals\nSome CIs contain \\(\\theta\\), some don’t\n\\((1-\\alpha)\\)% of all CIs will contain \\(\\theta\\)"
  },
  {
    "objectID": "slides/lecture-06-slides.html#ci-when-distribution-is-known",
    "href": "slides/lecture-06-slides.html#ci-when-distribution-is-known",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "CI When Distribution is Known",
    "text": "CI When Distribution is Known\nBy the Central Limit Theorem, sample mean \\(\\bar{X}\\) is approximately normal.\n\nWe know:\n\n\\(\\bar{X}\\) is unbiased: \\(\\mathbb{E}[\\bar{X}] = \\mu\\)\nVariance: \\(\\text{Var}(\\bar{X}) = \\sigma^2/n\\)\nDistribution is symmetric"
  },
  {
    "objectID": "slides/lecture-06-slides.html#visualizing-the-sampling-distribution",
    "href": "slides/lecture-06-slides.html#visualizing-the-sampling-distribution",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Visualizing the Sampling Distribution",
    "text": "Visualizing the Sampling Distribution"
  },
  {
    "objectID": "slides/lecture-06-slides.html#probability-within-z-standard-errors",
    "href": "slides/lecture-06-slides.html#probability-within-z-standard-errors",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Probability Within \\(z\\) Standard Errors",
    "text": "Probability Within \\(z\\) Standard Errors\n\n\n\n\\(z\\)\nProbability within \\(\\pm z\\) SE\n\n\n\n\n1\n68.3%\n\n\n2\n95.4%\n\n\n3\n99.7%\n\n\n\n\nFor a 95% CI, use \\(z_{0.025} \\approx 1.96\\)"
  },
  {
    "objectID": "slides/lecture-06-slides.html#formula-for-ci",
    "href": "slides/lecture-06-slides.html#formula-for-ci",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Formula for CI",
    "text": "Formula for CI\n\\[\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\\]\n\nThis requires knowing (or estimating) \\(\\sigma\\)!"
  },
  {
    "objectID": "slides/lecture-06-slides.html#the-problem",
    "href": "slides/lecture-06-slides.html#the-problem",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "The Problem",
    "text": "The Problem\nWhat if we don’t know the distribution of our data?\n\nCLT helps for means, but needs large \\(n\\)\nWhat about other statistics (median, variance)?\n\n\nSolution: Use the data itself to estimate uncertainty!"
  },
  {
    "objectID": "slides/lecture-06-slides.html#resampling-methods",
    "href": "slides/lecture-06-slides.html#resampling-methods",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Resampling Methods",
    "text": "Resampling Methods\nKey insight: If sample is large enough, it approximates the population.\n\nSo we can sub-sample from our data to understand variability!"
  },
  {
    "objectID": "slides/lecture-06-slides.html#samples-approach-population",
    "href": "slides/lecture-06-slides.html#samples-approach-population",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Samples Approach Population",
    "text": "Samples Approach Population"
  },
  {
    "objectID": "slides/lecture-06-slides.html#resampling-from-the-sample",
    "href": "slides/lecture-06-slides.html#resampling-from-the-sample",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Resampling from the Sample",
    "text": "Resampling from the Sample"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrapping",
    "href": "slides/lecture-06-slides.html#bootstrapping",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrap = repeatedly resample with replacement from your data\n\nAlgorithm:\n\nGiven dataset of size \\(n\\), draw sample of size \\(n\\) with replacement\nCompute statistic (mean, median, etc.) on bootstrap sample\nRepeat many times (e.g., 1000)\nUse distribution of statistics for inference"
  },
  {
    "objectID": "slides/lecture-06-slides.html#why-sample-with-replacement",
    "href": "slides/lecture-06-slides.html#why-sample-with-replacement",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Why Sample With Replacement?",
    "text": "Why Sample With Replacement?\nWe want independent samples from our proxy population.\n\nWithout replacement: we’d get the same sample back every time!\n(Only \\(n\\) datapoints, so sampling \\(n\\) without replacement = original sample)"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-example",
    "href": "slides/lecture-06-slides.html#bootstrap-example",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap Example",
    "text": "Bootstrap Example\n\n\nCode\nrng = np.random.default_rng(42)\nsample_size = 1000\nn_bootstraps = 1000\n\noriginal_sample = rng.normal(loc=3.2, scale=1.5, size=sample_size)\n\nbootstrapped_means = []\nbootstrapped_std = []\nfor _ in range(n_bootstraps):\n    resample = rng.choice(original_sample, size=sample_size, replace=True)\n    bootstrapped_means.append(np.mean(resample))\n    bootstrapped_std.append(np.std(resample, ddof=1))\n\nprint(f\"True mean: 3.2, Bootstrap mean: {np.mean(bootstrapped_means):.2f}\")\nprint(f\"True std: 1.5, Bootstrap std: {np.mean(bootstrapped_std):.2f}\")\n\n\nTrue mean: 3.2, Bootstrap mean: 3.16\nTrue std: 1.5, Bootstrap std: 1.48"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-distributions",
    "href": "slides/lecture-06-slides.html#bootstrap-distributions",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap Distributions",
    "text": "Bootstrap Distributions"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-confidence-intervals",
    "href": "slides/lecture-06-slides.html#bootstrap-confidence-intervals",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap Confidence Intervals",
    "text": "Bootstrap Confidence Intervals\nThe magic: use percentiles of bootstrap distribution!\n\nFor 95% CI:\n\nLower bound = 2.5th percentile\nUpper bound = 97.5th percentile"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-ci-example",
    "href": "slides/lecture-06-slides.html#bootstrap-ci-example",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap CI Example",
    "text": "Bootstrap CI Example\n\n\nCode\nlower_bound_mean = np.percentile(bootstrapped_means, 2.5)\nupper_bound_mean = np.percentile(bootstrapped_means, 97.5)\nprint(f\"95% CI for Mean: ({lower_bound_mean:.3f}, {upper_bound_mean:.3f})\")\nprint(f\"True mean: 3.2\")\n\nlower_bound_std = np.percentile(bootstrapped_std, 2.5)\nupper_bound_std = np.percentile(bootstrapped_std, 97.5)\nprint(f\"95% CI for Std: ({lower_bound_std:.3f}, {upper_bound_std:.3f})\")\nprint(f\"True std: 1.5\")\n\n\n95% CI for Mean: (3.066, 3.247)\nTrue mean: 3.2\n95% CI for Std: (1.414, 1.548)\nTrue std: 1.5"
  },
  {
    "objectID": "slides/lecture-06-slides.html#ci-visualization",
    "href": "slides/lecture-06-slides.html#ci-visualization",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "CI Visualization",
    "text": "CI Visualization"
  },
  {
    "objectID": "slides/lecture-06-slides.html#what-does-a-95-ci-mean",
    "href": "slides/lecture-06-slides.html#what-does-a-95-ci-mean",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "What Does a 95% CI Mean?",
    "text": "What Does a 95% CI Mean?\nThe uncertainty comes from sampling, not the parameter!\n\nIf we repeatedly:\n\nDraw a new sample from the population\nCompute a 95% CI\n\n\n\nThen 95% of those CIs will contain the true parameter."
  },
  {
    "objectID": "slides/lecture-06-slides.html#demonstrating-ci-coverage",
    "href": "slides/lecture-06-slides.html#demonstrating-ci-coverage",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Demonstrating CI Coverage",
    "text": "Demonstrating CI Coverage\n\n\nProportion of CIs that MISS the true mean: 0.0580\n(Expected: ~0.05)"
  },
  {
    "objectID": "slides/lecture-06-slides.html#visualizing-ci-coverage",
    "href": "slides/lecture-06-slides.html#visualizing-ci-coverage",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Visualizing CI Coverage",
    "text": "Visualizing CI Coverage"
  },
  {
    "objectID": "slides/lecture-06-slides.html#application-nba-scoring",
    "href": "slides/lecture-06-slides.html#application-nba-scoring",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Application: NBA Scoring",
    "text": "Application: NBA Scoring\nLet’s compare SGA and Giannis with bootstrap CIs!\n\n\nSGA 95% CI: (30.97, 34.42)\nGiannis 95% CI: (28.70, 32.09)"
  },
  {
    "objectID": "slides/lecture-06-slides.html#overlapping-cis",
    "href": "slides/lecture-06-slides.html#overlapping-cis",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Overlapping CIs",
    "text": "Overlapping CIs\nSGA’s CI doesn’t contain Giannis’s point estimate, but…\n\nThe CIs overlap!\n\n\nThis means we should account for both players’ variability when comparing them."
  },
  {
    "objectID": "slides/lecture-06-slides.html#better-comparison-difference-of-means",
    "href": "slides/lecture-06-slides.html#better-comparison-difference-of-means",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Better Comparison: Difference of Means",
    "text": "Better Comparison: Difference of Means\n\n\nP(Giannis &gt;= SGA): 0.0340"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-distributions-comparison",
    "href": "slides/lecture-06-slides.html#bootstrap-distributions-comparison",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap Distributions Comparison",
    "text": "Bootstrap Distributions Comparison"
  },
  {
    "objectID": "slides/lecture-06-slides.html#bootstrap-under-the-null",
    "href": "slides/lecture-06-slides.html#bootstrap-under-the-null",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrap Under the Null",
    "text": "Bootstrap Under the Null\nFor a proper hypothesis test, combine both players’ data:\n\n\nCode\nnp.random.seed(42)\nn_games_sga = len(compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"])\nn_games_giannis = len(compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"])\nobserved_diff = (compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"].mean()\n                - compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"].mean())\n\nn_bootstraps = 1000\nbootstrapped_diffs = []\nfor _ in range(n_bootstraps):\n    sga_sample = compare_df[\"PTS\"].sample(n=n_games_sga, replace=True)\n    giannis_sample = compare_df[\"PTS\"].sample(n=n_games_giannis, replace=True)\n    diff = sga_sample.mean() - giannis_sample.mean()\n    bootstrapped_diffs.append(diff)\n\np_value = np.mean(np.array(bootstrapped_diffs) &gt;= observed_diff)\nprint(f\"Observed difference: {observed_diff:.2f}\")\nprint(f\"Bootstrap p-value: {p_value:.4f}\")\n\n\nObserved difference: 2.30\nBootstrap p-value: 0.0440"
  },
  {
    "objectID": "slides/lecture-06-slides.html#key-insight",
    "href": "slides/lecture-06-slides.html#key-insight",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Key Insight",
    "text": "Key Insight\nOriginal p-value (last lecture): ~0.004\nBootstrap p-value (accounting for both variabilities): ~0.04\n\nAn order of magnitude difference!\n\n\nProperly accounting for uncertainty matters!"
  },
  {
    "objectID": "slides/lecture-06-slides.html#summary",
    "href": "slides/lecture-06-slides.html#summary",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Summary",
    "text": "Summary\nConfidence Intervals\n\nGive a range of plausible values for parameters\n95% CI means 95% of such CIs contain the true value\n\n\nBootstrapping\n\nResample with replacement from your data\nEstimate any statistic’s distribution\nNo distributional assumptions needed!\n\n\n\nApplication\n\nBootstrap CIs are easy to compute\nCan test hypotheses by comparing distributions"
  },
  {
    "objectID": "slides/lecture-06-slides.html#next-time",
    "href": "slides/lecture-06-slides.html#next-time",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Next Time",
    "text": "Next Time\nPermutation Tests\n\nAnother simulation-based method\nPowerful and widely applicable\nThe “one test” unifying framework"
  },
  {
    "objectID": "slides/lecture-08-slides.html#from-inference-to-prediction",
    "href": "slides/lecture-08-slides.html#from-inference-to-prediction",
    "title": "Lecture 08: Linear Regression",
    "section": "From Inference to Prediction",
    "text": "From Inference to Prediction\nSo far we’ve focused on inference:\n\nEstimating parameters\nTesting hypotheses\nQuantifying uncertainty\n\n\nNow: Prediction — using data to forecast outcomes"
  },
  {
    "objectID": "slides/lecture-08-slides.html#predictions-from-patterns",
    "href": "slides/lecture-08-slides.html#predictions-from-patterns",
    "title": "Lecture 08: Linear Regression",
    "section": "Predictions from Patterns",
    "text": "Predictions from Patterns\nHow do we make predictions?\n\nLook for patterns in data\nUse patterns to make informed guesses\n\n\nWhy not just memorize?\n\nMemorization doesn’t generalize\nWe need predictions for new, unseen data"
  },
  {
    "objectID": "slides/lecture-08-slides.html#the-galton-dataset",
    "href": "slides/lecture-08-slides.html#the-galton-dataset",
    "title": "Lecture 08: Linear Regression",
    "section": "The Galton Dataset",
    "text": "The Galton Dataset\nClassic dataset: heights of parents and children\n\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\ngender\nheight\nkids\nmale\nfemale\n\n\n\n\n0\n1\n78.5\n67.0\nM\n73.2\n4\nTrue\nFalse\n\n\n1\n1\n78.5\n67.0\nF\n69.2\n4\nFalse\nTrue\n\n\n2\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n3\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n4\n2\n75.5\n66.5\nM\n73.5\n4\nTrue\nFalse"
  },
  {
    "objectID": "slides/lecture-08-slides.html#the-simplest-prediction",
    "href": "slides/lecture-08-slides.html#the-simplest-prediction",
    "title": "Lecture 08: Linear Regression",
    "section": "The Simplest Prediction",
    "text": "The Simplest Prediction\nTask: Predict height of a new child (no other information)\n\nSolution: Use the average height!\n\n\nMean height: 66.76 inches"
  },
  {
    "objectID": "slides/lecture-08-slides.html#distribution-of-heights",
    "href": "slides/lecture-08-slides.html#distribution-of-heights",
    "title": "Lecture 08: Linear Regression",
    "section": "Distribution of Heights",
    "text": "Distribution of Heights"
  },
  {
    "objectID": "slides/lecture-08-slides.html#can-we-do-better",
    "href": "slides/lecture-08-slides.html#can-we-do-better",
    "title": "Lecture 08: Linear Regression",
    "section": "Can We Do Better?",
    "text": "Can We Do Better?\nWhat if we have more information?\n\nChild’s sex\nParents’ heights\n\n\nIf these are related to height, they should inform our prediction!"
  },
  {
    "objectID": "slides/lecture-08-slides.html#child-height-vs-parent-height",
    "href": "slides/lecture-08-slides.html#child-height-vs-parent-height",
    "title": "Lecture 08: Linear Regression",
    "section": "Child Height vs Parent Height",
    "text": "Child Height vs Parent Height"
  },
  {
    "objectID": "slides/lecture-08-slides.html#the-pattern",
    "href": "slides/lecture-08-slides.html#the-pattern",
    "title": "Lecture 08: Linear Regression",
    "section": "The Pattern",
    "text": "The Pattern\nTaller parents → taller children (generally)\n\nBut how much taller?\n\n\nWe need to quantify this relationship."
  },
  {
    "objectID": "slides/lecture-08-slides.html#combining-parent-heights",
    "href": "slides/lecture-08-slides.html#combining-parent-heights",
    "title": "Lecture 08: Linear Regression",
    "section": "Combining Parent Heights",
    "text": "Combining Parent Heights\nProblem: Father and mother heights are on different scales\n\nSolution: Standardize (z-score) heights\n\\[z = \\frac{\\text{height} - \\text{mean}}{\\text{std dev}}\\]\n\n\nThen combine: midparent height = average of z-scores"
  },
  {
    "objectID": "slides/lecture-08-slides.html#standardization",
    "href": "slides/lecture-08-slides.html#standardization",
    "title": "Lecture 08: Linear Regression",
    "section": "Standardization",
    "text": "Standardization"
  },
  {
    "objectID": "slides/lecture-08-slides.html#child-height-vs-midparent-height",
    "href": "slides/lecture-08-slides.html#child-height-vs-midparent-height",
    "title": "Lecture 08: Linear Regression",
    "section": "Child Height vs Midparent Height",
    "text": "Child Height vs Midparent Height"
  },
  {
    "objectID": "slides/lecture-08-slides.html#linear-regression",
    "href": "slides/lecture-08-slides.html#linear-regression",
    "title": "Lecture 08: Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nModel the relationship as a line:\n\\[\\text{predicted height} = \\text{slope} \\times \\text{midparent} + \\text{intercept}\\]\n\nOrdinary Least Squares (OLS): Find the line that minimizes squared errors\n\\[\\text{minimize } \\sum_i (\\text{predicted}_i - \\text{actual}_i)^2\\]"
  },
  {
    "objectID": "slides/lecture-08-slides.html#why-squared-errors",
    "href": "slides/lecture-08-slides.html#why-squared-errors",
    "title": "Lecture 08: Linear Regression",
    "section": "Why Squared Errors?",
    "text": "Why Squared Errors?\n\nNo cancellation of positive/negative errors\nLarger errors are penalized more heavily\nMathematically convenient (differentiable)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#fitting-the-model",
    "href": "slides/lecture-08-slides.html#fitting-the-model",
    "title": "Lecture 08: Linear Regression",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nCode\nmodel = ols(\"height ~ midparent\", data=galton).fit()\npredicted_heights = model.predict(galton[\"midparent\"]).values\nprint(f\"Intercept: {model.params['Intercept']:.2f}\")\nprint(f\"Slope: {model.params['midparent']:.2f}\")\n\n\nIntercept: 66.77\nSlope: 1.66"
  },
  {
    "objectID": "slides/lecture-08-slides.html#visualizing-the-fit",
    "href": "slides/lecture-08-slides.html#visualizing-the-fit",
    "title": "Lecture 08: Linear Regression",
    "section": "Visualizing the Fit",
    "text": "Visualizing the Fit"
  },
  {
    "objectID": "slides/lecture-08-slides.html#interpreting-the-slope",
    "href": "slides/lecture-08-slides.html#interpreting-the-slope",
    "title": "Lecture 08: Linear Regression",
    "section": "Interpreting the Slope",
    "text": "Interpreting the Slope\nSlope ≈ 1.7\n\nInterpretation: For every 1 standard deviation increase in midparent height, predicted child height increases by 1.7 inches."
  },
  {
    "objectID": "slides/lecture-08-slides.html#prediction-errors-residuals",
    "href": "slides/lecture-08-slides.html#prediction-errors-residuals",
    "title": "Lecture 08: Linear Regression",
    "section": "Prediction Errors (Residuals)",
    "text": "Prediction Errors (Residuals)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#evaluating-predictions",
    "href": "slides/lecture-08-slides.html#evaluating-predictions",
    "title": "Lecture 08: Linear Regression",
    "section": "Evaluating Predictions",
    "text": "Evaluating Predictions\n\n\nCode\nprint(f\"Mean error: {np.mean(errors):.2e}\")\nprint(f\"Root Mean Squared Error: {np.sqrt(np.mean(errors**2)):.2f} inches\")\n\n# Compare to naive prediction\nnaive_rmse = np.sqrt(np.mean((galton[\"height\"] - mean_height) ** 2))\nprint(f\"Naive RMSE (just use mean): {naive_rmse:.2f} inches\")\n\n\nMean error: -2.43e-14\nRoot Mean Squared Error: 3.39 inches\nNaive RMSE (just use mean): 3.58 inches\n\n\n\nUsing parent info improves predictions! (3.39 vs 3.58 inches)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#the-probabilistic-view",
    "href": "slides/lecture-08-slides.html#the-probabilistic-view",
    "title": "Lecture 08: Linear Regression",
    "section": "The Probabilistic View",
    "text": "The Probabilistic View\nNotice: residuals are approximately normal!\n\nThis motivates the probabilistic regression model:\n\\[Y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X, \\sigma^2)\\]\n\n\nThe response \\(Y\\) is a random variable with:\n\nMean that depends on \\(X\\)\nInherent variability \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#uncertainty-in-regression",
    "href": "slides/lecture-08-slides.html#uncertainty-in-regression",
    "title": "Lecture 08: Linear Regression",
    "section": "Uncertainty in Regression",
    "text": "Uncertainty in Regression\nBecause \\(Y\\) is random:\n\nDifferent samples → different estimates of \\(\\beta_0\\), \\(\\beta_1\\)\nNeed to quantify uncertainty (next lecture!)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#correlation",
    "href": "slides/lecture-08-slides.html#correlation",
    "title": "Lecture 08: Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nCorrelation coefficient (\\(r\\)): measures linear relationship strength\n\n\n\n\nValue\nInterpretation\n\n\n\n\n\\(r = 1\\)\nPerfect positive correlation\n\n\n\\(r = -1\\)\nPerfect negative correlation\n\n\n\\(r = 0\\)\nNo linear relationship"
  },
  {
    "objectID": "slides/lecture-08-slides.html#correlation-standardized-slope",
    "href": "slides/lecture-08-slides.html#correlation-standardized-slope",
    "title": "Lecture 08: Linear Regression",
    "section": "Correlation = Standardized Slope",
    "text": "Correlation = Standardized Slope\nKey insight: When both variables are standardized, the slope equals the correlation!\n\\[r = \\text{slope when } X \\text{ and } Y \\text{ are z-scored}\\]"
  },
  {
    "objectID": "slides/lecture-08-slides.html#demonstration",
    "href": "slides/lecture-08-slides.html#demonstration",
    "title": "Lecture 08: Linear Regression",
    "section": "Demonstration",
    "text": "Demonstration"
  },
  {
    "objectID": "slides/lecture-08-slides.html#galton-data-correlation",
    "href": "slides/lecture-08-slides.html#galton-data-correlation",
    "title": "Lecture 08: Linear Regression",
    "section": "Galton Data: Correlation",
    "text": "Galton Data: Correlation"
  },
  {
    "objectID": "slides/lecture-08-slides.html#summary",
    "href": "slides/lecture-08-slides.html#summary",
    "title": "Lecture 08: Linear Regression",
    "section": "Summary",
    "text": "Summary\nLinear Regression\n\nModel relationships as linear equations\nOLS finds the best-fitting line\nSlope quantifies the relationship\n\n\nCorrelation\n\nMeasures linear relationship strength\nEquals slope when variables are standardized\n\n\n\nProbabilistic Interpretation\n\n\\(Y\\) is a random variable\nRegression estimates expected value given \\(X\\)"
  },
  {
    "objectID": "slides/lecture-08-slides.html#next-time",
    "href": "slides/lecture-08-slides.html#next-time",
    "title": "Lecture 08: Linear Regression",
    "section": "Next Time",
    "text": "Next Time\nRegression Inference and Multiple Regression\n\nHypothesis tests for regression coefficients\nConfidence intervals for predictions\nMultiple predictors simultaneously"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: Understanding Uncertainty: An Introduction to Statistics through Data Generating Processes and Computational Simulations\nInstructor: Joey Rudoler\nClassroom: Academic Research Building (ARB), Room 401\nCourse Schedule:\n\nTentative schedule, subject to change.\n\n\nDate\nTopic\nAssignment\n\n\n\n\n7/09\nIntro, Why Statistics? (slides)\nAssignment 0\n\n\n7/10\nProgramming (slides)\nAssignment 1\n\n\n7/11\nProgramming cont.\n\n\n\n7/14\nProbability (slides)\nAssignment 2\n\n\n7/15\nProbability cont.\n\n\n\n7/16\nSampling and simulation (slides)\nAssignment 3\n\n\n7/17\nData Generating Processes and Statistical Models (slides)\n\n\n\n7/18\nNo class\n\n\n\n7/21\nHypothesis Testing (slides)\n\n\n\n7/22\nConfidence Intervals, bootstrapping (slides)\nAssignment 4\n\n\n7/23\nPermutation tests (slides)\n\n\n\n7/24\nRegression (slides)\n\n\n\n7/25\nNo class (final project meetings)\n\n\n\n7/28\nNo class (DC trip)\n\n\n\n7/29\nRegression cont. (Inference for Regression) (slides)\n\n\n\n7/30\nRegression cont. (Multiple Regression)\n\n\n\n7/31\nParadoxes and Statistical Gotchas\n\n\n\n8/01\nGuest Lecture (Sports Analytics and Betting)\n\n\n\n8/04\nGuest Lecture / Final Projects\n\n\n\n8/05\nFinal Projects\n\n\n\n8/06\nFinal Projects\n\n\n\n8/07\nFinal Projects\n\n\n\n8/08\nFinal Project Presentations (10:00 AM)\nFinal Project Guidelines"
  },
  {
    "objectID": "notebooks/lecture-01.html",
    "href": "notebooks/lecture-01.html",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "",
    "text": "This lecture will be, intentionally, a bit of a whirlwind. That’s because with the advent of large language models (LLMs) like ChatGPT, Claude, Gemini, etc. knowing how to program in specific languages like Python is becoming less important. You don’t need that much practice or to focus on the syntax of a specific language.\nInstead, the important thing is to understand the core concepts involved in programming, which are largely universal across languages. This high-level understanding will allow you to use LLMs effectively to write code in any language, including Python. If you don’t understand the concepts, you won’t be able to identify when the LLM is making mistakes or producing suboptimal code.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#variables-and-types",
    "href": "notebooks/lecture-01.html#variables-and-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables and types",
    "text": "Variables and types\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\n\n\n\n\n\n\nFunctions act on variables\n\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable.\n\n\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations.\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger (int)\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat (float)\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString (str)\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList (list)\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary (dict)\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean (bool)\nTrue or False values, e.g., True, False\n\n\n\nWe will discuss a few important ones in more detail\n\n\n\n\n\n\nEverything is an object\n\n\n\nIn Python, everything is an object. This means that even basic data types like integers and strings are treated as objects with methods and properties. For example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\nSee the later section on Object-Oriented Programming for more details.\n\n\n\nTypecasting\nTypecasting is the process of converting a variable from one type to another. Not all type conversions are allowed, but some common ones include:\n\n\n\n\n\n\n\n\nFrom Type\nTo Type\nExample\n\n\n\n\nInteger\nFloat\nfloat(5) → 5.0\n\n\nFloat\nInteger\nint(3.14) → 3\n\n\nString\nInteger\nint('42') → 42\n\n\nString\nFloat\nfloat('3.14') → 3.14\n\n\nList\nString\n''.join(['S', 'h', 'a', 'l', 'o', 'm']) → 'Shalom'\n\n\nString\nList\nlist('Shalom') → ['S', 'h', 'a', 'l', 'o', 'm']\n\n\nBoolean\nInteger\nint(True) → 1, int(False) → 0\n\n\nInteger\nBoolean\nbool(1) → True, bool(0) → False, bool(-1) → True\n\n\n\nThere are also some implicit type conversions that happen automatically in Python, such as when you perform arithmetic operations between integers and floats. For example:\n\nx = 5        # Integer\ny = 2.0      # Float\nresult = x + y  # Implicit conversion to float\nresult, type(result)  # result is now a float\n\n(7.0, float)",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#lists",
    "href": "notebooks/lecture-01.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\nWe often need to store multiple values together. The most basic way to achieve this is with a list. A list is an ordered collection of items that can be of any type, including other lists. “Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\n1print(my_list[0])\n\n\n1\n\nA code annotation\n\n\n\n\n1\n\n\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\n[1, 2, 3]\n['apple', 'banana']\n[1, 3, 'banana']\n['banana', 'apple', 3, 2, 1]\n\n\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#arrays-numpy",
    "href": "notebooks/lecture-01.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\nPackages\n\n\n\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array().\n\n\n\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]\n\n\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]\n\n\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\nTypecasting works in NumPy arrays as well via the astype() method.\n\nnp.array([1, 2, 3, 4.1], dtype=float).astype(int)  # Convert float array to int array\n\narray([1, 2, 3, 4])\n\n\n\nAdvanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]\n\n\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [ 1  2  3  4  5  6  7  8  9 10]\nBoolean mask: [False False  True  True  True  True  True  True  True  True]\nFiltered array: [ 3  4  5  6  7  8  9 10]",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dictionaries",
    "href": "notebooks/lecture-01.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}\n\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25\n\n\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dataframes",
    "href": "notebooks/lecture-01.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns). Think of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago\n\n\n\n\n\n\n\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#conditional-logic",
    "href": "notebooks/lecture-01.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false.\n\n\n\nHere’s an example of how to use these statements. Play around with the code below to see how it works. You can change the value of age to see how the output changes based on different conditions.\n\n\n\n\n\n\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false.\n\n\n\n\n\n\nBoolean expressions\n\n\n\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#loops",
    "href": "notebooks/lecture-01.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops.\n\nFor Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one.\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4. The enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n\n\n\n\nWhile Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#functions-and-functional-programming",
    "href": "notebooks/lecture-01.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result). You can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters. Here’s an example:\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\n1 Technically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing.Probably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]\n\n\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#object-oriented-programming",
    "href": "notebooks/lecture-01.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and \\ behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created.\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse\n\n\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality.\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\n\n\n\n\n\n\n\n\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#summary",
    "href": "notebooks/lecture-01.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html",
    "href": "notebooks/lecture-03.html",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "",
    "text": "Now that we have a good understanding of the basics of probability, we can start to explore how we deal with randomness computationally.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "href": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from probability distributions",
    "text": "Sampling from probability distributions\n\nA sample is a subset of data drawn from a more general population. That population can be thought of as a probability distribution – this distribution essentially describes how likely you are to observe different values when you sample from it.\nWe will quickly review some important concepts related to sampling.\n\nIndependent and identically distributed (IID) sampling\nWhen we sample from a probability distribution, we often assume that the samples are independent and identically distributed (IID). This means that each sample is drawn from the same distribution and that the samples do not influence each other.\nCoin flips are a good example of IID sampling. If you flip a fair coin multiple times, each flip has the same probability of being heads or tails (this is the “identically distributed” part), and the outcome of one flip does not affect the outcome of another (this is the “independent” part). The same is true for rolling a die!\nWe often apply this concept to more complex random processes as well, where we do not have such a clear understanding of the underlying process. For example, if we are sampling the heights of people in a city, we might assume that each person’s height is drawn from the same distribution (the distribution of heights in that city) and that one person’s height does not affect another’s. Whether or not the IID assumption holds in practice is an important question to consider when analyzing data – for example, do you think that the heights of people in a family are independent of each other?\n\n\nSampling with and without replacement\nAnother important concept in sampling is the distinction between sampling with replacement and sampling without replacement.\n\nSampling with replacement means that after we draw a sample from the population, we put it back before drawing the next sample. This means that the same object / instance can be selected multiple times.\nSampling without replacement means that once we draw a sample, we do not put it back before drawing the next sample. This means that each individual can only be selected once. This can introduce dependencies between samples, as the population changes after each draw.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#simulating-a-random-sample",
    "href": "notebooks/lecture-03.html#simulating-a-random-sample",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating a random sample",
    "text": "Simulating a random sample\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code.\n\n\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\nNormal distribution\n\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean).\n\n\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\n1rng = np.random.default_rng(seed=42)\n2samples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\n3plt.figure(figsize=(8, 5))\n4plt.hist(samples, bins=10, density=True)\n5plt.show()\n\n\n\n1\n\nCreate a random number generator with a fixed seed\n\n2\n\nSample 100 times from a normal distribution with mean 0 and standard deviation 1\n\n3\n\nCreate a figure with a specific size\n\n4\n\nPlot a histogram of the samples with 10 bins and normalized (bins add up to 1) to show the probability density\n\n5\n\nShow the plot\n\n\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]\n\n\n\n\n\n\n\n\n\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\nsubsample = rng.choice(samples, size=1000, replace=True)\nplt.hist(subsample, bins=10, density=True) \n\n\n(array([0.06841478, 0.18814065, 0.27610251, 0.44469608, 0.3103099 ,\n        0.58152565, 0.425149  , 0.10017879, 0.02443385, 0.02443385]),\n array([-1.95103519, -1.54176691, -1.13249863, -0.72323035, -0.31396207,\n         0.09530621,  0.50457449,  0.91384276,  1.32311104,  1.73237932,\n         2.1416476 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrng = np.random.default_rng(seed=42)\n1subsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n2pd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10, replace=False, random_state=42)\n\n\n\n1\n\nSample from the dataset without replacement\n\n2\n\nAnother way to sample; note RNG can be different in different packages\n\n\n\n\nSample:\n [-1.32269961 -1.13328721 -0.01680116 -1.68286977  0.54315427 -1.68733443\n  0.85797588 -0.85304393 -0.04992591 -0.36305385]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n83\n-0.381738\n\n\n53\n-0.319671\n\n\n70\n-1.275686\n\n\n45\n0.218689\n\n\n44\n0.116686\n\n\n39\n0.650593\n\n\n22\n1.222541\n\n\n80\n0.456775\n\n\n10\n0.879398\n\n\n0\n0.304717\n\n\n\n\n\n\n\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\n1possible_rolls = [1, 2, 3, 4, 5, 6]\n2probabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\n3rng.choice(possible_rolls, p=probabilities, size=100)\n\n\n\n1\n\nSet the values to choose from\n\n2\n\nSet the probabilities for each value (rolling a 6 has a 50% chance, rolling 1-5 has a 10% chance each)\n\n3\n\nSample 100 times from the specified distribution\n\n\n\n\narray([4, 6, 6, 6, 5, 3, 6, 1, 6, 6, 6, 4, 6, 6, 6, 2, 5, 1, 2, 6, 6, 6,\n       4, 4, 5, 2, 2, 5, 3, 6, 5, 6, 6, 4, 6, 6, 4, 3, 6, 2, 2, 1, 6, 6,\n       6, 6, 5, 6, 2, 2, 6, 5, 6, 6, 6, 6, 6, 4, 1, 5, 3, 5, 6, 3, 1, 3,\n       3, 6, 6, 6, 6, 5, 6, 2, 1, 1, 6, 5, 2, 6, 2, 6, 5, 4, 4, 6, 4, 1,\n       2, 6, 6, 6, 3, 6, 6, 6, 5, 3, 1, 6])\n\n\n\nSimulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nConsider dealing poker hands from a standard deck of cards. When you deal a hand, you draw cards one at a time, and each card drawn affects the next card that can be drawn (because you do not put the card back into the deck). The following code simulates dealing a hand of cards to 4 players, where each player gets 2 cards from a shuffled deck of cards. Then they play on a “board” of 5 community cards that are dealt from the remaining cards in the deck.\n\n\nCode\n1deck = np.arange(1, 14).repeat(4)\nprint(\"Deck of cards before shuffling:\\n\", deck)\n2deck = np.random.permutation(deck)\nprint(\"Deck of cards after shuffling:\\n\", deck)\nrng = np.random.default_rng(seed=21)\n3chosen_indices = rng.choice(len(deck), size=8, replace=False)\n4hands = deck[chosen_indices].reshape(4, 2)\nprint(\"Hands dealt to players:\\n\", hands)\n5remaining_deck = np.delete(deck, chosen_indices)\nprint(\"Remaining cards in the deck:\\n\", remaining_deck)\n6board = np.random.choice(remaining_deck, size=5, replace=False)\nprint(\"Community cards on the board:\\n\", board)\n\n\n\n1\n\nMake a deck of cards (Ace is 1, King is 13). 4 suits, each with cards 1 to 13\n\n2\n\nShuffle the deck\n\n3\n\nDeal the cards. We have 4 players, 2 cards to each of 4 players. That’s 8 cards total. So we select 8 cards from the deck without replacement. Here we select the indices, i.e. the positions in the deck that correspond to the cards we will deal to the players.\n\n4\n\nUse the indices to select the corresponding cards. Reshape the selected cards into a 4x2 array, where each row represents a player’s hand. In other words, split the 8 selected cards into 4 pairs, one pair for each player.\n\n5\n\nRemove the dealt cards from the deck.\n\n6\n\nSelect 5 more cards from the remaining deck to form the community cards. Here we select the actual cards, not the indices, because there is no need to keep track of which cards were dealt (since we are done dealing for the round).\n\n\n\n\nDeck of cards before shuffling:\n [ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12\n 13 13 13 13]\nDeck of cards after shuffling:\n [ 9  5 10 13  1  8  4 13  8 11 11 12 10  7  2 12  1 10  3  7  3  7  2  7\n 11 13 10 13  4  9  9  4  2  5  5  4  3  2 11  6 12  1  6  9 12  3  8  6\n  1  6  5  8]\nHands dealt to players:\n [[ 6  1]\n [ 4  7]\n [10  9]\n [ 2  3]]\nRemaining cards in the deck:\n [ 9  5 10 13  8  4 13  8 11 11 12 10  2 12  1  7  3  7  7 11 13 10 13  4\n  9  4  2  5  5  3  2 11  6 12  1  6  9 12  3  8  6  1  5  8]\nCommunity cards on the board:\n [13  7 13  3  8]\n\n\nBut it can get even more complex than that. In many real-world scenarios, the process of generating data involves multiple steps or conditions that affect the outcome.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code). We then tend to write loops that simulate the process step by step, keeping track of the state of things as we go along.\nLet’s consider an example of a musician busking for money in Rittenhouse Square. The musician’s earnings might depend on various factors like the weather and the number of passersby. To keep it simple, let’s assume that the musician earns $3 for every passerby who stops to listen. Of course, not every passerby will stop – let’s pretend every passerby has the same 20% chance of stopping.\nThe musician might want to know how much money they can expect to earn in a day of busking. We can simulate this process by generating a random number of passersby and then calculating the earnings based on the stopping probability.\n\n\n\n\n\n\nPoisson distribution\n\n\n\nThe Poisson distribution is commonly used to model the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It assumes that the events occur independently and at a constant average rate. In our example, we can use the Poisson distribution to model the number of passersby in a given time period (e.g., one hour of busking).\n\n\n\n\nCode\n1n_days = 5\n# simulate whether it rains each day\nrng = np.random.default_rng(seed=42)\n2rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\n3total_earnings = 0\n4for day in range(n_days):\n5    did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n    print(f\"Day {day + 1} ({rain_probabilities[day]:.2%} chance): {'Rain' if did_it_rain else 'No rain'}\")\n    if did_it_rain:\n6        passersby = rng.poisson(lam=50)\n    else:\n7        passersby = rng.poisson(lam=200)\n    print(f\"\\t Number of passersby: {passersby}\")\n8    listeners = rng.binomial(n=passersby, p=0.2)\n    print(f\"\\t Number of listeners: {listeners}\")\n9    earnings = 3 * listeners\n    print(f\"\\t Daily earnings: ${earnings}\")\n    print(\"-\" * 40)\n\n10    total_earnings += earnings\n\nprint(f\"Total earnings over {n_days} days: ${total_earnings}\")\n\n\n\n1\n\nSet the number of days to simulate\n\n2\n\nSet the probability of rain for each day. Sample from a uniform distribution to get a different probability for each day.\n\n3\n\nInitialize a variable to keep track of total earnings\n\n4\n\nSimulate the process for each day (for-loop)\n\n5\n\nFor each day, “flip a coin” tp decide if it rains based on the probability. Binomial with n=1 is the same as a Bernoulli.\n\n6\n\nFewer passersby stop when it rains. Sample from a Poisson.\n\n7\n\nMore passersby stop when it does not rain. Sample from a Poisson with higher rate.\n\n8\n\nSimulate the number who stop to listen to the busker (binomial, 20% chance of stopping)\n\n9\n\nCompute (daily) earnings at $3 per listener\n\n10\n\nAdd the daily earnings to the total earnings\n\n\n\n\nDay 1 (54.18% chance): No rain\n     Number of passersby: 211\n     Number of listeners: 41\n     Daily earnings: $123\n----------------------------------------\nDay 2 (30.72% chance): No rain\n     Number of passersby: 226\n     Number of listeners: 54\n     Daily earnings: $162\n----------------------------------------\nDay 3 (60.10% chance): Rain\n     Number of passersby: 51\n     Number of listeners: 13\n     Daily earnings: $39\n----------------------------------------\nDay 4 (48.82% chance): Rain\n     Number of passersby: 56\n     Number of listeners: 17\n     Daily earnings: $51\n----------------------------------------\nDay 5 (6.59% chance): No rain\n     Number of passersby: 212\n     Number of listeners: 34\n     Daily earnings: $102\n----------------------------------------\nTotal earnings over 5 days: $477\n\n\nNow we can run the simulation many times to try to estimate the musician’s expected earnings over a week of busking.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe key here is to re-use the logic of the weekly busking simulation in a loop that runs 1000 times. Each time we run the simulation, we get a different weekly outcome based on the random number generator. By averaging these outcomes, we can get a good estimate of the expected earnings over a week of busking.\nimport numpy as np\ndef busk_one_week(rng, n_days=7):\n    \"\"\"Simulate the earnings of a busker in Rittenhouse Square over the course of a week.\n    Args:\n        rng: A NumPy random number generator.\n        n_days: The number of days to simulate (default is 7).\n    Returns:\n        total_earnings: The total earnings over the week.\n    \"\"\"\n    rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\n    total_earnings = 0 # initialize a variable to keep track of total earnings\n    for day in range(n_days):\n        # For each day, decide if it rains based on the probability\n        did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n        # Based on the outcome, the number of passersby changes\n        if did_it_rain:\n            passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n        else:\n            passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n        # Simulate the number who stop to listen to the busker\n        listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n        # Compute the busker's daily earnings\n        earnings = 3 * listeners  # $3 per listener\n\n        total_earnings += earnings\n\n    return total_earnings\n\nn_simulations = 1000\nrandom_seed = 33\n\nrng = np.random.default_rng(seed=random_seed)\n\n# Use the above function to simulate the expected earnings of a busker in Rittenhouse Square over the course of a week.\n# Run the simulation 1000 times and calculate the average earnings over 1000 simulations.\nearnings_by_week = [busk_one_week(rng) for _ in range(n_simulations)]\naverage_earnings = np.mean(earnings_by_week)\naverage_earnings",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html",
    "href": "notebooks/lecture-05.html",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "",
    "text": "In the introductory lecture, we talked about the role of inference: drawing more general conclusions (about a population) from specific observations (a sample). Now that we have covered the most important building blocks, we can start to actually talk about how to make statistical inferences.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "href": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "A worked example: The NBA’s most valuable player (MVP)",
    "text": "A worked example: The NBA’s most valuable player (MVP)\n“Shai Gilgeous Alexander (SGA) is the best player in the NBA” is a very broad claim that is hard to test. What makes a player “the best”? Does stating that they scored the most points make them the best? That his team won the most games?\nLet’s try to instead design a hypothesis that is more specific and testable. We can choose a specific metric to evaluate players, such as points per game (PPG). You can care about other metrics, like assists or rebounds, but let’s just focus on PPG for now.\nTo make the claim more specific and testable, we’ll start off with some assumptions:\n\nEvery player’s scoring follows a distribution with some mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Becuase PPG is an average over the points scored in each game, the CLT tells us that the distribution of average PPG is approximately normal (even if “points scored” is not). Specifically, PPG is a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\), where \\(n\\) is the number of games played.\nThe player with the highest \\(\\mu\\) is the best scorer in the league (i.e. assume that teammates, coaches, conferences, etc. are all negligible).\n\nTo make the claim that SGA is a better scorer than his competitors, we need to show how unlikely it is that he has the highest scoring average if his skill level is not actually the highest in the league.\n\\(H_0\\): \\(\\mu_{\\text{SGA}} \\leq c\\)\n\\(H_1\\): \\(\\mu_{\\text{SGA}} &gt; c\\)\nAll of a sudden, this is way more familiar, specific, and testable. We can now use data to compute the \\(p\\)-value for this hypothesis test.\nShai scored 32.7 PPG in 76 games the 2024-2025 season, which is the highest average in the league. The next-highest scorer was Giannis Antetokounmpo with 30.4 PPG in 72 games.\nWe will use data from the 2024-2025 NBA season, courtesy of Basketball Reference, to evaluate our hypothesis.\n\n\nCode\n1sga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n2sga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\n3compare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\ncompare_df = compare_df.replace({\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan}) \n4compare_df.dropna(subset=[\"PTS\"], inplace=True)\n5compare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\n6shai_sample_mean = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].mean()\nshai_sample_std = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].std()\nprint(f\"Shai's PPG is {shai_sample_mean:.2f} with a standard deviation of {shai_sample_std:.2f}.\")\n\ngiannis_sample_mean = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].mean()\ngiannis_sample_std = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].std()\nprint(f\"Giannis's PPG is {giannis_sample_mean:.2f} with a standard deviation of {giannis_sample_std:.2f}.\")\n\n7fig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(data=compare_df, x=\"PTS\", hue=\"player\", stat=\"density\", common_norm=False, bins=20, ax=ax)\nax.axvline(shai_sample_mean, color=\"blue\", linestyle=\"--\", label=f\"Shai's PPG: {shai_sample_mean:.2f}\")\nax.axvline(giannis_sample_mean, color=\"orange\", linestyle=\"--\", label=f\"Giannis's PPG: {giannis_sample_mean:.2f}\")\nax.set_title(\"Distribution of Points Per Game (PPG) for Shai and Giannis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nplt.show()\n\n\n\n1\n\nRead the data for Shai Gilgeous-Alexander and Giannis Antetokounmpo\n\n2\n\nAnnotate the data with the player’s name\n\n3\n\nCombine the dataframes and clean up the data\n\n4\n\nFilter out rows where the player did not play or was inactive\n\n5\n\nConvert PTS to float/numeric and Date to datetime\n\n6\n\n“DataFrame.query” is a method to filter the dataframe based on a condition. Here we are filtering the dataframe to only include rows where the player is Shai Gilgeous-Alexander. Then compute the mean and standard deviation of his points scored.\n\n7\n\nPlot the distribution of PPG for both players\n\n\n\n\nShai's PPG is 32.68 with a standard deviation of 7.54.\nGiannis's PPG is 30.39 with a standard deviation of 7.32.\n\n\n\n\n\n\n\n\n\nNow what is the null distribution? Our null hypothesis is that SGA’s scoring ability is not higher than his competitors’. So let’s say that under the null, SGA’s inherent scoring ability is equal to Giannis’s observed PPG of 30.4. That’s the mean of our normal distribution given by CLT. For the standard error, we can use the sample standard deviation of Shai’s points scored and divide it by the square root of the number of games he played (76).\nNow the question is: what is the probability of SGA scoring at least 32.7 PPG if his true scoring ability is actually 30.4 PPG?\nWe can just use the normal distribution to compute this probability!\n\n\n\n\n\n\nThe Cumulative Distribution Function (CDF)\n\n\n\n\n\nThe CDF of a normal distribution gives us the probability that a random variable is less than or equal to a certain value. In other words, it computes are under a probability density function (PDF) up to a certain point.\nTo compute the probability of a random variable being greater than a certain value, we can just subtract the CDF from 1. \\[\nP(X &gt; x) = 1 - P(X \\leq x) = 1 - \\text{CDF}(x) = 1 - \\int_{-\\infty}^{x} f(t) dt\n\\]\n\n\n\n\n\nCode\nshai_n_games = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].count()\n1p_value_clt = 1 - stats.norm.cdf(\n2    x=32.7,\n3    loc=30.4,\n4    scale=shai_sample_std/np.sqrt(shai_n_games))\nprint(\"p-value from CLT:\", p_value_clt)\n\nx = np.linspace(27, 35, 1000)\n5null_pdf = stats.norm.pdf(x, loc=30.4,\n                          scale=shai_sample_std/np.sqrt(shai_n_games)) \nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x, null_pdf, label=r\"Normal Distribution given $H_0:$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: 32.7\")\nax.fill_between(x, null_pdf, where=(x &gt;= 32.7), alpha=0.4, label=\"Area for p-value\")\nax.set_title(\"PPG distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Probability Density\")\nax.legend()\nplt.show()\n\n\n\n1\n\nThe \\(p\\)-value is the probability of scoring at least 32.7 PPG under the null hypothesis that SGA’s true scoring ability is at most 30.4 PPG. We can compute this using the CDF of the normal distribution. The probability of scoring at least 32.7 PPG is equal to \\(1 - \\text{CDF}(32.7)\\).\n\n2\n\nWe evaluate the CDF at 32.7 PPG, which is the observed PPG for SGA.\n\n3\n\nWe set the mean of the null distribution to 30.4 (Giannis’s observed PPG)\n\n4\n\nThe standard deviation of the null distribution is the sample standard deviation of Shai’s points scored divided by the square root of the number of games he played (standard error of the mean).\n\n5\n\nPlotting the PDF of the null distribution helps us visualize the probability of scoring at least 32.7 PPG under the null hypothesis.\n\n\n\n\np-value from CLT: 0.00391019858958408\n\n\n\n\n\n\n\n\n\nThis analysis tells us that if Shai’s true scoring ability is 30.4 PPG, the probability of him scoring at least 32.7 PPG over 76 games is very small.\nWe can also try simulating the null distribution through individual game scores to see how likely it is that SGA would score at least 32.7 PPG if his true scoring ability is actually 30.4 PPG. That is, we’ll simulate 76 games of scoring, we each game is drawn from a distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai’s points scored. We’ll repeat this simulation many times to get a distribution of PPG scores under the null hypothesis.\n\n\nCode\nrng = np.random.default_rng(42) \nn_sims = 5000\nshai_simulated_ppg = []\n1for _ in range(n_sims):\n2    low_minus_high = np.sqrt(12 * shai_sample_std**2)\n3    simulated_scores = rng.uniform(low=30.4 - low_minus_high/2,\n                                   high=30.4 + low_minus_high/2, \n                                   size=76)\n4    simulated_ppg = np.mean(simulated_scores)\n    shai_simulated_ppg.append(simulated_ppg)\nshai_simulated_ppg = np.array(shai_simulated_ppg) \n5p_value_simulated = np.mean(shai_simulated_ppg &gt;= 32.7)\nprint(\"p-value from simulation:\", p_value_simulated)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(shai_simulated_ppg, bins=30, stat=\"density\", ax=ax, label=\"Simulated PPG under $H_0$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: {32.7}\")\nax.axvline(30.4, color=\"blue\", linestyle=\"--\", label=f\"$H_0$ Mean: {30.4}\")\nax.set_title(\"Simulated PPG Distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Density\")\nax.legend()\nplt.show()\n\n\n\n1\n\nSimulate many seasons, each with the same number of games as Shai played (76)\n\n2\n\nVariance of continuous uniform distribution on \\([a, b]\\) is \\((b-a)^2 / 12\\)\n\n3\n\nSample 76 scores from a uniform distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai’s points scored\n\n4\n\nCompute the mean of the simulated scores to get the PPG for that simulation\n\n5\n\nCompute the p-value from the simulated PPG scores. It is the proportion of simulated PPG scores that are greater than or equal to 32.7 PPG.\n\n\n\n\np-value from simulation: 0.004\n\n\n\n\n\n\n\n\n\nFor this second approach, we didn’t use the CLT at all! Instead, we relied on simulation to understand the distribution of Shai’s scoring under the null hypothesis. And yet, the results are almost identical to the ones we got using the normal approximation!\nCan you think of any limitations of the approach we took? What assumptions did we make that might not be right? Next lecture we’ll look at approaches that do not make any assumptions about the distribution of the data, and instead rely on resampling techniques to evaluate hypotheses.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#errors-in-hypothesis-testing",
    "href": "notebooks/lecture-05.html#errors-in-hypothesis-testing",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Errors in hypothesis testing",
    "text": "Errors in hypothesis testing\nThere are two types of errors that can occur in hypothesis testing:\n\nFalse positive: reject the null hypothesis when it is actually true.\nFalse negative: fail to reject the null hypothesis when it is actually false.\n\nThese errors are also known as Type I and Type II errors, respectively.\nThis is a bit easier undestand if you tabulate the possible outcomes of a hypothesis test:\n\n\n\n\n\n\n\n\n\nReject \\(H_0\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nType I error (false positive)\nCorrect decision\n\n\n\\(H_0\\) is false\nCorrect decision\nType II error (false negative)\n\n\n\nThe convention in statistics is to denote the probability of a Type I error (false positive) as \\(\\alpha\\) and the probability of a Type II error (false negative) as \\(\\beta\\).",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#summary",
    "href": "notebooks/lecture-05.html#summary",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we studied hypothesis testing – a framework for evaluating claims about data and making data-driven decisions under uncertainty. We learned about the need to formalize claims and quantify uncertainty due to sampling variability, how to establish a null hypothesis, the meaning of the \\(p\\)-value, and how to compute it based on sampling distributions.\nIn the next lecture, we will think about what to do when we don’t have any idea about the underlying distribution of the data, which makes it hard to generate sampling distributions.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-07.html",
    "href": "notebooks/lecture-07.html",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "",
    "text": "There is one more frequently applicable sampling-based method that is super helpful in statistical inference: permutation (or randomization) tests.\nPermutation tests, like the bootstrap, are non-parametric – meaning they do not rely on assumptions about the underlying distribution of the data. Like the bootstrap, permutation tests rely on resampling – only instead of resampling with replacement, we resample without replacement.\nSay we have two samples, \\(X\\) and \\(Y\\), and we want to test if they have different underlying distributions (e.g. different means). Then, our null hypothesis (\\(H_0\\)) is that the two samples are drawn from the same distribution.\nSo, we can combine the two samples into one larger sample, \\(Z = X \\cup Y\\), and then randomly split \\(Z\\) into two new samples, \\(X'\\) and \\(Y'\\), of the same size as the original samples. We can then compute the test statistic (e.g. the difference in means) for the new samples and repeat this process many times to build a distribution of test statistics under the null hypothesis.\nThe only assumption we need to make is that the two samples are exchangeable under the null hypothesis. This means that, if the null hypothesis is true, the two samples could be shuffled without changing the underlying distribution.\nnp.random.permutation is a useful function for this. It randomly permutes the elements of an array, which we can use to create our new samples. Let’s define a function to perform a permutation test\n\n\nCode\ndef permutation_test(test_func, x, y, num_permutations=10000, rng=None, one_sided=True):\n    # Compute the observed test statistic\n    observed_stat = test_func(x, y)\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n    count = 0\n\n    if rng is None:\n        rng = np.random.default_rng()\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the test statistic for the permuted samples\n        permuted_stat = test_func(x_perm, y_perm)\n\n        # Compare the permuted statistic to the observed statistic\n        if one_sided:\n            if permuted_stat &gt;= observed_stat:\n                count += 1\n        else:\n            if np.abs(permuted_stat) &gt;= np.abs(observed_stat):\n                count += 1\n\n    # Compute the p-value\n    p_value = count / num_permutations\n    return p_value\n\n\nWe can apply this to our NBA data to test if SGA and Giannis have different scoring rates. This is quite similar to the bootstrap hypothesis test we did in the previous lecture, but instead of resampling with replacement, we will resample without replacement.\nBoth versions work, but the permutation test tends to be more powerful.\n\n\n\n\n\n\nStatistical Power\n\n\n\n\nPower\n\nThe probability of correctly rejecting the null hypothesis when it is false.\n\n\nA more “powerful” test is more likely to detect a true effect. The power of a test is often written as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false).\nWe usually want our tests to have high power, so we can detect true effects when they exist. We try to balance this against the risk of falsely rejecting the null hypothesis (Type I error) when it is actually true.\n\n\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace(\n    {\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan}\n)\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nrng = np.random.default_rng(42) \n# run the permutation test\np_value = permutation_test(\n    lambda x, y: np.mean(x) - np.mean(y), # lambda functions can be used inline without naming the function\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    rng=rng\n)\nprint(\n    f\"P-value for the hypothesis that SGA scores more points than Giannis: {p_value:.4f}\"\n)\n\n\nP-value for the hypothesis that SGA scores more points than Giannis: 0.0336",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-07.html#permutation-tests",
    "href": "notebooks/lecture-07.html#permutation-tests",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "",
    "text": "There is one more frequently applicable sampling-based method that is super helpful in statistical inference: permutation (or randomization) tests.\nPermutation tests, like the bootstrap, are non-parametric – meaning they do not rely on assumptions about the underlying distribution of the data. Like the bootstrap, permutation tests rely on resampling – only instead of resampling with replacement, we resample without replacement.\nSay we have two samples, \\(X\\) and \\(Y\\), and we want to test if they have different underlying distributions (e.g. different means). Then, our null hypothesis (\\(H_0\\)) is that the two samples are drawn from the same distribution.\nSo, we can combine the two samples into one larger sample, \\(Z = X \\cup Y\\), and then randomly split \\(Z\\) into two new samples, \\(X'\\) and \\(Y'\\), of the same size as the original samples. We can then compute the test statistic (e.g. the difference in means) for the new samples and repeat this process many times to build a distribution of test statistics under the null hypothesis.\nThe only assumption we need to make is that the two samples are exchangeable under the null hypothesis. This means that, if the null hypothesis is true, the two samples could be shuffled without changing the underlying distribution.\nnp.random.permutation is a useful function for this. It randomly permutes the elements of an array, which we can use to create our new samples. Let’s define a function to perform a permutation test\n\n\nCode\ndef permutation_test(test_func, x, y, num_permutations=10000, rng=None, one_sided=True):\n    # Compute the observed test statistic\n    observed_stat = test_func(x, y)\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n    count = 0\n\n    if rng is None:\n        rng = np.random.default_rng()\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the test statistic for the permuted samples\n        permuted_stat = test_func(x_perm, y_perm)\n\n        # Compare the permuted statistic to the observed statistic\n        if one_sided:\n            if permuted_stat &gt;= observed_stat:\n                count += 1\n        else:\n            if np.abs(permuted_stat) &gt;= np.abs(observed_stat):\n                count += 1\n\n    # Compute the p-value\n    p_value = count / num_permutations\n    return p_value\n\n\nWe can apply this to our NBA data to test if SGA and Giannis have different scoring rates. This is quite similar to the bootstrap hypothesis test we did in the previous lecture, but instead of resampling with replacement, we will resample without replacement.\nBoth versions work, but the permutation test tends to be more powerful.\n\n\n\n\n\n\nStatistical Power\n\n\n\n\nPower\n\nThe probability of correctly rejecting the null hypothesis when it is false.\n\n\nA more “powerful” test is more likely to detect a true effect. The power of a test is often written as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false).\nWe usually want our tests to have high power, so we can detect true effects when they exist. We try to balance this against the risk of falsely rejecting the null hypothesis (Type I error) when it is actually true.\n\n\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace(\n    {\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan}\n)\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nrng = np.random.default_rng(42) \n# run the permutation test\np_value = permutation_test(\n    lambda x, y: np.mean(x) - np.mean(y), # lambda functions can be used inline without naming the function\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    rng=rng\n)\nprint(\n    f\"P-value for the hypothesis that SGA scores more points than Giannis: {p_value:.4f}\"\n)\n\n\nP-value for the hypothesis that SGA scores more points than Giannis: 0.0336",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-07.html#there-is-really-only-one-test",
    "href": "notebooks/lecture-07.html#there-is-really-only-one-test",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "There is really only one test!",
    "text": "There is really only one test!\nThe more statistics you learn and the more you are exposed to work in quantitative fields, the more you will see a wide variety of complicated statistical techniques and methods.\nUltimately they all represent the same process:\n\nCompute a test statistic on the observed data.\nChoose a null hypothesis / model. Either specify the null distribution explicitly or use a simulation-based method to generate a distribution of test statistics under the null hypothesis.\nCompute a p-value by comparing the observed test statistic to the distribution of test statistics under the null hypothesis.\n\nMost of the literature in classical statistics focuses on mathematically deriving analytical solutions for 2 and 3. All of the fancy named statistical tests you see are variations with different test statistics and null hypotheses.\nCheck out this blog post by Allen Downey for more on this idea and explanation of the advantages of simulation-based methods for hypothesis testing.\n\nExample: Two-sample t-test\nOne of the most common statistical tests is the independent two-sample \\(t\\)-test, which tests if two independent samples have different means. This is used in many fields, including psychology, medicine, and social sciences – basically anywhere you want to compare two groups.\nRemembering the \\(t\\)-test is not an important point of this course – we introduce it here because it is used so frequently in practice that it is worth understanding the basic idea behind it (and that it is just a special case of the general hypothesis testing framework we have been discussing).\nWe learned already that (by the Central Limit Theorem) as the sample size increases, the sampling distribution of the sample mean becomes approximately normal, even if the underlying distribution is not normal. The exact distribution of the sample mean, however, is called the t-distribution. It is quite like the normal distribution, but has heavier tails (more likely to produce extreme values). See the plots below for an illustration of how the t-distribution converges to the standard normal distribution as the sample size increases.\n\n\nCode\nx = np.linspace(-4, 4, 100)\nstats.t.pdf(\n    x,\n    df=10,\n    loc=0,\n    scale=1\n)\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\n# Plotting the t-distribution and normal distribution for comparison\nfor i, sample_size in enumerate([2, 10, 30]):\n    df = 2 * sample_size - 2\n    sns.lineplot(\n        x=x,\n        y=stats.t.pdf(x, df=df, loc=0, scale=1),\n        label=f\"$t$ (df={df})\",\n        ax=ax[i]\n    )\n    sns.lineplot(\n        x=x,\n        y=stats.norm.pdf(x, loc=0, scale=1),\n        label=\"Normal\",\n        ax=ax[i]\n    )\n    ax[i].grid(True)\n    ax[i].set_title(f\"$t$ vs Normal ($n={sample_size}$)\")\n    ax[i].set_xlabel(\"x\")\n    ax[i].set_ylabel(\"Density\")\n    sns.move_legend(ax[i], loc=\"upper left\")\n    \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s create two independent samples from uniform distributions with different means and equal variance. We’ll perform a two-sample t-test to see if they are significantly different. In other words, we will test the null hypothesis that the two samples have the same mean against the alternative hypothesis that they have different means.\nWe’ll use the built-in scipy.stats.ttest_ind function to perform the t-test, but we will also manually compute the t-statistic and p-value to illustrate the process. We will also sample from the t-distribution to compare our results. Finally, we will perform a permutation test to see how it compares.\n\n\nCode\n# simulate a t-distribution\nrng = np.random.default_rng(43)  # set a random seed for reproducibility\n\n# take two samples from different uniform distributions\nsamples_a = rng.uniform(low=-1, high=3, size=10)\nsamples_b = rng.uniform(low=-3, high=1, size=10)\n\nt_parametric = stats.ttest_ind(\n    samples_a,\n    samples_b,\n    equal_var=True\n)\nprint(f\"Parametric t-test statistic: {t_parametric.statistic:.4f}, p-value: {t_parametric.pvalue:.6f}\")\n\n# calculate the t-statistic manually\n\ndiff_means = np.mean(samples_a) - np.mean(samples_b)\n# assume they have the same variance -- use the \"pooled\" or averaged variance\npooled_var = (np.var(samples_a, ddof=1) + np.var(samples_b, ddof=1))\npooled_std_error = np.sqrt(pooled_var / len(samples_a))\nt_stat= diff_means / pooled_std_error\nt_abs = np.abs(t_stat)\nprint(f\"Manual t-statistic: {t_stat:.4f}\")\n# two-tailed test: 1 - (CDF(t_statistic) - CDF(-t_statistic))\np_value_t_pdf = 1 - (stats.t.cdf(\n    t_abs,\n    df=len(samples_a) + len(samples_b) - 2\n) - stats.t.cdf(\n    -t_abs,\n    df=len(samples_a) + len(samples_b) - 2\n))\nprint(f\"Theoretical p-value from PDF of t: {p_value_t_pdf:.6f}\")\n\n# Sample from a t-distribution\nnum_samples = 10000\nt_samples = rng.standard_t(\n    df=len(samples_a) + len(samples_b) - 2,\n    size=num_samples\n)\n# calculate the p-value from the simulated t-distribution\np_value_simulated = np.mean(np.abs(t_samples) &gt;= t_abs)\nprint(f\"Simulated p-value by sampling from t: {p_value_simulated:.6f}\")\n\n\n## # Permutation test for t-statistic\ndef permutation_test_t(\n    x, y, num_permutations=10000, rng=None,\n):\n    # Compute the observed t-statistic\n    observed_stat = stats.ttest_ind(x, y, equal_var=True).statistic\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n\n    if rng is None:\n        rng = np.random.default_rng()\n    permuted_stats = []\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the t-statistic for the permuted samples\n        permuted_stat = stats.ttest_ind(x_perm, y_perm, equal_var=True).statistic\n\n        # Compare the permuted statistic to the observed statistic\n        permuted_stats.append(permuted_stat)\n    # Compute the p-value\n    permuted_stats = np.array(permuted_stats)\n    p_value = np.mean(np.abs(permuted_stats) &gt;= np.abs(observed_stat))\n\n    return permuted_stats, p_value\n\n# Run the permutation test\npermuted_stats, p_value = permutation_test_t(\n    samples_a,\n    samples_b,\n    num_permutations=10000,\n    rng = rng\n)\nprint(f\"Permutation test p-value: {p_value:.6f}\")\n\n\nParametric t-test statistic: 2.6958, p-value: 0.014783\nManual t-statistic: 2.6958\nTheoretical p-value from PDF of t: 0.014783\nSimulated p-value by sampling from t: 0.014400\nPermutation test p-value: 0.015700\n\n\nAll of these results are quite similar! It’s not a coincidence – they are all effectively the same test. We’re comparing the observed t-statistic to the distribution of t-statistics under the null hypothesis that the two samples are drawn from the same distribution.\nThis might be easier to appreciate visually:\n\n\nCode\nfig, ax = plt.subplots(1, 3, figsize=(12, 5), sharey=True, sharex=True)\n# plot the t-distribution and highlight the observed t-statistic\nax[0].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), label='t-distribution')\nax[0].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[0].fill_between(\n    x,\n    stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2),\n    where=np.abs(x) &gt;= t_abs,\n    alpha=0.3,\n    color='red',\n    label='Area for p-value'\n)\nax[0].set_title('t-distribution PDF')\nax[0].set_xlabel('$t$-value')\nax[0].set_ylabel('Density')\nax[0].legend(loc=\"upper left\", fontsize='x-small', frameon=False)\n\nsns.histplot(t_samples, bins=30, ax=ax[1], stat=\"density\", color=\"gray\", alpha=0.5)\nax[1].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), linestyle='--', label='t-distribution')   \nax[1].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[1].set_title('Sampled t-distribution')\nax[1].set_xlabel('$t$-value')\nax[1].set_ylabel('Density')\n\nsns.histplot(permuted_stats, bins=30, ax=ax[2], stat=\"density\", color=\"gray\", alpha=0.5)\nax[2].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), linestyle='--', label='t-distribution')\nax[2].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[2].set_title('Permutation Test Statistics')\nax[2].set_xlabel('$t$-value')\nax[2].set_ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\n\n\nSee how close the sampling distributions are to the theoretical t-distribution? This is why all the tests are so similar.",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-09.html",
    "href": "notebooks/lecture-09.html",
    "title": "Lecture 09: Regression inference and multiple regression",
    "section": "",
    "text": "Last lecture we introduced linear regression as a way to model the relationship between a response variable \\(Y\\) and some predictor variable \\(X\\). The model assumes that the relationship can be described by a linear equation:\n\\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\epsilon\\) is the error term. The error term captures the variability in \\(Y\\) that cannot be explained by \\(X\\), and we assume it follows a normal distribution with mean 0 (i.e. the deviations from the true regression line are zero on average) and some variance \\(\\sigma^2\\).\nThroughout this course, we have focused on sampling variability and how it affects our ability to make inferences about a population based on a sample. In the context of regression, this means that if we take different samples from the same population, we will get different estimates of the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\). This variability is due to the random nature of sampling and the inherent noise in the data.\nThe app below illustrates this concept by allowing you to draw samples from a population and see how the estimated regression coefficient (\\(\\hat{\\beta}_1\\)) varies across samples. Notice that the true slope of the regression line is fixed, but the estimated slope varies due to sampling variability.\nYou can also see how the stability of the estimate improves as the sample size increases, as well as the confidence implied by the \\(p\\)-value of the hypothesis test for the slope (\\(H_0: \\beta_1 = 0\\)). We will unpack this in more detail shortly.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render, reactive\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\n# Fixed population\nrng = np.random.default_rng(123)\nN_POP = 2000\nx_pop = rng.normal(size=N_POP)\ntrue_beta = 0.5\ny_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\npopulation = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\n\napp_ui = ui.page_fluid(\n    # Title\n    ui.div(\n        ui.h2(\"Sampling Variability in Regression Inference\"),\n        style=\"text-align:center; margin-bottom:20px; margin-top:20px; font-weight:bold;\"\n    ),\n    # Top row with inputs\n    ui.layout_columns(\n        ui.input_slider(\"n\", \"Sample size\", min=5, max=200, value=50, step=5, width=\"300px\"),\n        ui.input_numeric(\"perm\", \"Number of permutations\", value=500, min=100, max=2000, step=100, width=\"200px\"),\n        ui.input_action_button(\"resample\", \"Draw New Sample\", width=\"200px\"),\n    ),\n    # Second row with two side-by-side plots\n    ui.navset_tab(\n        ui.nav_panel(\"Visualization\",\n            ui.div(\n                ui.output_text(\"coef_text\"),\n                style=\"text-align:center; font-size:18px; margin-top:10px; margin-bottom:10px;\"\n            ),\n            ui.layout_columns(\n                ui.output_plot(\"scatter_plot\"),\n                ui.output_plot(\"perm_plot\"),\n            )\n        ),\n        ui.nav_panel(\"Permutation Table\",\n            ui.div(\n                \"First 50 rows of original sample and first 10 permutations:\",\n                style=\"text-align:center; font-size:16px; margin-top:10px; margin-bottom:10px;\"\n            ),\n            ui.output_table(\"perm_table\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # ✅ Store the current sample as state\n    current_sample = reactive.value(pd.DataFrame())\n\n    # Update sample when button is clicked or sample size changes\n    @reactive.effect\n    @reactive.event(input.resample, input.n)\n    def _():\n        idx = rng.choice(len(population), size=input.n(), replace=False)\n        current_sample.set(population.iloc[idx])\n\n    # ✅ Compute regression results based on stored sample\n    @reactive.calc\n    def regression_results():\n        data = current_sample()\n        if data.empty:\n            return {\"beta_hat\": np.nan, \"p_val\": np.nan, \"perm_coefs\": np.array([]), \"perm_table\": pd.DataFrame()}\n\n        model = smf.ols(\"y ~ x\", data=data).fit()\n        beta_hat = model.params[\"x\"]\n        p_val = model.pvalues[\"x\"]\n\n        perm_coefs = []\n        perm_tables = []\n        for i in range(input.perm()):\n            y_perm = rng.permutation(data[\"y\"])\n            perm_model = smf.ols(\"y ~ x\", data=data.assign(y=y_perm)).fit()\n            perm_coefs.append(perm_model.params[\"x\"])\n\n            if i &lt; 10:  # store first 10 permutations for table\n                df_perm = pd.DataFrame({\"x\": data[\"x\"].values, f\"y_perm_{i+1}\": y_perm})\n                perm_tables.append(df_perm[[f\"y_perm_{i+1}\"]])\n\n        # Merge the first few permutations into one table\n        if perm_tables:\n            perm_table_df = pd.concat([data[[\"x\", \"y\"]].reset_index(drop=True)] + perm_tables, axis=1)\n        else:\n            perm_table_df = pd.DataFrame()\n        \n        # compute permutation p-value\n        perm_coefs = np.array(perm_coefs)\n        # two-tailed p-value\n        p_val_perm = np.mean(np.abs(perm_coefs) &gt;= np.abs(beta_hat))\n\n        return {\n            \"beta_hat\": beta_hat,\n            \"p_val\": p_val,\n            \"p_val_perm\": p_val_perm,\n            \"perm_coefs\": np.array(perm_coefs),\n            \"perm_table\": perm_table_df\n        }\n\n    @output\n    @render.text\n    def coef_text():\n        res = regression_results()\n        if np.isnan(res[\"beta_hat\"]):\n            return \"Click 'Draw New Sample' to begin.\"\n        return f\"True slope: {true_beta:.3f}, Estimated slope: {res['beta_hat']:.3f}, Parametric p-value: {res['p_val']:.4f}, Permutation p-value: {res['p_val_perm']:.4f}\"\n\n    @output\n    @render.plot\n    def scatter_plot():\n        data = current_sample()\n        res = regression_results()\n\n        plt.figure(figsize=(7, 5))\n        sns.scatterplot(x=\"x\", y=\"y\", data=population, alpha=0.1, color=\"gray\")\n\n        if not data.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"blue\", label=\"Sampled Points\")\n            x_vals = np.linspace(population[\"x\"].min(), population[\"x\"].max(), 100)\n            y_vals = res[\"beta_hat\"] * x_vals + data[\"y\"].mean() - res[\"beta_hat\"] * data[\"x\"].mean()\n            plt.plot(x_vals, y_vals, color=\"red\", lw=2, label=\"Estimated Regression Line\")\n            # Add true population regression line\n            plt.plot(x_vals, true_beta * x_vals, alpha=0.3, color=\"gray\", linestyle=\"--\", label=\"True Regression Line\")\n            plt.legend()\n\n\n        plt.xlabel(\"X\")\n        plt.ylabel(\"Y\")\n        plt.xlim(population[\"x\"].min(), population[\"x\"].max())\n        plt.ylim(population[\"y\"].min(), population[\"y\"].max())\n        plt.title(\"Sampled Points and Fitted Regression Line\")\n\n    @output\n    @render.plot\n    def perm_plot():\n        res = regression_results()\n        if len(res[\"perm_coefs\"]) == 0:\n            plt.figure()\n            plt.text(0.5, 0.5, \"Click 'Draw New Sample' to start.\", ha=\"center\", va=\"center\")\n            return\n\n        plt.figure(figsize=(7, 4))\n        sns.histplot(res[\"perm_coefs\"], bins=30, kde=False, color=\"gray\")\n        plt.axvline(res[\"beta_hat\"], color=\"red\", lw=2, label=\"Observed β̂\")\n        plt.axvline(true_beta, color=\"blue\", lw=2, linestyle=\"--\", label=\"True β\")\n        plt.xlabel(r\"Estimated slope ($\\hat{\\beta}$)\")\n        plt.ylabel(\"Count\")\n        plt.xlim(min(-1, res[\"perm_coefs\"].min()), max(1, res[\"perm_coefs\"].max()))\n        plt.title(r\"Null Distribution of $\\hat{\\beta}$ (Permutation Test)\")\n        plt.legend()\n    @output\n    @render.table\n    def perm_table():\n        res = regression_results()\n        df = res[\"perm_table\"]\n        if df.empty:\n            return pd.DataFrame()\n        return df.head(50)  # Only show first 50 rows\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nFitting the regression model\n\n\n\n\n\nAs in the previous lecture, we use the statsmodels library to fit the regression model.\nUnder the hood, the app is doing the following:\n# Fixed population\nrng = np.random.default_rng(123)\nN_POP = 2000\nx_pop = rng.normal(size=N_POP)\ntrue_beta = 0.5\ny_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\npopulation = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\nThis creates a fixed population of 2000 points with a known slope of 0.5. The app then allows you to draw samples from this population and fit a regression model to the sampled data.\nMore specifically, we subsample the population to create a new sample of size n:\nidx = rng.choice(len(population), size=n, replace=False)\ndata = population.iloc[idx]\nThen we fit the regression model using statsmodels:\nmodel = smf.ols('y ~ x', data=data).fit()\n    beta_hat = model.params['x']\n\n\n\n\n\nIn regression analysis, we often want to test hypotheses about the relationship between the predictor variable \\(X\\) and the response variable \\(Y\\). The most common hypothesis to test is whether the slope of the regression line (\\(\\beta_1\\)) is different from zero, which would indicate that there is a relationship between \\(X\\) and \\(Y\\).\nSo we can set up the following hypotheses: \\[\n\\begin{align*}\nH_0:& ~\\beta_1 = 0 \\quad \\text{(no relationship)} \\\\\nH_1:& ~\\beta_1 \\neq 0 \\quad \\text{(there is a relationship)}\n\\end{align*}\n\\]\nWe’re back in our comfort zone now. How can we simulate the distribution of the estimated slope \\(\\hat{\\beta}_1\\) under the null hypothesis? We’ll use a permutation test – take a shot at implementing it in the code cell below.\n\nExerciseHintSolution\n\n\n\n\n\n\n\n\n\n\n\nTo implement the permutation test, you will need to:\n\nFit the regression model to the original data to get the observed slope \\(\\hat{\\beta}_1\\).\nFor each permutation:\n\nPermute the response variable \\(Y\\) to break the relationship with \\(X\\).\nFit the regression model to the permuted data.\nStore the estimated slope \\(\\hat{\\beta}_1\\) from the permuted data.\n\nCompute the p-value as the proportion of permuted slopes that are greater than or equal to the observed slope in absolute value (two-tailed test).\n\n\n\n\n\ndef permutation_test_regression(data, n_permutations=1000, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    # first, fit the regression model (on the original data)\n    model = sm.ols('y ~ x', data=data).fit()\n    beta_hat = model.params['x']\n    \n    perm_beta_hats = [] # list to store permuted slopes\n    for _ in range(n_permutations):\n        # permute the response variable y\n        # this breaks the relationship between x and y\n        y_perm = rng.permutation(data['y'])\n        # fit the regression model to the permuted data\n        perm_model = sm.OLS(y_perm, X).fit()\n        # store the estimated slope\n        perm_beta_hats.append(perm_model.params['x'])\n    # the slopes from the permuted data are\n    # your null distribution\n    perm_beta_hats = np.array(perm_beta_hats)\n    # compute p-value\n    p_val = np.mean(np.abs(perm_beta_hats) &gt;= np.abs(beta_hat))\n    \n    return {\n        \"beta_hat\": beta_hat,\n        \"perm_beta_hats\": perm_beta_hats,\n        \"p_val\": p_val,\n    }\nresult_dict = permutation_test_regression(data, n_permutations=1000, rng=rng)\nprint(\"Observed slope (beta_hat):\", result_dict['beta_hat'])\nprint(\"Permutation p-value:\", result_dict['p_val'])\n\n\n\n\nThis is precisely the permutation test that generates the null distribution displayed in the app.\nNotice that the permuation test \\(p\\)-value is nearly identical to the parametric \\(p\\)-value computed by statsmodels using the assumption that the errors are normally distributed.\n\n\n\n\n\n\nParametric inference in regression\n\n\n\n\n\nThe app uses statsmodels to compute the regression coefficients and their associated p-values. To get these \\(p\\)-values (without doing simulations / randomization), you have to make assumptions about the underlying probability distributions.\nIn the linear regression model, the typical assumption is that (as we discussed in the previous lecture) the errors \\(\\epsilon\\) are normally distributed. This makes the \\(Y\\) values themselves normal random variables (i.e. \\(Y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X, \\sigma^2)\\)). This assumption allows us to use the properties of the normal distribution to figure out analytical expressions for the uncertainty in the estimated coefficients (a.k.a. the standard errors) and the \\(p\\)-values for hypothesis tests.\nThe details of the parametric inference are beyond the scope of this course but you are encouraged to take more advanced courses in statistics or econometrics to learn more about it!\n\n\n\n\n\n\n\\(p\\)-values are nice, but they don’t tell us the whole story. We might want to get a sense of the range of plausible values for the regression coefficients, because the effect size is important too.\n\n\n\n\n\n\nEffect Size\n\n\n\n\n\nEffect size refers to the magnitude of a quantity you are interested in estimating – often a difference in averages or a regression coefficient.\nFor a simple example, let’s go all the way back to coin flips. We saw examples where we tried to estimate the probability of seeing a certain set of outcomes if the coin was fair. We saw that with lots of coin flips, we were able to confidently detect a biased coin. But we saw an example where the coin was very biased (only 25% heads). What if the coin was just a little biased (say 51% heads)? This is a much smaller effect size, and it would be harder to detect with the same number of coin flips – but with enough flips we could still detect it. Is a 1% difference in probability of heads worth fighting over? What about a 0.1% difference?\nThis comes up a lot in scientific research. Say you are testing a new drug and you find that it reduces symptoms by 5% compared to a placebo. Is that a meaningful effect? What if the drug only reduces symptoms by 0.5%? Producing the drug might be very expensive, so a small effect size might not be worth the cost (or possibly the risk of side effects).\n\n\n\nIn addition to hypothesis testing, we can also use bootstrapping to construct confidence intervals for the regression coefficients.\nHere’s some code that does this:\n\n\nCode\nimport statsmodels.formula.api as sm\nimport pandas as pd\nimport numpy as np\n\n\ndef bootstrap_regression_ci(data, x_col, y_col, n_bootstraps=1000, alpha=0.05):\n    n = len(data)\n    coefs = []\n    for _ in range(n_bootstraps):\n1        sample = data.sample(n, replace=True)\n2        model = sm.ols(f\"{y_col} ~ {x_col}\", data=sample).fit()\n3        coefs.append(model.params[x_col])\n    coefs = np.array(coefs)\n4    lower_bound = np.percentile(coefs, 100 * alpha / 2)\n    upper_bound = np.percentile(coefs, 100 * (1 - alpha / 2))\n    return coefs, lower_bound, upper_bound\n\n\nrng = np.random.default_rng(123)\n# Generate synthetic data\nx = rng.normal(size=100)\ny = 1.5 * x + 2 + rng.normal(size=100)\ndata = pd.DataFrame({\"x\": x, \"y\": y})\n\n5model = sm.ols(\"y ~ x\", data=data).fit()\n\nbootstrap_coefs, lower_bound, upper_bound = bootstrap_regression_ci(\n    data, \"x\", \"y\", n_bootstraps=1000\n)\nprint(\"#\" * 40)\nprint(f\"Bootstrap CI for slope: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nprint(\"#\" * 40)\n6model.summary(slim=True)\n\n\n\n1\n\nBootstrap the data to create multiple samples (same size as the original sample, sampled with replacement).\n\n2\n\nFit the regression model to each bootstrap sample.\n\n3\n\nStore the estimated coefficients from each bootstrap sample.\n\n4\n\nCompute the confidence intervals based on the distribution of the bootstrap estimates.\n\n5\n\nFit the regression model to the original data to get the point estimates.\n\n6\n\nPrint statsmodels summary table with the confidence intervals.\n\n\n\n\n########################################\nBootstrap CI for slope: [1.360, 1.771]\n########################################\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.699\n\n\nModel:\nOLS\nAdj. R-squared:\n0.696\n\n\nNo. Observations:\n100\nF-statistic:\n227.5\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n2.71e-27\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.0933\n0.093\n22.569\n0.000\n1.909\n2.277\n\n\nx\n1.5567\n0.103\n15.084\n0.000\n1.352\n1.761\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe summary table has lots of extra information you can ignore for now, but the key part is the coef column which contains the point estimates of the regression coefficients, and the [0.025, 0.975] columns which contain the lower and upper bounds of the 95% confidence intervals.\nWe also printed the bootstrapped confidence intervals, and you can see that they are very similar to the ones computed by statsmodels using the normality assumption.\nStill confused? Try the following app to visualize what’s going on:\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 950\n\nfrom shiny import App, ui, render, reactive\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(123)\nN_POP = 1000\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"beta\", \"True Slope (β)\", min=-2, max=2, value=0.5, step=0.1),\n            ui.input_slider(\"sigma\", \"Noise SD (σ)\", min=0.1, max=2.0, value=1.0, step=0.1),\n            ui.input_slider(\"n\", \"Original Sample Size\", min=10, max=500, value=100),\n            ui.input_action_button(\"draw_sample\", \"Draw Original Sample\"),\n            ui.input_action_button(\"bootstrap1\", \"Bootstrap 1x\", disabled=True),\n            ui.input_action_button(\"bootstrap10\", \"Bootstrap 10x\", disabled=True),\n            ui.input_action_button(\"bootstrap100\", \"Bootstrap 100x\", disabled=True),\n            ui.input_action_button(\"reset\", \"Reset\", disabled=True),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Scatter and Slopes Histogram\",\n                ui.output_plot(\"scatter_plot\"),\n                ui.div(\n                    ui.output_text(\"count_text\"),\n                    style=\"text-align:center; font-size:16px; margin-top:10px;\"\n                ),\n                ui.output_plot(\"hist_plot\"),\n            ),\n            ui.nav_panel(\"Bootstrapped Regression Lines\",\n                ui.output_plot(\"lines_plot\"),\n            ),\n        ),\n    )\n)\n\ndef server(input, output, session):\n    original_sample = reactive.value(pd.DataFrame())\n    current_bootstrap = reactive.value(pd.DataFrame())\n    bootstrap_coefs = reactive.value([])\n\n    # Generate population reactively\n    @reactive.calc\n    def population():\n        x = rng.normal(size=N_POP)\n        y = input.beta() * x + input.sigma() * rng.normal(size=N_POP)\n        return pd.DataFrame({\"x\": x, \"y\": y})\n\n    # Helper to enable/disable buttons\n    def set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False):\n        session.send_input_message(\"bootstrap1\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"bootstrap10\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"bootstrap100\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"draw_sample\", {\"disabled\": not enable_draw})\n        session.send_input_message(\"reset\", {\"disabled\": not enable_reset})\n\n    @reactive.effect\n    def _init_buttons():\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Reset sample & bootstrap whenever population changes\n    @reactive.effect\n    @reactive.event(input.beta, input.sigma, input.n)\n    def _reset_on_population_change():\n        original_sample.set(pd.DataFrame())\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Draw original sample\n    @reactive.effect\n    @reactive.event(input.draw_sample)\n    def draw_sample():\n        sample = population().sample(n=input.n(), replace=False)\n        original_sample.set(sample)\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=True, enable_draw=False, enable_reset=True)\n\n    def bootstrap_once():\n        sample = original_sample()\n        if sample.empty:\n            return\n        resample = sample.sample(n=len(sample), replace=True)\n        current_bootstrap.set(resample)\n        model = smf.ols(\"y ~ x\", data=resample).fit()\n        bootstrap_coefs.set(bootstrap_coefs() + [(model.params[\"Intercept\"], model.params[\"x\"])])\n\n    # Bootstrap buttons\n    @reactive.effect\n    @reactive.event(input.bootstrap1)\n    def boot1():\n        bootstrap_once()\n\n    @reactive.effect\n    @reactive.event(input.bootstrap10)\n    def boot10():\n        for _ in range(10):\n            bootstrap_once()\n\n    @reactive.effect\n    @reactive.event(input.bootstrap100)\n    def boot100():\n        for _ in range(100):\n            bootstrap_once()\n\n    # Reset button\n    @reactive.effect\n    @reactive.event(input.reset)\n    def reset_all():\n        original_sample.set(pd.DataFrame())\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Bootstrap count text\n    @output\n    @render.text\n    def count_text():\n        count = len(bootstrap_coefs())\n        if count == 0:\n            return \"No bootstrap samples yet.\"\n        elif count == 1:\n            return \"1 bootstrap sample drawn.\"\n        else:\n            return f\"{count} bootstrap samples drawn.\"\n\n    # Scatter plot\n    @output\n    @render.plot\n    def scatter_plot():\n        plt.figure(figsize=(7, 4.5))\n        pop = population()\n\n        pop_model = smf.ols(\"y ~ x\", data=pop).fit()\n        x_vals = np.linspace(pop[\"x\"].min(), pop[\"x\"].max(), 100)\n        y_vals = pop_model.params[\"Intercept\"] + pop_model.params[\"x\"] * x_vals\n        plt.plot(x_vals, y_vals, color=\"gray\", lw=2, linestyle=\":\", label=\"Population line\")\n\n        sample = original_sample()\n        if not sample.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=sample, color=\"black\", alpha=0.4, label=\"Original sample\")\n            model = smf.ols(\"y ~ x\", data=sample).fit()\n            y_vals_s = model.params[\"Intercept\"] + model.params[\"x\"] * x_vals\n            plt.plot(x_vals, y_vals_s, color=\"black\", linestyle=\"--\", label=\"Original line\", alpha=0.7, lw=2)\n\n        boot = current_bootstrap()\n        if not boot.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=boot, color=\"blue\", alpha=0.7, label=\"Bootstrap sample\")\n            model_b = smf.ols(\"y ~ x\", data=boot).fit()\n            y_vals_b = model_b.params[\"Intercept\"] + model_b.params[\"x\"] * x_vals\n            plt.plot(x_vals, y_vals_b, color=\"blue\", lw=2, label=\"Bootstrap line\")\n\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.title(\"Population, original sample, and (most recent) bootstrap sample\")\n        plt.legend()\n\n    # Histogram\n    @output\n    @render.plot\n    def hist_plot():\n        coefs = bootstrap_coefs()\n        # unpack (Intercept, slope) tuples\n        plt.figure(figsize=(7, 3.5))\n\n        if coefs:\n            coefs = [coef[1] for coef in coefs]  # extract slopes only\n            sns.histplot(coefs, bins=20, color=\"lightblue\", kde=False)\n            q_low, q_high = np.quantile(coefs, [0.025, 0.975])\n            plt.axvline(q_low, color=\"purple\", linestyle=\"--\", lw=2, label=\"2.5% Quantile\")\n            plt.axvline(q_high, color=\"purple\", linestyle=\"--\", lw=2, label=\"97.5% Quantile\")\n            plt.fill_betweenx(\n                [0, plt.ylim()[1]], q_low, q_high, color=\"purple\", alpha=0.05, label=\"95% CI\"\n            )\n\n        sample = original_sample()\n        if not sample.empty:\n            model_orig = smf.ols(\"y ~ x\", data=sample).fit()\n            obs_slope = model_orig.params[\"x\"]\n            plt.axvline(obs_slope, color=\"black\", lw=2, label=r\"Observed $\\hat{\\beta}$\")\n\n        plt.xlabel(\"Bootstrap Slope Estimates\")\n        plt.ylabel(\"Frequency\")\n        plt.title(r\"Bootstrap distribution of $\\hat{\\beta}$\")\n        plt.legend()\n\n    @output\n    @render.plot\n    def lines_plot():\n        plt.figure(figsize=(7, 5))\n        x_vals = np.linspace(-3, 3, 100)\n\n        # Draw all bootstrap lines\n        for (intercept, slope) in bootstrap_coefs():\n            plt.plot(x_vals, intercept + slope * x_vals, color=\"blue\", alpha=0.1)\n\n        # Draw original sample line\n        sample = original_sample()\n        if not sample.empty:\n            m = smf.ols(\"y ~ x\", data=sample).fit()\n            plt.plot(x_vals, m.params[\"Intercept\"] + m.params[\"x\"] * x_vals,\n                     color=\"black\", lw=2, label=\"Original Line\")\n\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.title(\"Bootstrap Regression Lines\")\n        plt.legend()\n\n\napp = App(app_ui, server)\n\nThe bootstrap distribution of the estimated slope \\(\\hat{\\beta}_1\\) gives us confidence intervals for the slope. See the second panel of the app for a visualization of bootstrapped regression lines (slope and intercept). As you draw more bootstrap samples, you’ll see that the regression lines concentrate around the original sample’s regression line, but their distribution gives us a sense of the variability in the regression predictions.\n\n\n\n\n\n\nPrediction Intervals\n\n\n\n\n\nIn addition to confidence intervals for the regression coefficients and their associated predictions, one can also try to account for the randomness in the response variable \\(Y\\) itself. That is, in the linear regression model, we assume that the response variable \\(Y\\) is normally distributed around the regression line with some variance \\(\\sigma^2\\).\nIn other words, the confidence intervals tell us about the variability of our predictions, but not about the variability of the actual response variable \\(Y\\). Prediction intervals aim to capture this additional source of variability by providing a range of values within which we expect future observations to fall, given the same values of the predictor variables.\nDetails of generating predictions intervals are beyond the scope of this course (for now).",
    "crumbs": [
      "Home",
      "Lecture 09: Regression Inference and Multiple Regression"
    ]
  },
  {
    "objectID": "notebooks/lecture-09.html#inference-in-regression",
    "href": "notebooks/lecture-09.html#inference-in-regression",
    "title": "Lecture 09: Regression inference and multiple regression",
    "section": "",
    "text": "Last lecture we introduced linear regression as a way to model the relationship between a response variable \\(Y\\) and some predictor variable \\(X\\). The model assumes that the relationship can be described by a linear equation:\n\\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\epsilon\\) is the error term. The error term captures the variability in \\(Y\\) that cannot be explained by \\(X\\), and we assume it follows a normal distribution with mean 0 (i.e. the deviations from the true regression line are zero on average) and some variance \\(\\sigma^2\\).\nThroughout this course, we have focused on sampling variability and how it affects our ability to make inferences about a population based on a sample. In the context of regression, this means that if we take different samples from the same population, we will get different estimates of the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\). This variability is due to the random nature of sampling and the inherent noise in the data.\nThe app below illustrates this concept by allowing you to draw samples from a population and see how the estimated regression coefficient (\\(\\hat{\\beta}_1\\)) varies across samples. Notice that the true slope of the regression line is fixed, but the estimated slope varies due to sampling variability.\nYou can also see how the stability of the estimate improves as the sample size increases, as well as the confidence implied by the \\(p\\)-value of the hypothesis test for the slope (\\(H_0: \\beta_1 = 0\\)). We will unpack this in more detail shortly.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render, reactive\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\n# Fixed population\nrng = np.random.default_rng(123)\nN_POP = 2000\nx_pop = rng.normal(size=N_POP)\ntrue_beta = 0.5\ny_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\npopulation = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\n\napp_ui = ui.page_fluid(\n    # Title\n    ui.div(\n        ui.h2(\"Sampling Variability in Regression Inference\"),\n        style=\"text-align:center; margin-bottom:20px; margin-top:20px; font-weight:bold;\"\n    ),\n    # Top row with inputs\n    ui.layout_columns(\n        ui.input_slider(\"n\", \"Sample size\", min=5, max=200, value=50, step=5, width=\"300px\"),\n        ui.input_numeric(\"perm\", \"Number of permutations\", value=500, min=100, max=2000, step=100, width=\"200px\"),\n        ui.input_action_button(\"resample\", \"Draw New Sample\", width=\"200px\"),\n    ),\n    # Second row with two side-by-side plots\n    ui.navset_tab(\n        ui.nav_panel(\"Visualization\",\n            ui.div(\n                ui.output_text(\"coef_text\"),\n                style=\"text-align:center; font-size:18px; margin-top:10px; margin-bottom:10px;\"\n            ),\n            ui.layout_columns(\n                ui.output_plot(\"scatter_plot\"),\n                ui.output_plot(\"perm_plot\"),\n            )\n        ),\n        ui.nav_panel(\"Permutation Table\",\n            ui.div(\n                \"First 50 rows of original sample and first 10 permutations:\",\n                style=\"text-align:center; font-size:16px; margin-top:10px; margin-bottom:10px;\"\n            ),\n            ui.output_table(\"perm_table\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # ✅ Store the current sample as state\n    current_sample = reactive.value(pd.DataFrame())\n\n    # Update sample when button is clicked or sample size changes\n    @reactive.effect\n    @reactive.event(input.resample, input.n)\n    def _():\n        idx = rng.choice(len(population), size=input.n(), replace=False)\n        current_sample.set(population.iloc[idx])\n\n    # ✅ Compute regression results based on stored sample\n    @reactive.calc\n    def regression_results():\n        data = current_sample()\n        if data.empty:\n            return {\"beta_hat\": np.nan, \"p_val\": np.nan, \"perm_coefs\": np.array([]), \"perm_table\": pd.DataFrame()}\n\n        model = smf.ols(\"y ~ x\", data=data).fit()\n        beta_hat = model.params[\"x\"]\n        p_val = model.pvalues[\"x\"]\n\n        perm_coefs = []\n        perm_tables = []\n        for i in range(input.perm()):\n            y_perm = rng.permutation(data[\"y\"])\n            perm_model = smf.ols(\"y ~ x\", data=data.assign(y=y_perm)).fit()\n            perm_coefs.append(perm_model.params[\"x\"])\n\n            if i &lt; 10:  # store first 10 permutations for table\n                df_perm = pd.DataFrame({\"x\": data[\"x\"].values, f\"y_perm_{i+1}\": y_perm})\n                perm_tables.append(df_perm[[f\"y_perm_{i+1}\"]])\n\n        # Merge the first few permutations into one table\n        if perm_tables:\n            perm_table_df = pd.concat([data[[\"x\", \"y\"]].reset_index(drop=True)] + perm_tables, axis=1)\n        else:\n            perm_table_df = pd.DataFrame()\n        \n        # compute permutation p-value\n        perm_coefs = np.array(perm_coefs)\n        # two-tailed p-value\n        p_val_perm = np.mean(np.abs(perm_coefs) &gt;= np.abs(beta_hat))\n\n        return {\n            \"beta_hat\": beta_hat,\n            \"p_val\": p_val,\n            \"p_val_perm\": p_val_perm,\n            \"perm_coefs\": np.array(perm_coefs),\n            \"perm_table\": perm_table_df\n        }\n\n    @output\n    @render.text\n    def coef_text():\n        res = regression_results()\n        if np.isnan(res[\"beta_hat\"]):\n            return \"Click 'Draw New Sample' to begin.\"\n        return f\"True slope: {true_beta:.3f}, Estimated slope: {res['beta_hat']:.3f}, Parametric p-value: {res['p_val']:.4f}, Permutation p-value: {res['p_val_perm']:.4f}\"\n\n    @output\n    @render.plot\n    def scatter_plot():\n        data = current_sample()\n        res = regression_results()\n\n        plt.figure(figsize=(7, 5))\n        sns.scatterplot(x=\"x\", y=\"y\", data=population, alpha=0.1, color=\"gray\")\n\n        if not data.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"blue\", label=\"Sampled Points\")\n            x_vals = np.linspace(population[\"x\"].min(), population[\"x\"].max(), 100)\n            y_vals = res[\"beta_hat\"] * x_vals + data[\"y\"].mean() - res[\"beta_hat\"] * data[\"x\"].mean()\n            plt.plot(x_vals, y_vals, color=\"red\", lw=2, label=\"Estimated Regression Line\")\n            # Add true population regression line\n            plt.plot(x_vals, true_beta * x_vals, alpha=0.3, color=\"gray\", linestyle=\"--\", label=\"True Regression Line\")\n            plt.legend()\n\n\n        plt.xlabel(\"X\")\n        plt.ylabel(\"Y\")\n        plt.xlim(population[\"x\"].min(), population[\"x\"].max())\n        plt.ylim(population[\"y\"].min(), population[\"y\"].max())\n        plt.title(\"Sampled Points and Fitted Regression Line\")\n\n    @output\n    @render.plot\n    def perm_plot():\n        res = regression_results()\n        if len(res[\"perm_coefs\"]) == 0:\n            plt.figure()\n            plt.text(0.5, 0.5, \"Click 'Draw New Sample' to start.\", ha=\"center\", va=\"center\")\n            return\n\n        plt.figure(figsize=(7, 4))\n        sns.histplot(res[\"perm_coefs\"], bins=30, kde=False, color=\"gray\")\n        plt.axvline(res[\"beta_hat\"], color=\"red\", lw=2, label=\"Observed β̂\")\n        plt.axvline(true_beta, color=\"blue\", lw=2, linestyle=\"--\", label=\"True β\")\n        plt.xlabel(r\"Estimated slope ($\\hat{\\beta}$)\")\n        plt.ylabel(\"Count\")\n        plt.xlim(min(-1, res[\"perm_coefs\"].min()), max(1, res[\"perm_coefs\"].max()))\n        plt.title(r\"Null Distribution of $\\hat{\\beta}$ (Permutation Test)\")\n        plt.legend()\n    @output\n    @render.table\n    def perm_table():\n        res = regression_results()\n        df = res[\"perm_table\"]\n        if df.empty:\n            return pd.DataFrame()\n        return df.head(50)  # Only show first 50 rows\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\nFitting the regression model\n\n\n\n\n\nAs in the previous lecture, we use the statsmodels library to fit the regression model.\nUnder the hood, the app is doing the following:\n# Fixed population\nrng = np.random.default_rng(123)\nN_POP = 2000\nx_pop = rng.normal(size=N_POP)\ntrue_beta = 0.5\ny_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\npopulation = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\nThis creates a fixed population of 2000 points with a known slope of 0.5. The app then allows you to draw samples from this population and fit a regression model to the sampled data.\nMore specifically, we subsample the population to create a new sample of size n:\nidx = rng.choice(len(population), size=n, replace=False)\ndata = population.iloc[idx]\nThen we fit the regression model using statsmodels:\nmodel = smf.ols('y ~ x', data=data).fit()\n    beta_hat = model.params['x']\n\n\n\n\n\nIn regression analysis, we often want to test hypotheses about the relationship between the predictor variable \\(X\\) and the response variable \\(Y\\). The most common hypothesis to test is whether the slope of the regression line (\\(\\beta_1\\)) is different from zero, which would indicate that there is a relationship between \\(X\\) and \\(Y\\).\nSo we can set up the following hypotheses: \\[\n\\begin{align*}\nH_0:& ~\\beta_1 = 0 \\quad \\text{(no relationship)} \\\\\nH_1:& ~\\beta_1 \\neq 0 \\quad \\text{(there is a relationship)}\n\\end{align*}\n\\]\nWe’re back in our comfort zone now. How can we simulate the distribution of the estimated slope \\(\\hat{\\beta}_1\\) under the null hypothesis? We’ll use a permutation test – take a shot at implementing it in the code cell below.\n\nExerciseHintSolution\n\n\n\n\n\n\n\n\n\n\n\nTo implement the permutation test, you will need to:\n\nFit the regression model to the original data to get the observed slope \\(\\hat{\\beta}_1\\).\nFor each permutation:\n\nPermute the response variable \\(Y\\) to break the relationship with \\(X\\).\nFit the regression model to the permuted data.\nStore the estimated slope \\(\\hat{\\beta}_1\\) from the permuted data.\n\nCompute the p-value as the proportion of permuted slopes that are greater than or equal to the observed slope in absolute value (two-tailed test).\n\n\n\n\n\ndef permutation_test_regression(data, n_permutations=1000, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    # first, fit the regression model (on the original data)\n    model = sm.ols('y ~ x', data=data).fit()\n    beta_hat = model.params['x']\n    \n    perm_beta_hats = [] # list to store permuted slopes\n    for _ in range(n_permutations):\n        # permute the response variable y\n        # this breaks the relationship between x and y\n        y_perm = rng.permutation(data['y'])\n        # fit the regression model to the permuted data\n        perm_model = sm.OLS(y_perm, X).fit()\n        # store the estimated slope\n        perm_beta_hats.append(perm_model.params['x'])\n    # the slopes from the permuted data are\n    # your null distribution\n    perm_beta_hats = np.array(perm_beta_hats)\n    # compute p-value\n    p_val = np.mean(np.abs(perm_beta_hats) &gt;= np.abs(beta_hat))\n    \n    return {\n        \"beta_hat\": beta_hat,\n        \"perm_beta_hats\": perm_beta_hats,\n        \"p_val\": p_val,\n    }\nresult_dict = permutation_test_regression(data, n_permutations=1000, rng=rng)\nprint(\"Observed slope (beta_hat):\", result_dict['beta_hat'])\nprint(\"Permutation p-value:\", result_dict['p_val'])\n\n\n\n\nThis is precisely the permutation test that generates the null distribution displayed in the app.\nNotice that the permuation test \\(p\\)-value is nearly identical to the parametric \\(p\\)-value computed by statsmodels using the assumption that the errors are normally distributed.\n\n\n\n\n\n\nParametric inference in regression\n\n\n\n\n\nThe app uses statsmodels to compute the regression coefficients and their associated p-values. To get these \\(p\\)-values (without doing simulations / randomization), you have to make assumptions about the underlying probability distributions.\nIn the linear regression model, the typical assumption is that (as we discussed in the previous lecture) the errors \\(\\epsilon\\) are normally distributed. This makes the \\(Y\\) values themselves normal random variables (i.e. \\(Y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X, \\sigma^2)\\)). This assumption allows us to use the properties of the normal distribution to figure out analytical expressions for the uncertainty in the estimated coefficients (a.k.a. the standard errors) and the \\(p\\)-values for hypothesis tests.\nThe details of the parametric inference are beyond the scope of this course but you are encouraged to take more advanced courses in statistics or econometrics to learn more about it!\n\n\n\n\n\n\n\\(p\\)-values are nice, but they don’t tell us the whole story. We might want to get a sense of the range of plausible values for the regression coefficients, because the effect size is important too.\n\n\n\n\n\n\nEffect Size\n\n\n\n\n\nEffect size refers to the magnitude of a quantity you are interested in estimating – often a difference in averages or a regression coefficient.\nFor a simple example, let’s go all the way back to coin flips. We saw examples where we tried to estimate the probability of seeing a certain set of outcomes if the coin was fair. We saw that with lots of coin flips, we were able to confidently detect a biased coin. But we saw an example where the coin was very biased (only 25% heads). What if the coin was just a little biased (say 51% heads)? This is a much smaller effect size, and it would be harder to detect with the same number of coin flips – but with enough flips we could still detect it. Is a 1% difference in probability of heads worth fighting over? What about a 0.1% difference?\nThis comes up a lot in scientific research. Say you are testing a new drug and you find that it reduces symptoms by 5% compared to a placebo. Is that a meaningful effect? What if the drug only reduces symptoms by 0.5%? Producing the drug might be very expensive, so a small effect size might not be worth the cost (or possibly the risk of side effects).\n\n\n\nIn addition to hypothesis testing, we can also use bootstrapping to construct confidence intervals for the regression coefficients.\nHere’s some code that does this:\n\n\nCode\nimport statsmodels.formula.api as sm\nimport pandas as pd\nimport numpy as np\n\n\ndef bootstrap_regression_ci(data, x_col, y_col, n_bootstraps=1000, alpha=0.05):\n    n = len(data)\n    coefs = []\n    for _ in range(n_bootstraps):\n1        sample = data.sample(n, replace=True)\n2        model = sm.ols(f\"{y_col} ~ {x_col}\", data=sample).fit()\n3        coefs.append(model.params[x_col])\n    coefs = np.array(coefs)\n4    lower_bound = np.percentile(coefs, 100 * alpha / 2)\n    upper_bound = np.percentile(coefs, 100 * (1 - alpha / 2))\n    return coefs, lower_bound, upper_bound\n\n\nrng = np.random.default_rng(123)\n# Generate synthetic data\nx = rng.normal(size=100)\ny = 1.5 * x + 2 + rng.normal(size=100)\ndata = pd.DataFrame({\"x\": x, \"y\": y})\n\n5model = sm.ols(\"y ~ x\", data=data).fit()\n\nbootstrap_coefs, lower_bound, upper_bound = bootstrap_regression_ci(\n    data, \"x\", \"y\", n_bootstraps=1000\n)\nprint(\"#\" * 40)\nprint(f\"Bootstrap CI for slope: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nprint(\"#\" * 40)\n6model.summary(slim=True)\n\n\n\n1\n\nBootstrap the data to create multiple samples (same size as the original sample, sampled with replacement).\n\n2\n\nFit the regression model to each bootstrap sample.\n\n3\n\nStore the estimated coefficients from each bootstrap sample.\n\n4\n\nCompute the confidence intervals based on the distribution of the bootstrap estimates.\n\n5\n\nFit the regression model to the original data to get the point estimates.\n\n6\n\nPrint statsmodels summary table with the confidence intervals.\n\n\n\n\n########################################\nBootstrap CI for slope: [1.360, 1.771]\n########################################\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.699\n\n\nModel:\nOLS\nAdj. R-squared:\n0.696\n\n\nNo. Observations:\n100\nF-statistic:\n227.5\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n2.71e-27\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.0933\n0.093\n22.569\n0.000\n1.909\n2.277\n\n\nx\n1.5567\n0.103\n15.084\n0.000\n1.352\n1.761\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe summary table has lots of extra information you can ignore for now, but the key part is the coef column which contains the point estimates of the regression coefficients, and the [0.025, 0.975] columns which contain the lower and upper bounds of the 95% confidence intervals.\nWe also printed the bootstrapped confidence intervals, and you can see that they are very similar to the ones computed by statsmodels using the normality assumption.\nStill confused? Try the following app to visualize what’s going on:\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 950\n\nfrom shiny import App, ui, render, reactive\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(123)\nN_POP = 1000\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"beta\", \"True Slope (β)\", min=-2, max=2, value=0.5, step=0.1),\n            ui.input_slider(\"sigma\", \"Noise SD (σ)\", min=0.1, max=2.0, value=1.0, step=0.1),\n            ui.input_slider(\"n\", \"Original Sample Size\", min=10, max=500, value=100),\n            ui.input_action_button(\"draw_sample\", \"Draw Original Sample\"),\n            ui.input_action_button(\"bootstrap1\", \"Bootstrap 1x\", disabled=True),\n            ui.input_action_button(\"bootstrap10\", \"Bootstrap 10x\", disabled=True),\n            ui.input_action_button(\"bootstrap100\", \"Bootstrap 100x\", disabled=True),\n            ui.input_action_button(\"reset\", \"Reset\", disabled=True),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Scatter and Slopes Histogram\",\n                ui.output_plot(\"scatter_plot\"),\n                ui.div(\n                    ui.output_text(\"count_text\"),\n                    style=\"text-align:center; font-size:16px; margin-top:10px;\"\n                ),\n                ui.output_plot(\"hist_plot\"),\n            ),\n            ui.nav_panel(\"Bootstrapped Regression Lines\",\n                ui.output_plot(\"lines_plot\"),\n            ),\n        ),\n    )\n)\n\ndef server(input, output, session):\n    original_sample = reactive.value(pd.DataFrame())\n    current_bootstrap = reactive.value(pd.DataFrame())\n    bootstrap_coefs = reactive.value([])\n\n    # Generate population reactively\n    @reactive.calc\n    def population():\n        x = rng.normal(size=N_POP)\n        y = input.beta() * x + input.sigma() * rng.normal(size=N_POP)\n        return pd.DataFrame({\"x\": x, \"y\": y})\n\n    # Helper to enable/disable buttons\n    def set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False):\n        session.send_input_message(\"bootstrap1\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"bootstrap10\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"bootstrap100\", {\"disabled\": not enable_bootstrap})\n        session.send_input_message(\"draw_sample\", {\"disabled\": not enable_draw})\n        session.send_input_message(\"reset\", {\"disabled\": not enable_reset})\n\n    @reactive.effect\n    def _init_buttons():\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Reset sample & bootstrap whenever population changes\n    @reactive.effect\n    @reactive.event(input.beta, input.sigma, input.n)\n    def _reset_on_population_change():\n        original_sample.set(pd.DataFrame())\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Draw original sample\n    @reactive.effect\n    @reactive.event(input.draw_sample)\n    def draw_sample():\n        sample = population().sample(n=input.n(), replace=False)\n        original_sample.set(sample)\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=True, enable_draw=False, enable_reset=True)\n\n    def bootstrap_once():\n        sample = original_sample()\n        if sample.empty:\n            return\n        resample = sample.sample(n=len(sample), replace=True)\n        current_bootstrap.set(resample)\n        model = smf.ols(\"y ~ x\", data=resample).fit()\n        bootstrap_coefs.set(bootstrap_coefs() + [(model.params[\"Intercept\"], model.params[\"x\"])])\n\n    # Bootstrap buttons\n    @reactive.effect\n    @reactive.event(input.bootstrap1)\n    def boot1():\n        bootstrap_once()\n\n    @reactive.effect\n    @reactive.event(input.bootstrap10)\n    def boot10():\n        for _ in range(10):\n            bootstrap_once()\n\n    @reactive.effect\n    @reactive.event(input.bootstrap100)\n    def boot100():\n        for _ in range(100):\n            bootstrap_once()\n\n    # Reset button\n    @reactive.effect\n    @reactive.event(input.reset)\n    def reset_all():\n        original_sample.set(pd.DataFrame())\n        current_bootstrap.set(pd.DataFrame())\n        bootstrap_coefs.set([])\n        set_buttons(enable_bootstrap=False, enable_draw=True, enable_reset=False)\n\n    # Bootstrap count text\n    @output\n    @render.text\n    def count_text():\n        count = len(bootstrap_coefs())\n        if count == 0:\n            return \"No bootstrap samples yet.\"\n        elif count == 1:\n            return \"1 bootstrap sample drawn.\"\n        else:\n            return f\"{count} bootstrap samples drawn.\"\n\n    # Scatter plot\n    @output\n    @render.plot\n    def scatter_plot():\n        plt.figure(figsize=(7, 4.5))\n        pop = population()\n\n        pop_model = smf.ols(\"y ~ x\", data=pop).fit()\n        x_vals = np.linspace(pop[\"x\"].min(), pop[\"x\"].max(), 100)\n        y_vals = pop_model.params[\"Intercept\"] + pop_model.params[\"x\"] * x_vals\n        plt.plot(x_vals, y_vals, color=\"gray\", lw=2, linestyle=\":\", label=\"Population line\")\n\n        sample = original_sample()\n        if not sample.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=sample, color=\"black\", alpha=0.4, label=\"Original sample\")\n            model = smf.ols(\"y ~ x\", data=sample).fit()\n            y_vals_s = model.params[\"Intercept\"] + model.params[\"x\"] * x_vals\n            plt.plot(x_vals, y_vals_s, color=\"black\", linestyle=\"--\", label=\"Original line\", alpha=0.7, lw=2)\n\n        boot = current_bootstrap()\n        if not boot.empty:\n            sns.scatterplot(x=\"x\", y=\"y\", data=boot, color=\"blue\", alpha=0.7, label=\"Bootstrap sample\")\n            model_b = smf.ols(\"y ~ x\", data=boot).fit()\n            y_vals_b = model_b.params[\"Intercept\"] + model_b.params[\"x\"] * x_vals\n            plt.plot(x_vals, y_vals_b, color=\"blue\", lw=2, label=\"Bootstrap line\")\n\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.title(\"Population, original sample, and (most recent) bootstrap sample\")\n        plt.legend()\n\n    # Histogram\n    @output\n    @render.plot\n    def hist_plot():\n        coefs = bootstrap_coefs()\n        # unpack (Intercept, slope) tuples\n        plt.figure(figsize=(7, 3.5))\n\n        if coefs:\n            coefs = [coef[1] for coef in coefs]  # extract slopes only\n            sns.histplot(coefs, bins=20, color=\"lightblue\", kde=False)\n            q_low, q_high = np.quantile(coefs, [0.025, 0.975])\n            plt.axvline(q_low, color=\"purple\", linestyle=\"--\", lw=2, label=\"2.5% Quantile\")\n            plt.axvline(q_high, color=\"purple\", linestyle=\"--\", lw=2, label=\"97.5% Quantile\")\n            plt.fill_betweenx(\n                [0, plt.ylim()[1]], q_low, q_high, color=\"purple\", alpha=0.05, label=\"95% CI\"\n            )\n\n        sample = original_sample()\n        if not sample.empty:\n            model_orig = smf.ols(\"y ~ x\", data=sample).fit()\n            obs_slope = model_orig.params[\"x\"]\n            plt.axvline(obs_slope, color=\"black\", lw=2, label=r\"Observed $\\hat{\\beta}$\")\n\n        plt.xlabel(\"Bootstrap Slope Estimates\")\n        plt.ylabel(\"Frequency\")\n        plt.title(r\"Bootstrap distribution of $\\hat{\\beta}$\")\n        plt.legend()\n\n    @output\n    @render.plot\n    def lines_plot():\n        plt.figure(figsize=(7, 5))\n        x_vals = np.linspace(-3, 3, 100)\n\n        # Draw all bootstrap lines\n        for (intercept, slope) in bootstrap_coefs():\n            plt.plot(x_vals, intercept + slope * x_vals, color=\"blue\", alpha=0.1)\n\n        # Draw original sample line\n        sample = original_sample()\n        if not sample.empty:\n            m = smf.ols(\"y ~ x\", data=sample).fit()\n            plt.plot(x_vals, m.params[\"Intercept\"] + m.params[\"x\"] * x_vals,\n                     color=\"black\", lw=2, label=\"Original Line\")\n\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.title(\"Bootstrap Regression Lines\")\n        plt.legend()\n\n\napp = App(app_ui, server)\n\nThe bootstrap distribution of the estimated slope \\(\\hat{\\beta}_1\\) gives us confidence intervals for the slope. See the second panel of the app for a visualization of bootstrapped regression lines (slope and intercept). As you draw more bootstrap samples, you’ll see that the regression lines concentrate around the original sample’s regression line, but their distribution gives us a sense of the variability in the regression predictions.\n\n\n\n\n\n\nPrediction Intervals\n\n\n\n\n\nIn addition to confidence intervals for the regression coefficients and their associated predictions, one can also try to account for the randomness in the response variable \\(Y\\) itself. That is, in the linear regression model, we assume that the response variable \\(Y\\) is normally distributed around the regression line with some variance \\(\\sigma^2\\).\nIn other words, the confidence intervals tell us about the variability of our predictions, but not about the variability of the actual response variable \\(Y\\). Prediction intervals aim to capture this additional source of variability by providing a range of values within which we expect future observations to fall, given the same values of the predictor variables.\nDetails of generating predictions intervals are beyond the scope of this course (for now).",
    "crumbs": [
      "Home",
      "Lecture 09: Regression Inference and Multiple Regression"
    ]
  },
  {
    "objectID": "notebooks/lecture-09.html#multiple-regression",
    "href": "notebooks/lecture-09.html#multiple-regression",
    "title": "Lecture 09: Regression inference and multiple regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nSo far, we have only considered simple linear regression, where we have one predictor variable \\(X\\) and one response variable \\(Y\\). However, in many real-world scenarios, we have multiple predictor variables that can influence the response variable simultaneously.\nThis leads us to multiple linear regression, which extends the simple linear regression model to include multiple predictors: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon \\]\nwhere \\(X_1, X_2, \\ldots, X_p\\) are the predictor variables, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\ldots, \\beta_p\\) are the slopes for each predictor, and \\(\\epsilon\\) is the error term.\nIn multiple regression, we can still use the same principles of hypothesis testing and confidence intervals.\nLet’s start off with a simple example of multiple regression using the statsmodels library.\nWe’re going to use a dataset with information about students’ academic performance in secondary school.1 The dataset contains various features such as study time, family information, and lifestyle (e.g. how much they go out),and much more. The goal is to predict the final grade of the students in a secondary school course (a numeric value out of 20). There are datasets for two subjects: Mathematics and Portuguese, but we’ll focus on the Mathematics dataset.\n1 Paper: Cortez, P (2008) “Using Data Mining to Predict Secondary School Student Performance”. Dataset: https://doi.org/10.24432/C5TG7T\n\nCode\nstudent_data = pd.read_csv(\"../data/student-mat.csv\", sep=\";\")\nstudent_data.head()\n\n\n\n\n\n\n\n\n\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\n...\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences\nG1\nG2\nG3\n\n\n\n\n0\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\n...\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\n\n\n1\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\n...\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\n\n\n2\nGP\nF\n15\nU\nLE3\nT\n1\n1\nat_home\nother\n...\n4\n3\n2\n2\n3\n3\n10\n7\n8\n10\n\n\n3\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\n...\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\n\n\n4\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\n...\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\n\n\n\n\n5 rows × 33 columns\n\n\n\nHere’s a breakdown of what the columns in the dataset represent:\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nType / Values\n\n\n\n\nschool\nStudent’s school\nGP = Gabriel Pereira, MS = Mousinho da Silveira\n\n\nsex\nStudent’s sex\nF = female, M = male\n\n\nage\nStudent’s age\nNumeric (15–22)\n\n\naddress\nHome address type\nU = urban, R = rural\n\n\nfamsize\nFamily size\nLE3 = ≤3, GT3 = &gt;3\n\n\nPstatus\nParent’s cohabitation status\nT = living together, A = apart\n\n\nMedu\nMother’s education\n0 = none, 1 = primary, 2 = 5th–9th, 3 = secondary, 4 = higher\n\n\nFedu\nFather’s education\n0 = none, 1 = primary, 2 = 5th–9th, 3 = secondary, 4 = higher\n\n\nMjob\nMother’s job\nteacher, health, services, at_home, other\n\n\nFjob\nFather’s job\nteacher, health, services, at_home, other\n\n\nreason\nReason to choose this school\nhome, reputation, course, other\n\n\nguardian\nStudent’s guardian\nmother, father, other\n\n\ntraveltime\nHome-to-school travel time\n1 = &lt;15m, 2 = 15–30m, 3 = 30–60m, 4 = &gt;1h\n\n\nstudytime\nWeekly study time\n1 = &lt;2h, 2 = 2–5h, 3 = 5–10h, 4 = &gt;10h\n\n\nfailures\nNumber of past class failures\nn if 1≤n&lt;3, else 4\n\n\nschoolsup\nExtra educational support\nyes / no\n\n\nfamsup\nFamily educational support\nyes / no\n\n\npaid\nExtra paid classes in the subject\nyes / no\n\n\nactivities\nExtra-curricular activities\nyes / no\n\n\nnursery\nAttended nursery school\nyes / no\n\n\nhigher\nWants to take higher education\nyes / no\n\n\ninternet\nInternet access at home\nyes / no\n\n\nromantic\nWith a romantic relationship\nyes / no\n\n\nfamrel\nQuality of family relationships\n1 = very bad … 5 = excellent\n\n\nfreetime\nFree time after school\n1 = very low … 5 = very high\n\n\ngoout\nGoing out with friends\n1 = very low … 5 = very high\n\n\nDalc\nWorkday alcohol consumption\n1 = very low … 5 = very high\n\n\nWalc\nWeekend alcohol consumption\n1 = very low … 5 = very high\n\n\nhealth\nCurrent health status\n1 = very bad … 5 = very good\n\n\nabsences\nNumber of school absences\nNumeric (0–93)\n\n\nG1\nFirst period grade\n0–20\n\n\nG2\nSecond period grade\n0–20\n\n\nG3\nFinal grade (target variable)\n0–20\n\n\n\n\nLet’s pick a model. If we want to predict the final grade \\(G3\\), then perhaps we can use information about the student’s family history to help us. In particular, one might expect that the education level of the parents might be a good predictor of the student’s performance (educated parents might be more likely to help with homework, have higher expectations, etc.).\nWe can fit a multiple regression model with the final grade \\(G3\\) as the response variable and the father’s education level \\(Fedu\\) as a predictor.\n\n\nCode\nmodel = smf.ols(\"G3 ~ Fedu\", data=student_data).fit()\nmodel.summary(slim=True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nG3\nR-squared:\n0.023\n\n\nModel:\nOLS\nAdj. R-squared:\n0.021\n\n\nNo. Observations:\n395\nF-statistic:\n9.352\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n0.00238\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n8.7967\n0.576\n15.264\n0.000\n7.664\n9.930\n\n\nFedu\n0.6419\n0.210\n3.058\n0.002\n0.229\n1.055\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNice, it looks like a good predictor. Looking at the confidence interval and \\(p\\)-value, we see that it is pretty clear there is a nonzero effect of Fedu on G3. So we’re done, right?\nWrong! There are many other variables in the dataset that could also influence the final grade. Perhaps most obviously, the mother’s education level Medu could also be important.\nLet’s take a look at how these variables are related to each other. Below we’ll show the correlation matrix between these variables, and then we’ll visualize their relationships using pairwise scatterplots.\n\n\nCode\nstudent_data[[\"Fedu\", \"Medu\", \"G3\"]].corr().apply(lambda x: x**2).style.background_gradient(cmap=\"coolwarm\")\n\n\n\n\n\n\n\n \nFedu\nMedu\nG3\n\n\n\n\nFedu\n1.000000\n0.388696\n0.023243\n\n\nMedu\n0.388696\n1.000000\n0.047153\n\n\nG3\n0.023243\n0.047153\n1.000000\n\n\n\n\n\n\n\nCode\nsns.pairplot(student_data, vars=[\"Medu\", \"Fedu\", \"G3\"], kind=\"reg\", height=3, aspect=1.2)\n\n\n\n\n\n\n\n\n\nWow, now that we look at the data, we see that Fedu and Medu are correlated with each other (meaning that students with educated fathers tend to also have educated mothers). And it seems like Medu is even more strongly correlated with the final grade G3 than Fedu is.\nAll of a sudden this is quite complicated. If a student’s parents are often both educated, then how can we tell which parent is more important for the student’s final grade? We don’t know how to disentangle the effects of Fedu and Medu on G3 because they are correlated with each other.\nThis problem is called multicollinearity. It occurs when two or more predictor variables in a regression model are highly correlated with each other, making it difficult to determine the individual effect of each predictor on the response variable.\nThe question we really want to answer is: how much does each predictor contribute to the final grade, after accounting for the other predictors?\nLet’s think about predicting the final grade G3 based on the father’s education Fedu, while accounting for the mother’s education Medu.\nFirst, we can fit a simple linear regression model with Medu as the predictor and G3 as the response variable. Whatever is left over from this regression (the residuals) is the part of G3 that is not already explained by Medu.\nThen, we fit another regression model to predict Fedu using Medu as the predictor. The residuals from this regression represent the part of Fedu that was not already accounted for by Medu.\nFinally, we can fit a regression model with the residuals of Fedu as the predictor and the residuals of G3 as the response variable. The slope of this regression model tells us how much Fedu contributes to G3 after accounting for the effect of Medu.\nLet’s see this in action:\n\n\nCode\nfrom statsmodels.formula.api import ols\n\nfrom matplotlib import gridspec\nfig = plt.figure(figsize=(10, 8)); gs = gridspec.GridSpec(3, 2, figure=fig)\n# Top 2x2 grid\nax1, ax2, ax3, ax4 = (fig.add_subplot(gs[0, 0]), \n                      fig.add_subplot(gs[0, 1]),\n                      fig.add_subplot(gs[1, 0]),\n                      fig.add_subplot(gs[1, 1]))\nax5 = fig.add_subplot(gs[2, :])  # bottom row spanning both columns\n\n# regress G3 on Fedu, obtaining the residuals\nresiduals_G3 = ols(\"G3 ~ Medu\", data=student_data).fit().resid\n\n# plot the regression of G3 on Medu\nsns.regplot(x=\"Medu\", y=\"G3\", data=student_data, line_kws={\"color\": \"red\"}, ax=ax1)\nax1.set_title(\"Regression of G3 on Medu\")\n# plot the residuals of the regression\nsns.residplot(x=\"Medu\", y=\"G3\", data=student_data, ax=ax2)\nax2.set_title(\"Residuals of G3 on Medu\")\nax2.set_ylabel(\"G3 - Predicted G3\")\n\n# then regress Fedu on Medu to get the residuals\nresiduals_Fedu = ols(\"Fedu ~ Medu\", data=student_data).fit().resid\n# plot the regression of Fedu on Medu\nsns.regplot(x=\"Medu\", y=\"Fedu\", data=student_data, line_kws={\"color\": \"red\"}, ax=ax3)\nax3.set_title(\"Regression of Fedu on Medu\")\n# plot the residuals of Fedu on Medu\nsns.residplot(x=\"Medu\", y=\"Fedu\", data=student_data, ax=ax4)\nax4.set_title(\"Residuals of Fedu on Medu\")\nax4.set_ylabel(\"Fedu - Predicted Fedu\")\n\n# finally, regress the residuals of G3 on the residuals of Fedu\nmodel = ols(\"residuals_G3 ~ residuals_Fedu\", data=pd.DataFrame({\n    \"residuals_G3\": residuals_G3,\n    \"residuals_Fedu\": residuals_Fedu\n})).fit()\n\nsns.regplot(x=residuals_Fedu, y=residuals_G3, line_kws={\"color\": \"red\"}, ax=ax5)\nax5.set_title(\"Regression of Residuals G3 on Residuals Fedu\")\n\nplt.tight_layout()\nplt.show()\n\n# print the summary of the final model\nmodel.summary(slim=True)\n\n\n\n\n\n\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nresiduals_G3\nR-squared:\n0.001\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.002\n\n\nNo. Observations:\n395\nF-statistic:\n0.1968\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n0.658\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.714e-16\n0.225\n4.31e-15\n1.000\n-0.443\n0.443\n\n\nresiduals_Fedu\n0.1176\n0.265\n0.444\n0.658\n-0.404\n0.639\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe point of this approach is to isolate the effect of Fedu on G3 while controlling for how Medu influences both Fedu and G3.\nThis probably seems a bit cumbersome. Thankfully, this is exactly what multiple regression does for us! We can fit a multiple regression model with both Medu and Fedu as predictors of G3, and the model will automatically account for the correlation between the predictors.\nBelow we fit a multiple regression model with Medu and Fedu as predictors of G3, and get exactly the same result for the slope of Fedu as we did with the manual approach above.\n\n\nCode\nmodel = ols(\"G3 ~ Fedu + Medu\", data=student_data).fit()\nmodel.summary(slim=True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nG3\nR-squared:\n0.048\n\n\nModel:\nOLS\nAdj. R-squared:\n0.043\n\n\nNo. Observations:\n395\nF-statistic:\n9.802\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n7.01e-05\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n7.8205\n0.648\n12.073\n0.000\n6.547\n9.094\n\n\nFedu\n0.1176\n0.265\n0.443\n0.658\n-0.404\n0.639\n\n\nMedu\n0.8359\n0.264\n3.168\n0.002\n0.317\n1.355\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAfter fitting the multiple regression model, we can see that the coefficient for Fedu is much smaller than it was in the simple regression model, and the confidence interval overlaps with zero (meaning there may be no effect). The coefficient for Medu is larger, and its confidence interval does not overlap with zero, indicating that it has a significant effect on G3. 2\n2 One could make similar statements based on the p-values.So, multiple regression allows us to include multiple predictors in our model and account for their simultaneous effects on the response variable.\nIt is important to realize that multiple regression does not solve the problem of multicollinearity completely. If the predictors are highly correlated with each other, it can still lead to unstable estimates and difficulties in interpretation. Take an extreme example: what if the parents’ education levels were perfectly correlated (i.e., for every student, if the father has a certain education level, the mother has exactly the same level)? In this case, we would not be able to distinguish the effects of Fedu and Medu on G3 at all. There would be absolutely no data to distinguish between them (specifically, the residuals of Fedu on Medu would be zero for all students).",
    "crumbs": [
      "Home",
      "Lecture 09: Regression Inference and Multiple Regression"
    ]
  },
  {
    "objectID": "notebooks/lecture-09.html#causality",
    "href": "notebooks/lecture-09.html#causality",
    "title": "Lecture 09: Regression inference and multiple regression",
    "section": "Causality",
    "text": "Causality\nSo does this mean that we can conclude that the Mother’s education level causes the student’s final grade to increase?\nNot necessarily. For instance, it could be both parents’ education levels and a student’s final grade are influenced by a third variable, such as the family’s socioeconomic status. Meaning, more wealthy families can afford continuing education for their children across generations, and this leads to both higher parental education levels and better student performance (through tutoring and other resources).\nThis is the reason for the common adage in statistics: correlation does not imply causation. Just because two variables are correlated does not mean that one causes the other.\nThere are two main reasons why correlation does not imply causation:\n\nConfounding Variables: There may be a third variable that influences both variables, leading to a spurious correlation between them. The above example of parental education and student performance is an example of this. In the regression model we fit earlier, the mother’s education level Medu was a confounding variable that influenced both the father’s education level Fedu and the student’s final grade G3.\nReverse Causation: The relationship could be reversed, meaning that the second variable causes the first, rather than the other way around. In our student performance example, it is quite unlikely that the student’s final grade causes the parents’ education levels to change (we know this is not possible), but one could easily fit a regression model that suggests this is the case if we were not careful.\n\nTo drive home this second point, let’s do that:\n\n\nCode\nfrom statsmodels.formula.api import ols\nmodel = ols(\"Medu ~ Fedu + G3\", data=student_data).fit()\nmodel.summary(slim=True)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nMedu\nR-squared:\n0.404\n\n\nModel:\nOLS\nAdj. R-squared:\n0.401\n\n\nNo. Observations:\n395\nF-statistic:\n132.8\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n9.00e-45\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.9051\n0.136\n6.658\n0.000\n0.638\n1.172\n\n\nFedu\n0.6080\n0.040\n15.319\n0.000\n0.530\n0.686\n\n\nG3\n0.0299\n0.009\n3.168\n0.002\n0.011\n0.048\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThis regression model suggests that the student’s final grade G3 causes the mother’s education level Medu to increase. Without inventing a time machine, we know this is not possible.\n\nIncluding Confounding Variables in Regression\nLuckily for us, some of these problems can be mitigated by including confounding variables in our regression model. We saw an example of this earlier when we included both Medu and Fedu as predictors of G3. The problem is that the regression model does not automatically tell us which variable is the cause and which is the effect. It only tells us how the variables are related to each other.\nTo determine causality, we need to use additional methods such as randomized controlled trials (RCTs), or more advanced statistical techniques that we’re not going to cover in this course.\n\n\nRandomized Controlled Trials (RCTs)\nRCTs are considered the gold standard for establishing causality. In an RCT, participants are randomly assigned to either a treatment group (which receives the intervention) or a control group (which does not receive the intervention). This randomization helps to eliminate confounding variables and ensures that any differences in outcomes between the groups can be attributed to the treatment itself.\nThink about it this way: if we randomly assigned students to different levels of parental education (e.g., some students have educated parents, while others do not), then we could directly measure the effect of parental education on student performance without worrying about confounding variables (like how wealthy the family is).\nUnfortunately, RCTs are not always feasible or ethical in practice. For example, we cannot randomly assign students to different families or change their parental education levels. In such cases, we must rely on observational data and statistical techniques to infer causality, while being aware of the limitations and potential biases in our analyses.",
    "crumbs": [
      "Home",
      "Lecture 09: Regression Inference and Multiple Regression"
    ]
  },
  {
    "objectID": "assignments/assignment-00.html",
    "href": "assignments/assignment-00.html",
    "title": "Assignment 00",
    "section": "",
    "text": "print(\"Hello, World!\")\n\nHello, World!\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"A\": [1, 2, 3, 4, 5]})\ndf[\"B\"] = df[\"A\"] ** 2\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1\n\n\n1\n2\n4\n\n\n2\n3\n9\n\n\n3\n4\n16\n\n\n4\n5\n25\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.pointplot(data=df, x=\"A\", y=\"B\")"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02: Probability",
    "section": "",
    "text": "Problems adapted from MIT OpenCourseWare.\nThis assignment focuses mostly on the mathematical fundamentals of probability. It will have a few programming tasks, but most of the work will “pen and paper” style calculations. If you want, you can use Python to help with the calculations, but it is not required.\n\nProblem 1\n\n1a: Layla’s Puppies\nOur dog Layla had two puppies. The older one is female.\nWhat is the probability that both puppies are female?\n\n\n1b. Ariel’s Kittens\nOur cat Ariel had two kittens. At least one of them is male.\nWhat is the probability that both kittens are male? Show your work.\n\n\n\nProblem 2: The Blue Taxi\nIn a city with 100 taxis, 1 is blue and 99 are green. A witness reports the hit-and-run taxi was blue. Under test conditions, that witness sees blue cars as blue 99% of the time, and green cars as blue 2% of the time.\nExplain why seeing blue doesn’t mean the blue taxi is most likely guilty. You may use numbers or a small diagram or table to illustrate your reasoning.\n\n\n4. Birthday Matches\nLet (n) people (not including yourself) each have a birthday ({1,,365}). Assume that each birthday is equally likely, and that the birthdays of different people are independent.\n\nWrite down an equation for the probability that no one shares your birthday.\nFor which (n) does that probability first drop below 0.5?\n\n\n\n5. 6-Card Poker Hands\nWe draw 6 cards from a standard 52-card deck. Two interesting types of hands:\n\nTwo-pair: two cards of one rank, two of another, and two of two other ranks.\nThree-of-a-kind: three cards of one rank and three of three other distinct ranks.\n\nQuestion: 1. Derive the probability of each hand (two-pair vs. three-of-a-kind). 2. Which is more likely?\n\n\n6. Non-Transitive Dice\nThree dice are labeled as follows:\n\nBlue: [3,3,3,3,3,6]\nOrange: [1,4,4,4,4,4]\nWhite: [2,2,2,5,5,5]\n\nCompute\n\nP(White &gt; Orange)\nP(Orange &gt; Blue)\nP(Blue &gt; White)\n\nCan you order the dice from “best” to “worst” based on these comparisons?"
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 04: Hypothesis Testing and Bootstrapping",
    "section": "",
    "text": "Hypothesis testing with flights data\nThis is the least structured problem you have received to date. The goal is for you to explore the given dataset and apply hypothesis testing techniques to turn vague questions into concrete statistical tests.\nBelow is code to import a dataset of flights across the US in January 2025.\nThink about how you might use this data to design hypothesis tests and answer the following questions: 1. Does the time of day affect flight delays? 2. Do flights from different airlines have different delay patterns? (Hint: is there a difference in how long flights are delayed, or how often they are delayed?)\n\nimport pandas as pd\n\nurl = (\n    \"https://www.dropbox.com/scl/fi/\"\n    \"nnlww9mk1vmevn1lytywr/flights_ontime.csv\"\n    \"?rlkey=iska1a863ezg640lvd86wgoky&dl=1\"\n)\n\n# 1) read FL_DATE and pull times in as strings\ntime_cols = [\"ARR_TIME\", \"DEP_TIME\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\"]\n\ndf = (\n    pd.read_csv(\n        url,\n        parse_dates=[\"FL_DATE\"],\n        date_format=lambda x: pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\"),\n        dtype={col: str for col in time_cols},  # force them to string\n    )\n    .dropna(subset=time_cols)                   # drop rows missing any of the times\n)\n\n# 2) define a one-liner to zero-pad & parse “hhmm”\ndef hhmm_to_time(col):\n    return (\n        pd.to_datetime(\n            col.str.zfill(4),     # “1”→“0001”, “59”→“0059”, “1323”→“1323”\n            format=\"%H%M\",\n            errors=\"coerce\"       # invalid → NaT\n        )\n        .dt.time                  # extract python datetime.time\n    )\n\n# 3) apply it to all of them in one go\ndf[time_cols] = df[time_cols].apply(hhmm_to_time)"
  }
]