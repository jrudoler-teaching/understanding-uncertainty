[
  {
    "objectID": "slides/lecture-00-slides.html#welcome",
    "href": "slides/lecture-00-slides.html#welcome",
    "title": "Lecture 00",
    "section": "Welcome!",
    "text": "Welcome!\nSo begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#why-statistics",
    "href": "slides/lecture-00-slides.html#why-statistics",
    "title": "Lecture 00",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction."
  },
  {
    "objectID": "slides/lecture-00-slides.html#description",
    "href": "slides/lecture-00-slides.html#description",
    "title": "Lecture 00",
    "section": "Description",
    "text": "Description\n\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\n\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features.\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n3\n1002755\nNaN\n85098326012\nunconfirmed\nGarry\nbrooklyn\nClinton Hill\n40.68514\n-73.95976\nUnited States\n...\n$74\n30.0\n270.0\n7/5/2019\n4.64\n4.0\n1.0\n322.0\nNaN\nNaN\n\n\n4\n1003689\nEntire Apt: Spacious Studio/Loft by central park\n92037596077\nverified\nLyndon\nmanhattan\nEast Harlem\n40.79851\n-73.94399\nUnited States\n...\n$41\n10.0\n9.0\n11/19/2018\n0.10\n3.0\n1.0\n289.0\nPlease no smoking in the house, porch or on th...\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\nNow there’s a lot you can do, but let’s start by visualizing the prices of listings.\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\ncount    102316.000000\nmean        625.291665\nstd         331.677344\nmin          50.000000\n25%         340.000000\n50%         624.000000\n75%         913.000000\nmax        1200.000000\nName: price, dtype: float64\n\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the geopandas library to plot the locations of listings on a map of New York City.\n\n\n\n\n\n\n\n\n\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics only describe the data.\nWhy is this limiting? After all, we like data – it tells us things about the world and it’s objective and quantifiable.\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\nLet’s look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small “sample” or subset of the data?\n\n\n\n\n\n\n\n\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\n\n\n\n\n\n\nSample vs. Population\n\n\nA population is the entire set of data that you are interested in. A sample is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.\nNote that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.\nOften, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.\n\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more."
  },
  {
    "objectID": "slides/lecture-00-slides.html#inference",
    "href": "slides/lecture-00-slides.html#inference",
    "title": "Lecture 00",
    "section": "Inference",
    "text": "Inference\nSo what if we want to answer questions about a population based on a sample? This is where inference comes in. Specifically, we want to use the given sample to infer something about the population.\nHow do we do this if we can’t ever see the entire population? The answer is that we need a link which connects the sample to the population – specifically, we can explicitly treat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\n\nThere is always a DGP\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.\nThere has to be a DGP, even if we don’t know what it is. The DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\nThis is where the model comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess – a guess about where your data come from.\nFor example, we might assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. (So the probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.)\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\nThe inference question we want to ask is roughly:\n“If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\nNotice that this is a question about the probability of the sample, given a certain model of the DGP. In our Airbnb example above, it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\nWhat should we do now? Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. After all, the model is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n\n\n\n\n\n\nDon’t try this at home!\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. This requires domain knowledge and subject matter expertise. In the Airbnb example, you would want to know something about the five boroughs of New York City before starting your analysis. Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption (Manhattan sees vastly more tourism than the other boroughs, and Brooklyn and Queens have by far the most residents according to recent census data).\nStatistical inference is a powerful tool, but it is not a substitute for understanding the data and the context in which it was collected. Modeling might be guesswork, but it is best if it is informed guesswork."
  },
  {
    "objectID": "slides/lecture-00-slides.html#prediction",
    "href": "slides/lecture-00-slides.html#prediction",
    "title": "Lecture 00",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data, we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. We won’t go into the details of the model we use here, but the basic idea is that we assume the features of the listing (e.g., price) is related to the probability of it being in a certain borough (perhaps more expensive listings are more likely to be in Manhattan, for example).\n\n\n\n\n\n\n\nFitting a model\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nThe model for a coin flip, for example, has a single parameter: the probability of landing on heads. If you turn the knob to 0.5, you get a fair coin; if you turn it to 1.0, you get a coin that always lands on heads; if you turn it to 0.0, you get a coin that always lands on tails.\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data.\n\n\n\n\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================\n\n\n\n\n\n\n\n\n\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. For now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. For example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance.\n\n\n\n\nNow let’s take a look at the distribution of the model’s predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n\n\n\n\n\n\nPrediction and inference can interact\n\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP."
  },
  {
    "objectID": "slides/lecture-00-slides.html#summary",
    "href": "slides/lecture-00-slides.html#summary",
    "title": "Lecture 00",
    "section": "Summary",
    "text": "Summary\nIn this introductory lecture we talked about the 3 objectives of data analysis: description, inference, and prediction. Hopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. Don’t worry, we’ll get there!\nNext up, we’ll cover some of the basic programming concepts that are important for data science. After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. From there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learning any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments."
  },
  {
    "objectID": "notebooks/lecture-10.html",
    "href": "notebooks/lecture-10.html",
    "title": "Lecture 10: ANOVA",
    "section": "",
    "text": "We want to emphasize that nearly any statistical method with a closed-form solution can be replicated with a simulation.\nTake ANOVA, or analysis of variance, for example. The ANOVA test is used to determine whether there are any statistically significant differences between the means of two or more groups. It is a parametric test that assumes the data is normally distributed and that the variances of the groups are equal.\nThe exact test we compute is the F-test, which compares the variance between groups to the variance within groups. The null hypothesis is that all group means are equal, while the alternative hypothesis is that at least one group mean is different. The F-test relies on the fact that the ratio of two independent chi-squared variables (i.e. a normal variable, squared) follows an F-distribution.\nHere we will simulate the F-test by generating random data from a normal distribution and computing the F-statistic for different group means. We will see that under the null hypothesis (group means are equal), the F-statistic follows an F-distribution with degrees of freedom equal to the number of groups minus one and the total number of observations minus the number of groups.\nIn the next lecture we will compare the F-test to a non-parametric permutation test, which does not make any assumptions about the distribution of the data."
  },
  {
    "objectID": "notebooks/lecture-05.html",
    "href": "notebooks/lecture-05.html",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "",
    "text": "In the introductory lecture, we talked about the role of inference: drawing more general conclusions (about a population) from specific observations (a sample). Now that we have covered the most important building blocks, we can start to actually talk about how to make statistical inferences.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "href": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "A worked example: The NBA’s most valuable player (MVP)",
    "text": "A worked example: The NBA’s most valuable player (MVP)\n“Shai Gilgeous Alexander (SGA) is the best player in the NBA” is a very broad claim that is hard to test. What makes a player “the best”? Does stating that they scored the most points make them the best? That his team won the most games?\nLet’s try to instead design a hypothesis that is more specific and testable. We can choose a specific metric to evaluate players, such as points per game (PPG). You can care about other metrics, like assists or rebounds, but let’s just focus on PPG for now.\nTo make the claim more specific and testable, we’ll start off with some assumptions: - Every player’s scoring follows a distribution with some mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Becuase PPG is an average over the points scored in each game, the CLT tells us that the distribution of average PPG is approximately normal (even if “points scored” is not). Specifically, PPG is a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\), where \\(n\\) is the number of games played. - The player with the highest \\(\\mu\\) is the best scorer in the league (i.e. assume that teammates, coaches, conferences, etc. are all negligible).\nTo make the claim that SGA is a better scorer than his competitors, we need to show how unlikely it is that he has the highest scoring average if his skill level is not actually the highest in the league.\n\\(H_0\\): \\(\\mu_{\\text{SGA}} \\leq \\mu_{\\text{other}}\\)\n\\(H_1\\): \\(\\mu_{\\text{SGA}} &gt; \\mu_{\\text{other}}\\)\nAll of a sudden, this is way more familiar, specific, and testable. We can now use data to compute the \\(p\\)-value for this hypothesis test.\nShai scored 32.7 PPG in 76 games the 2024-2025 season, which is the highest average in the league. The next-highest scorer was Giannis Antetokounmpo with 30.4 PPG in 72 games.\nWe will use data from the 2024-2025 NBA season, courtesy of Basketball Reference, to evaluate our hypothesis.\n\n\nCode\n# read the data for Shai Gilgeous-Alexander and Giannis Antetokounmpo\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace({\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan})\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nshai_sample_mean = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].mean()\nshai_sample_std = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].std()\nprint(f\"Shai's PPG is {shai_sample_mean:.2f} with a standard deviation of {shai_sample_std:.2f}.\")\n\ngiannis_sample_mean = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].mean()\ngiannis_sample_std = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].std()\nprint(f\"Giannis's PPG is {giannis_sample_mean:.2f} with a standard deviation of {giannis_sample_std:.2f}.\")\n\n# plot the distribution of PPG for both players\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(data=compare_df, x=\"PTS\", hue=\"player\", stat=\"density\", common_norm=False, bins=20, ax=ax)\nax.axvline(shai_sample_mean, color=\"blue\", linestyle=\"--\", label=f\"Shai's PPG: {shai_sample_mean:.2f}\")\nax.axvline(giannis_sample_mean, color=\"orange\", linestyle=\"--\", label=f\"Giannis's PPG: {giannis_sample_mean:.2f}\")\nax.set_title(\"Distribution of Points Per Game (PPG) for Shai and Giannis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\n\n\nShai's PPG is 32.68 with a standard deviation of 7.54.\nGiannis's PPG is 30.39 with a standard deviation of 7.32.\n\n\nText(0.5, 0, 'Points Per Game (PPG)')\n\n\n\n\n\n\n\n\n\nNow what is the null distribution? Our null hypothesis is that SGA’s scoring ability is not higher than his competitors’. So let’s say that under the null, SGA’s inherent scoring ability is equal to Giannis’s, and both are equal to Giannis’ observed PPG of 30.4. That’s the mean of our normal distribution given by CLT. For the standard error, we can use the sample standard deviation of Shai’s points scored and divide it by the square root of the number of games he played (76).\nNow the question is: what is the probability of SGA scoring at least 32.7 PPG if his true scoring ability is actually 30.4 PPG?\nWe can just use the normal distribution to compute this probability!\n\n\n\n\n\n\nThe Cumulative Distribution Function (CDF)\n\n\n\n\n\nThe CDF of a normal distribution gives us the probability that a random variable is less than or equal to a certain value. In other words, it computes are under a probability density function (PDF) up to a certain point.\nTo compute the probability of a random variable being greater than a certain value, we can just subtract the CDF from 1. \\[\nP(X &gt; x) = 1 - P(X \\leq x) = 1 - \\text{CDF}(x) = 1 - \\int_{-\\infty}^{x} f(t) dt\n\\]\n\n\n\n\n\nCode\n## PDF of normal\nshai_n_games = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].count()\np_value_clt = 1 - stats.norm.cdf(\n    x=32.7, \n    loc=30.4, # the mean of the null distribution (Giannis's PPG)\n    scale=shai_sample_std/np.sqrt(shai_n_games)) \nprint(\"p-value from CLT:\", p_value_clt)\n# visualize the p-value\nx = np.linspace(27, 35, 1000)\nnull_pdf = stats.norm.pdf(x, loc=30.4, scale=shai_sample_std/np.sqrt(shai_n_games))\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x, null_pdf, label=r\"Normal Distribution given $H_0:$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: 32.7\")\nax.fill_between(x, null_pdf, where=(x &gt;= 32.7), alpha=0.4, label=\"Area for p-value\")\nax.set_title(\"PPG distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Probability Density\")\nax.legend()\nplt.show()\n\n\np-value from CLT: 0.00391019858958408\n\n\n\n\n\n\n\n\n\nThis analysis tells us that if Shai’s true scoring ability is 30.4 PPG, the probability of him scoring at least 32.7 PPG over 76 games is very small.\nWe can also try simulating the null distribution through individual game scores to see how likely it is that SGA would score at least 32.7 PPG if his true scoring ability is actually 30.4 PPG. That is, we’ll simulate 76 games of scoring, we each game is drawn from a distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai’s points scored. We’ll repeat this simulation many times to get a distribution of PPG scores under the null hypothesis.\n\n\nCode\nrng = np.random.default_rng(42)  # for reproducibility\nn_sims = 5000\nshai_simulated_ppg = []\nfor _ in range(n_sims):\n    # simulate 76 games of scoring, each game is drawn from a distribution with \n    # mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai's points scored\n    # Variance of continuous uniform distribution on [a, b] is (b-a)^2 / 12\n    low_minus_high = np.sqrt(12 * shai_sample_std**2)\n    simulated_scores = rng.uniform(low=30.4 - low_minus_high/2, high=30.4 + low_minus_high/2, size=76)\n    simulated_ppg = np.mean(simulated_scores)\n    shai_simulated_ppg.append(simulated_ppg)\n# compute the p-value from the simulated PPG scores\nshai_simulated_ppg = np.array(shai_simulated_ppg)\np_value_simulated = np.mean(shai_simulated_ppg &gt;= 32.7)\nprint(\"p-value from simulation:\", p_value_simulated)\n\n# plot the distribution of simulated PPG scores\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(shai_simulated_ppg, bins=30, stat=\"density\", ax=ax, label=\"Simulated PPG under $H_0$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: {32.7}\")\nax.set_title(\"Simulated PPG Distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Density\")\nax.legend()\nplt.show()\n\n\np-value from simulation: 0.004\n\n\n\n\n\n\n\n\n\nFor this second approach, we didn’t use the CLT at all! Instead, we relied on simulation to understand the distribution of Shai’s scoring under the null hypothesis. And yet, the results are almost identical to the ones we got using the normal approximation!\nCan you think of any limitations of the approach we took? What assumptions did we make that might not be right? Next lecture we’ll look at approaches that do not make any assumptions about the distribution of the data, and instead rely on resampling techniques to evaluate hypotheses.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#summary",
    "href": "notebooks/lecture-05.html#summary",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#footnotes",
    "href": "notebooks/lecture-05.html#footnotes",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfrom here, accessed July 2025↩︎\nThey survey around 50,000 housing units each month. See the BLS’s documentation for more information on the sample size and design of the Current Population Survey, which is used to estimate the unemployment rate.↩︎\nThis is often phrased as “under the assumption that the null hypothesis is true” or more succinctly “under the null”.↩︎",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html",
    "href": "notebooks/lecture-03.html",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "",
    "text": "Now that we have a good understanding of the basics of probability, we can start to explore how we deal with randomness computationally.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "href": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from probability distributions",
    "text": "Sampling from probability distributions\n\nA sample is a subset of data drawn from a more general population. That population can be thought of as a probability distribution – this distribution essentially describes how likely you are to observe different values when you sample from it.\nWe will quickly review some important concepts related to sampling.\n\nIndependent and identically distributed (IID) sampling\nWhen we sample from a probability distribution, we often assume that the samples are independent and identically distributed (IID). This means that each sample is drawn from the same distribution and that the samples do not influence each other.\nCoin flips are a good example of IID sampling. If you flip a fair coin multiple times, each flip has the same probability of being heads or tails (this is the “identically distributed” part), and the outcome of one flip does not affect the outcome of another (this is the “independent” part). The same is true for rolling a die!\nWe often apply this concept to more complex random processes as well, where we do not have such a clear understanding of the underlying process. For example, if we are sampling the heights of people in a city, we might assume that each person’s height is drawn from the same distribution (the distribution of heights in that city) and that one person’s height does not affect another’s. Whether or not the IID assumption holds in practice is an important question to consider when analyzing data – for example, do you think that the heights of people in a family are independent of each other?\n\n\nSampling with and without replacement\nAnother important concept in sampling is the distinction between sampling with replacement and sampling without replacement.\n\nSampling with replacement means that after we draw a sample from the population, we put it back before drawing the next sample. This means that the same object / instance can be selected multiple times.\nSampling without replacement means that once we draw a sample, we do not put it back before drawing the next sample. This means that each individual can only be selected once. This can introduce dependencies between samples, as the population changes after each draw.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#simulating-a-random-sample",
    "href": "notebooks/lecture-03.html#simulating-a-random-sample",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating a random sample",
    "text": "Simulating a random sample\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code.\n\n\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\nNormal distribution\n\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean).\n\n\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\nrng = np.random.default_rng(seed=42)  # Create a random number generator with a fixed seed\nsamples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\nplt.figure(figsize=(8, 5))\nplt.hist(samples, bins=10, density=True)\nplt.show()\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]\n\n\n\n\n\n\n\n\n\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\n# Sampling without replacement\nrng = np.random.default_rng(seed=42)\nsubsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n# another way to sample; note RNG can be different in different packages\npd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10, replace=False, random_state=42)\n\n\nSample:\n [-1.32269961 -1.13328721 -0.01680116 -1.68286977  0.54315427 -1.68733443\n  0.85797588 -0.85304393 -0.04992591 -0.36305385]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n83\n-0.381738\n\n\n53\n-0.319671\n\n\n70\n-1.275686\n\n\n45\n0.218689\n\n\n44\n0.116686\n\n\n39\n0.650593\n\n\n22\n1.222541\n\n\n80\n0.456775\n\n\n10\n0.879398\n\n\n0\n0.304717\n\n\n\n\n\n\n\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\npossible_rolls = [1, 2, 3, 4, 5, 6]\nprobabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\nrng.choice(possible_rolls, p=probabilities, size=100)\n\n\narray([4, 6, 6, 6, 5, 3, 6, 1, 6, 6, 6, 4, 6, 6, 6, 2, 5, 1, 2, 6, 6, 6,\n       4, 4, 5, 2, 2, 5, 3, 6, 5, 6, 6, 4, 6, 6, 4, 3, 6, 2, 2, 1, 6, 6,\n       6, 6, 5, 6, 2, 2, 6, 5, 6, 6, 6, 6, 6, 4, 1, 5, 3, 5, 6, 3, 1, 3,\n       3, 6, 6, 6, 6, 5, 6, 2, 1, 1, 6, 5, 2, 6, 2, 6, 5, 4, 4, 6, 4, 1,\n       2, 6, 6, 6, 3, 6, 6, 6, 5, 3, 1, 6])\n\n\n\nSimulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nConsider dealing poker hands from a standard deck of cards. When you deal a hand, you draw cards one at a time, and each card drawn affects the next card that can be drawn (because you do not put the card back into the deck).\n\n\nCode\n# Make a deck of cards (Ace is 1, King is 13)\ndeck = np.arange(1, 14).repeat(4)  # 4 suits, each with cards 1 to 13\nprint(\"Deck of cards before shuffling:\\n\", deck)\ndeck = np.random.permutation(deck)\nprint(\"Deck of cards after shuffling:\\n\", deck)\n# deal 2 cards to each of 4 players\nrng = np.random.default_rng(seed=21)\n# Get flat indices of 8 cards from deck\n# we want to know exactly which cards are dealt\nchosen_indices = rng.choice(len(deck), size=8, replace=False)\nhands = deck[chosen_indices].reshape(4, 2)\nprint(\"Hands dealt to players:\\n\", hands)\n# remove the dealt cards from the deck\nremaining_deck = np.delete(deck, chosen_indices)\nprint(\"Remaining cards in the deck:\\n\", remaining_deck)\nboard = np.random.choice(remaining_deck, size=5, replace=False)\nprint(\"Community cards on the board:\\n\", board)\n\n\nDeck of cards before shuffling:\n [ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12\n 13 13 13 13]\nDeck of cards after shuffling:\n [ 3  7 11  8 13  9  1  6  1 12  3  6  9  5 10  9  9  2 10  7  2 11  4  8\n  4 12  6 10  2 12  7  6  3 13  8  4  5  2  3 13  4  1  5  5  8 11 13 11\n 12 10  7  1]\nHands dealt to players:\n [[10 13]\n [ 4  5]\n [ 2 12]\n [ 4 10]]\nRemaining cards in the deck:\n [ 3  7 11  8  9  1  6  1 12  3  6  9 10  9  9  7  2 11  8  4 12  6 10  2\n  7  6  3 13  8  5  2  3 13  4  1  5  5  8 11 13 11 12  7  1]\nCommunity cards on the board:\n [ 2  7 11  7  8]\n\n\nBut it can get even more complex than that. In many real-world scenarios, the process of generating data involves multiple steps or conditions that affect the outcome.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code). We then tend to write loops that simulate the process step by step, keeping track of the state of things as we go along.\nLet’s consider an example of a musician busking for money in Rittenhouse Square. The musician’s earnings might depend on various factors like the weather and and the number of passersby. To keep it simple, let’s assume that the musician earns $3 for every passerby who stops to listen. Of course, not every passerby will stop – let’s pretend every passerby has the same 20% chance of stopping.\nThe musician might want to know how much money they can expect to earn in a day of busking. We can simulate this process by generating a random number of passersby and then calculating the earnings based on the stopping probability.\n\n\n\n\n\n\nPoisson distribution\n\n\n\nThe Poisson distribution is commonly used to model the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It assumes that the events occur independently and at a constant average rate. In our example, we can use the Poisson distribution to model the number of passersby in a given time period (e.g., one hour of busking).\n\n\n\n\nCode\nn_days = 5\n# simulate whether it rains each day\nrng = np.random.default_rng(seed=42)\nrain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\ntotal_earnings = 0 # initialize a variable to keep track of total earnings\nfor day in range(n_days):\n    # For each day, decide if it rains based on the probability\n    did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n    print(f\"Day {day + 1} ({rain_probabilities[day]:.2%} chance): {'Rain' if did_it_rain else 'No rain'}\")\n    # Based on the outcome, the number of passersby changes\n    if did_it_rain:\n        passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n    else:\n        passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n    print(f\"\\t Number of passersby: {passersby}\")\n    # Simulate the number who stop to listen to the busker\n    listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n    print(f\"\\t Number of listeners: {listeners}\")\n    # Compute the busker's daily earnings\n    earnings = 3 * listeners  # $3 per listener\n    print(f\"\\t Daily earnings: ${earnings}\")\n    print(\"-\" * 40)\n\n    total_earnings += earnings\n\nprint(f\"Total earnings over {n_days} days: ${total_earnings}\")\n\n\nDay 1 (54.18% chance): No rain\n     Number of passersby: 211\n     Number of listeners: 41\n     Daily earnings: $123\n----------------------------------------\nDay 2 (30.72% chance): No rain\n     Number of passersby: 226\n     Number of listeners: 54\n     Daily earnings: $162\n----------------------------------------\nDay 3 (60.10% chance): Rain\n     Number of passersby: 51\n     Number of listeners: 13\n     Daily earnings: $39\n----------------------------------------\nDay 4 (48.82% chance): Rain\n     Number of passersby: 56\n     Number of listeners: 17\n     Daily earnings: $51\n----------------------------------------\nDay 5 (6.59% chance): No rain\n     Number of passersby: 212\n     Number of listeners: 34\n     Daily earnings: $102\n----------------------------------------\nTotal earnings over 5 days: $477\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe key here is to re-use the logic of the weekly busking simulation in a loop that runs 1000 times. Each time we run the simulation, we get a different weekly outcome based on the random number generator. By averaging these outcomes, we can get a good estimate of the expected earnings over a week of busking.\nimport numpy as np\ndef busk_one_week(rng, n_days=7):\n    \"\"\"Simulate the earnings of a busker in Rittenhouse Square over the course of a week.\n    Args:\n        rng: A NumPy random number generator.\n        n_days: The number of days to simulate (default is 7).\n    Returns:\n        total_earnings: The total earnings over the week.\n    \"\"\"\n    rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\n    total_earnings = 0 # initialize a variable to keep track of total earnings\n    for day in range(n_days):\n        # For each day, decide if it rains based on the probability\n        did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n        # Based on the outcome, the number of passersby changes\n        if did_it_rain:\n            passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n        else:\n            passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n        # Simulate the number who stop to listen to the busker\n        listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n        # Compute the busker's daily earnings\n        earnings = 3 * listeners  # $3 per listener\n\n        total_earnings += earnings\n\n    return total_earnings\n\nn_simulations = 1000\nrandom_seed = 33\n\nrng = np.random.default_rng(seed=random_seed)\n\n# Use the above function to simulate the expected earnings of a busker in Rittenhouse Square over the course of a week.\n# Run the simulation 1000 times and calculate the average earnings over 1000 simulations.\nearnings_by_week = [busk_one_week(rng) for _ in range(n_simulations)]\naverage_earnings = np.mean(earnings_by_week)\naverage_earnings",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html",
    "href": "notebooks/lecture-01.html",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "",
    "text": "This lecture will be, intentionally, a bit of a whirlwind. That’s because with the advent of large language models (LLMs) like ChatGPT, Claude, Gemini, etc. knowing how to program in specific languages like Python is becoming less important. You don’t need that much practice or to focus on the syntax of a specific language.\nInstead, the important thing is to understand the core concepts involved in programming, which are largely universal across languages. This high-level understanding will allow you to use LLMs effectively to write code in any language, including Python. If you don’t understand the concepts, you won’t be able to identify when the LLM is making mistakes or producing suboptimal code.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#variables-and-types",
    "href": "notebooks/lecture-01.html#variables-and-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables and types",
    "text": "Variables and types\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\n\n\n\n\n\n\nFunctions act on variables\n\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable.\n\n\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations.\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean\nTrue or False values, e.g., True, False\n\n\n\nLet’s discuss a few important ones in more detail\n\n\n\n\n\n\nEverything is an object\n\n\n\nIn Python, everything is an object. This means that even basic data types like integers and strings are treated as objects with methods and properties. For example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\nSee the later section on Object-Oriented Programming for more details.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#lists",
    "href": "notebooks/lecture-01.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\nWe often need to store multiple values together. The most basic way to achieve this is with a list. A list is an ordered collection of items that can be of any type, including other lists. “Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\nprint(my_list[0])  # Output: 1\n\n1\n\n\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\n[1, 2, 3]\n['apple', 'banana']\n[1, 3, 'banana']\n['banana', 'apple', 3, 2, 1]\n\n\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#arrays-numpy",
    "href": "notebooks/lecture-01.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\nPackages\n\n\n\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array().\n\n\n\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]\n\n\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]\n\n\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\n\nAdvanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\n\nCode\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]\n\n\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [ 1  2  3  4  5  6  7  8  9 10]\nBoolean mask: [False False  True  True  True  True  True  True  True  True]\nFiltered array: [ 3  4  5  6  7  8  9 10]",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dictionaries",
    "href": "notebooks/lecture-01.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}\n\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25\n\n\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dataframes",
    "href": "notebooks/lecture-01.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns). Think of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago\n\n\n\n\n\n\n\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#conditional-logic",
    "href": "notebooks/lecture-01.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false.\n\n\n\nHere’s an example of how to use these statements. Play around with the code below to see how it works. You can change the value of age to see how the output changes based on different conditions.\n\n\n\n\n\n\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false.\n\n\n\n\n\n\nBoolean expressions\n\n\n\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#loops",
    "href": "notebooks/lecture-01.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops.\n\nFor Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one.\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4. The enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n\n\n\n\nWhile Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#functions-and-functional-programming",
    "href": "notebooks/lecture-01.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result). You can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters. Here’s an example:\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\nProbably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]\n\n\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#object-oriented-programming",
    "href": "notebooks/lecture-01.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and \\ behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created.\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse\n\n\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality.\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\n\n\n\n\n\n\n\n\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#summary",
    "href": "notebooks/lecture-01.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#footnotes",
    "href": "notebooks/lecture-01.html#footnotes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing.↩︎",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: Understanding Uncertainty: An Introduction to Statistics through Data Generating Processes and Computational Simulations\nInstructor: Joey Rudoler\nClassroom: Academic Research Building (ARB), Room 401\nCourse Schedule:\n\nTentative schedule, subject to change.\n\n\nDate\nTopic\nAssignment\n\n\n\n\n7/09\nIntro, Why Statistics?\n\n\n\n7/10\nProgramming\n\n\n\n7/11\nProgramming cont.\n\n\n\n7/14\nProbability\n\n\n\n7/15\nProbability cont.\n\n\n\n7/16\nSampling and simulation\n\n\n\n7/17\nData Generating Processes and Statistical Models\n\n\n\n7/18\nHypothesis Testing\n\n\n\n7/21\nHypothesis Testing cont.\n\n\n\n7/22\nConfidence Intervals, bootstrapping\n\n\n\n7/23\nPermutation tests\n\n\n\n7/24\nRegression\n\n\n\n7/25\nRegression cont.\n\n\n\n7/28\nClassification\n\n\n\n7/29\nCrash course: Deep Learning and AI\n\n\n\n7/30\nAdvanced Data Visualization\n\n\n\n7/31\nParadoxes and Statistical Gotchas\n\n\n\n8/01\nGuest Lecture / Final Projects\n\n\n\n8/04\nGuest Lecture / Final Projects\n\n\n\n8/05\nGuest Lecture / Final Projects\n\n\n\n8/06\nGuest Lecture / Final Projects\n\n\n\n8/07\nFinal Project Presentations\n\n\n\n8/08\nWrap-up and course evaluation"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nDescription\nDue Date\n\n\n\n\nAssignment 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "understanding-uncertainty",
    "section": "",
    "text": "This website contains course materials for the Understanding Uncertainty course, which is a part of the US-Israel Academic Bridge Fellowship hosted at the University of Pennsylvania.\n\n\n\nSomething from DALL-E’s imagination"
  },
  {
    "objectID": "notebooks/lecture-00.html",
    "href": "notebooks/lecture-00.html",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#welcome",
    "href": "notebooks/lecture-00.html#welcome",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#why-statistics",
    "href": "notebooks/lecture-00.html#why-statistics",
    "title": "Lecture 00",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#description",
    "href": "notebooks/lecture-00.html#description",
    "title": "Lecture 00",
    "section": "Description",
    "text": "Description\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features.\n\n\nCode\n# from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.rename(columns={\"neighbourhood_group\": \"borough\"})\nairbnb = airbnb.dropna(subset=[\"borough\", \"price\", \"long\", \"lat\"])\nairbnb[\"borough\"] = airbnb[\"borough\"].str.lower()\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"manhatan\", \"manhattan\")\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"brookln\", \"brooklyn\")\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n# print the first 5 rows\nairbnb[:5]\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n3\n1002755\nNaN\n85098326012\nunconfirmed\nGarry\nbrooklyn\nClinton Hill\n40.68514\n-73.95976\nUnited States\n...\n$74\n30.0\n270.0\n7/5/2019\n4.64\n4.0\n1.0\n322.0\nNaN\nNaN\n\n\n4\n1003689\nEntire Apt: Spacious Studio/Loft by central park\n92037596077\nverified\nLyndon\nmanhattan\nEast Harlem\n40.79851\n-73.94399\nUnited States\n...\n$41\n10.0\n9.0\n11/19/2018\n0.10\n3.0\n1.0\n289.0\nPlease no smoking in the house, porch or on th...\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\nNow there’s a lot you can do, but let’s start by visualizing the prices of listings.\n\n\nCode\n# plot a histogram of the price column\nplt.figure(figsize=(10, 5))\nplt.hist(airbnb['price'], bins=50)\nplt.title('Prices of Airbnb Listings in NYC')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\nCode\nairbnb[\"price\"].describe()\n\n\ncount    102316.000000\nmean        625.291665\nstd         331.677344\nmin          50.000000\n25%         340.000000\n50%         624.000000\n75%         913.000000\nmax        1200.000000\nName: price, dtype: float64\n\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the geopandas library to plot the locations of listings on a map of New York City.\n\n\nCode\nimport geopandas as gpd\nfrom geodatasets import get_path\n# load the shapefile of NYC neighborhoods\nnyc_neighborhoods = gpd.read_file(get_path('nybb'))\nnyc_neighborhoods = nyc_neighborhoods.to_crs(epsg=4326)  # convert to WGS84\n# plot the neighborhoods with airbnb listings\nnyc_neighborhoods.plot(figsize=(8, 8), color='white', edgecolor='black')\n# plot the airbnb listings on top of the neighborhoods\n# use the 'long' and 'lat' columns to create a GeoDataFrame\nairbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb['long'], airbnb['lat']), crs='EPSG:4326')\n# set the coordinate reference system to WGS84\nairbnb_gdf.plot(ax=plt.gca(), column=\"price\", markersize=3, alpha=0.05, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\nplt.title('Airbnb Listings in NYC')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics only describe the data.\nWhy is this limiting? After all, we like data – it tells us things about the world and it’s objective and quantifiable.\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\nLet’s look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small “sample” or subset of the data?\n\n\nCode\n# separately plot 3 samples of airbnb listings\nfig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(4):\n    # sample 1000 listings\n    sample = airbnb.sample(100, random_state=i)\n    # plot the neighborhoods with airbnb listings\n    nyc_neighborhoods.plot(ax=ax[i], color='white', edgecolor='black')\n    # plot the airbnb listings on top of the neighborhoods\n    # use the 'long' and 'lat' columns to create a GeoDataFrame\n    airbnb_gdf_sample = gpd.GeoDataFrame(sample, geometry=gpd.points_from_xy(sample['long'], sample['lat']), crs='EPSG:4326')\n    # set the coordinate reference system to WGS84\n    airbnb_gdf_sample.plot(ax=ax[i], column=\"price\", markersize=3, alpha=0.8, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\n    ax[i].set_title(f'Airbnb Listings in NYC (Sample {i+1})\\n Average Price: ${sample[\"price\"].mean():.2f}')\n    ax[i].set_xlabel('Longitude')\n    ax[i].set_ylabel('Latitude')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\n\n\n\n\n\nSample vs. Population\n\n\n\n\n\nA population is the entire set of data that you are interested in. A sample is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.\nNote that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.\nOften, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#inference",
    "href": "notebooks/lecture-00.html#inference",
    "title": "Lecture 00",
    "section": "Inference",
    "text": "Inference\nSo what if we want to answer questions about a population based on a sample? This is where inference comes in. Specifically, we want to use the given sample to infer something about the population.\nHow do we do this if we can’t ever see the entire population? The answer is that we need a link which connects the sample to the population – specifically, we can explicitly treat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\nThere is always a DGP\n\n\n\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.\nThere has to be a DGP, even if we don’t know what it is. The DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\nThis is where the model comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess – a guess about where your data come from.\nFor example, we might assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. (So the probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.)\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\n\nCode\n# number of listings per borough\nborough_counts = airbnb['borough'].value_counts().rename_axis('borough').reset_index(name='count')\nborough_counts['borough'] = borough_counts['borough'].str.title()  # capitalize\n# normalize the counts to proportions\nborough_counts['proportion'] = borough_counts['count'] / borough_counts['count'].sum()\n\nplt.figure(figsize=(8, 5))\nplt.bar(borough_counts['borough'], borough_counts['proportion'])\nplt.axhline(y=1/5, color='r', linestyle='--', label='Equal Proportion (20%)')\nplt.title('Proportion of Airbnb Listings per Borough')\nplt.xlabel('Borough', fontsize=14)\nplt.ylabel('Proportion', fontsize=14)\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe inference question we want to ask is roughly:\n“If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\nNotice that this is a question about the probability of the sample, given a certain model of the DGP. In our Airbnb example above, it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\nWhat should we do now? Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. After all, the model is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\n\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n\n\n\n\n\nDon’t try this at home!\n\n\n\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. This requires domain knowledge and subject matter expertise. In the Airbnb example, you would want to know something about the five boroughs of New York City before starting your analysis. Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption (Manhattan sees vastly more tourism than the other boroughs, and Brooklyn and Queens have by far the most residents according to recent census data).\nStatistical inference is a powerful tool, but it is not a substitute for understanding the data and the context in which it was collected. Modeling might be guesswork, but it is best if it is informed guesswork.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#prediction",
    "href": "notebooks/lecture-00.html#prediction",
    "title": "Lecture 00",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data, we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. We won’t go into the details of the model we use here, but the basic idea is that we assume the features of the listing (e.g., price) is related to the probability of it being in a certain borough (perhaps more expensive listings are more likely to be in Manhattan, for example).\n\n\n\n\n\n\nFitting a model\n\n\n\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nThe model for a coin flip, for example, has a single parameter: the probability of landing on heads. If you turn the knob to 0.5, you get a fair coin; if you turn it to 1.0, you get a coin that always lands on heads; if you turn it to 0.0, you get a coin that always lands on tails.\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data.\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport seaborn as sns\n\n# prepare the data for linear regression\nfeature_columns = [\"price\", \"room_type\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"review_rate_number\"]\nairbnb_clean = airbnb.dropna(subset=feature_columns + [\"borough\"])\nX = airbnb_clean[feature_columns]\n# convert categorical variables to lowercase\nX.loc[:, \"room_type\"] = X[\"room_type\"].str.lower()\ny = airbnb_clean[\"borough\"].values.reshape(-1)\n# convert categorical variables to dummy variables\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\nX[\"rating_interaction\"] = X[\"reviews_per_month\"] * X[\"review_rate_number\"]\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n# fit the logistic regression model\nmodel = LogisticRegression(max_iter=1000, solver=\"newton-cholesky\")\nmodel.fit(X_train, y_train)\n# make predictions on the test set\ny_pred = model.predict(X_test)\n# calculate the prediction accuracy\naccuracy = np.mean(y_pred == y_test)\nprint(f\"{40*'='}\")\nprint(f'Prediction Accuracy: {accuracy:.2%}')\nprint(f\"{40*'='}\")\n# calculate the confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=airbnb['borough'].unique())\n# confusion_matrix = [confusion_matrix[i].T for i in range(len(confusion_matrix))] \nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\nax = ax.flatten()\nax[-1].axis('off')  # turn off the last subplot since we have only 5 boroughs\n# plot the confusion matrix for each borough\nfor i, borough in enumerate(airbnb['borough'].unique()):\n    sns.heatmap(confusion_matrix[i], annot=True, fmt='d', cmap='Blues', ax=ax[i], cbar=False)\n    ax[i].set_xlabel('Predicted', fontsize=10)\n    ax[i].set_ylabel('Actual', fontsize=10)\n    ax[i].set_xticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\n    ax[i].set_yticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\nplt.tight_layout()\n_ = plt.show()\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================\n\n\n\n\n\n\n\n\n\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\n\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. For now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. For example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance.\n\n\n\nNow let’s take a look at the distribution of the model’s predictions.\n\n\nCode\n# plot the counts of predicted listings per borough side by side with the actual counts\nimport seaborn as sns\nplot_df = pd.DataFrame({\n    'predicted': pd.Series(y_pred.flatten()),\n    'actual': pd.Series(y_test.flatten())\n})\nplot_df = plot_df.melt(var_name='type', value_name='borough')\nfig = plt.figure(figsize=(10, 6))\nax = sns.catplot(x='borough', hue='type', data=plot_df, kind='count', height=5, aspect=1.2, alpha=0.7)\nsns.move_legend(ax, loc=\"center right\", title=None)\nplt.title('Predicted vs Actual Airbnb Listings per Borough')\nplt.xlabel('Borough')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 3000x1800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n\n\n\n\n\nPrediction and inference can interact\n\n\n\n\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#summary",
    "href": "notebooks/lecture-00.html#summary",
    "title": "Lecture 00",
    "section": "Summary",
    "text": "Summary\nIn this introductory lecture we talked about the 3 objectives of data analysis: description, inference, and prediction. Hopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. Don’t worry, we’ll get there!\nNext up, we’ll cover some of the basic programming concepts that are important for data science. After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. From there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learning any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html",
    "href": "notebooks/lecture-02.html",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathbb{P}}\n\\renewcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n\\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability",
    "href": "notebooks/lecture-02.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\\]\nBy definition this quantity cannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and it must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs).\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)). If we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say “there’s a 100% chance that something will happen”.\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events \\(H\\) and \\(T\\) cover 100% of the possible outcomes. So we can write: \\[\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\\]\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event \\(A\\), the event either happens or it doesn’t. So we can compute the probability of the event not occurring as: \\[\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\\]\n\nProbability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\).\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows: \\[\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\\]\n\n\nAddition and multiplication rules (and / or)\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that \\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as \\(B\\) above, so we can use the same calculation: \\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\).\n\n\n\n\n\n\nNote on notation\n\n\n\n\n\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively. The notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively. So we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads. Technically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs).\n\n\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events \\(A\\) and \\(B\\), the following rules hold:\n\nAddition rule: For any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\n\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram. \n\n\n\n\nMultiplication rule: For any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset.\n\n\n\n\n\n\n\nConditional probability\n\n\n\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”. The conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day.\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\nSo the conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\\]\n\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”\n\n\n\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n\nIndependence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\nHow does this relate to the multiplication rule? If \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\). That is, knowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply \\(\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\).\n\n\nComplicated counting\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes.\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let’s think about the case where the first flip is heads. The probability of getting a head on the first flip is \\(\\frac{1}{2}\\), and the probability of getting tails on the 4 other flips is also \\(\\frac{1}{2}\\) each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips: \\[\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}\\]\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting “exactly one head”. For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions: \\[\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16\\]\nThere are two common types of outcomes we want to count: permutations and combinations.\nA combination is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are choosing an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\\[\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nIt is clear here that there are 5 possible “slots” where we can place a head.\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still applies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\nLet’s take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices. \\[\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nNow let’s consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\\[\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n\\]\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads. This gives us a total of \\(5 \\cdot 4 = 20\\) ways to choose 2 flips to be heads out of 5 flips.\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order – where flip 1 was heads first).\nThis illustrates the key distinction between combinations and permutations. A permutation is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. If you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting.\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign “slots” in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length \\(n\\), the number of permutations is: \\[\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n\\]\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\nNamely, the number of combinations of \\(k\\) items from a set of \\(n\\) items is given by the formula: \\[\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n\\]\nThis formula counts all of the possible permutations of the sequence, and then divides by the number of ways to arrange the \\(k\\) items that are selected (which is \\(k!\\)) and the number of ways to arrange the remaining \\(n-k\\) items (which is \\((n-k)!\\)).\nIn our example, we have \\(n = 5\\) (the number of flips) and \\(k = 2\\) (the number of heads). So the number of combinations of 2 heads from 5 flips is: \\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-functions",
    "href": "notebooks/lecture-02.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\nHowever, it is often more convenient to work with probability functions. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails).\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\n\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values.\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n\n\n\n\n\n\n\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\nCode\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#random-variables",
    "href": "notebooks/lecture-02.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\).\nWe use probability functions to describe the probabilities associated with random variables. Specifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nBernoulli random variable\n\n\n\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin.\n\n\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUniform random variable\n\n\n\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\).\n\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "href": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nWe call the probability function for a random variable a probability distribution, which describes how the probabilities are distributed across the possible values of the random variable.\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point.\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. We can use a bar chart to visualize the counts of each value.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.figure(figsize=(8, 5))\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\", rwidth=0.8)\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let’s check it out:\n\n\nCode\n# load in the dice rolls data\ndice_rolls_df = pd.read_csv(\"../data/dice_rolls.csv\")\nprint(\"Total number of dice rolls:\", len(dice_rolls_df))\ndice_rolls_df.head(10)\n\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.hist(dice_rolls_df['rolls'], bins=np.arange(0.5, 7.5, 1), density=True, rwidth=0.8)\nplt.title('Histogram of Dice Rolls')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.xticks(np.arange(1, 7))\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that when the \\(y\\)-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a histogram to visualize the distribution of the data.\nThe histogram is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins.\n\n\nCode\ndf = pd.read_csv(\"../data/airbnb.csv\")\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.dropna(subset=[\"price\"])\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb['price'], bins=50, stat=\"proportion\", edgecolor='black')\nplt.title('Histogram of Airbnb Prices')\nplt.xlabel('Price (USD)')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea under a probability distribution\n\n\n\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. This is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1.\nFor discrete: \\[ \\sum_{x} f(x) = 1 \\] For continuous: \\[ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\]\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable \\(X\\) falls between \\(a\\) and \\(b\\), we can compute the area under the PDF from \\(a\\) to \\(b\\): \\[ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx \\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#expectation",
    "href": "notebooks/lecture-02.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play.\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you’re not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero.\n\n\n\n\n\n\nGambling warning: the house always wins\n\n\n\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n\n\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\mathbb{E}[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\nSo in our roulette example, you can either lose $1 (with 90% probability) or win $9 (with 10% probability). The expectation would be: \\[\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win $10 on one game and lose $1 on the other 9 games. The average outcome is: \\[\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as \\(\\bar{X}\\), and referred to as the sample mean. \\[\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nThis approximation becomes more accurate as the number of samples \\(n\\) increases. We will talk about this more in a future lecture.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "href": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe average is a useful summary of a random variable’s central tendency, but it does not tell us anything about how spread out the values are.\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game – the average amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\n\n\n\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can’t afford to lose all of it on a single game.\nWe need a statistic that captures the typical distance between the values of the random variable and the average value.\n\n\n\n\n\n\nWhy distance from the average?\n\n\n\n\n\nLet’s imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what – the only question is how much you win. Let’s take an example where the payouts still differ by $10: you get $5 if you “lose” and $15 if you “win”.\nIn this case, the expected value is: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n\\] So you can expect to win $6 per game on average.\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\nThe answer is simple: we subtract the average from each value of \\(X\\): \\[X' = X - \\mathbb{E}[X]\\] This gives us a new random variable \\(X'\\) that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\\[\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0\\]\nor more generally: \\[\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0\\]\n\n\n\nSo let’s compute exactly that - the distance from the average. The formula for distance between two vectors \\(x\\) and \\(y\\) is: \\[\nd^2 = \\sum_{i} (x_i - y_i)^2\n\\] where \\(x_i\\) and \\(y_i\\) are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange (\\(a^2 + b^2 = c^2\\)).\nIn our case, we want to compute the distance between the values of the random variable \\(X\\) and the average value \\(\\mathbb{E}[X]\\). \\[\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n\\]\nNow we’re getting somewhere! However, this is adding up all of the squared distances – that means that the more values we have, the larger the distance will be. This is not quite right – instead we want to compute the average distance from the mean in order to get a sense of how spread out the values typically are.\nSo we need to divide by the number of values: \\[\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n\\]\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. If we replace the average with the expectation, we get the formula for the variance of a random variable \\(X\\): \\[\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n\\]\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if \\(X\\) represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the standard deviation:\n\\[\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n\\]\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset: \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\]\n\n\n\n\n\n\nSample variance vs. population variance\n\n\n\n\n\nTechnically, the formula above is an imperfect estimate of the population standard deviation. It’s in general a little bit too small, because the sample mean \\(\\bar{X}\\) does not perfectly represent the population mean \\(\\mathbb{E}[X]\\). We can correct for this by dividing by \\(n-1\\) instead of \\(n\\): \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\] This is called the sample standard deviation.\nWhy is the initial estimate too small? In a small dataset, the sample mean “overfits” the data, meaning it is closer to the individual data points than the true population mean. Let’s think about this in terms of coin flips. If we flip a coin once, the sample mean is either \\(\\hat{X}=0\\) or 1, depending on whether we got heads or tails. But the true population mean is \\(\\mathbb{E}[X]=\\frac{1}{2}\\). If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either \\((1-1)^2\\) or \\((0-0)^2\\)).\nBy contrast, the true (population) standard deviation is \\(\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\\), which is larger than the estimate using the sample mean.\nThis bias in computing the standard deviation gets smaller as the sample size \\(n\\) increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the \\(n-1\\) correction to get a more accurate estimate of the population standard deviation.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#summary",
    "href": "notebooks/lecture-02.html#summary",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. Probability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\nIn particular, we covered:\n\nThe basic definition of probability and how to compute it for simple events.\nThe addition and multiplication rules for calculating probabilities of multiple events.\nThe concept of independence and how it affects probabilities.\nRandom variables and their probability distributions\nThe expectation (or expected value) of a random variable\nVariance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html",
    "href": "notebooks/lecture-04.html",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#data-generating-processes",
    "href": "notebooks/lecture-04.html#data-generating-processes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#statistical-models",
    "href": "notebooks/lecture-04.html#statistical-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\nLet’s return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a Bernoulli distribution, which describes the probability of each outcome. Specifically,\n\\[P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}\\]\nwhere (X) is the outcome of the coin flip, (p) is the probability of heads, and (1 - p) is the probability of tails. If we assume a fair coin, then (p = 0.5).\nNow clearly, this model does not capture the complexity of a real-world coin flip, which is of course influenced by many factors such as the weight of the coin, the force of the flip, air resistance, etc. Statistical models are always reductive in this sense. But the important thing is that a Bernoulli distribution really does do a good job of describing the outcomes of a coin flip. As long as that is the case, we can use the model to make predictions about the data generated by the process.\nTo see this, consider the following process: you roll a fair die 100 times. For each roll, you record whether the number you rolled was even or odd. Since there are three even numbers (2, 4, 6) and three odd numbers (1, 3, 5), and each number has an equal probability of being rolled, the probability of rolling an even number is 0.5 and the probability of rolling an odd number is also 0.5. This is statistically identical to flipping a fair coin! So we can use the same Bernoulli distribution to model the outcomes of this process, even though it completely ignore the details of the die rolling process itself. We lose information about the specific numbers rolled, but as long as we accurately capture the probability of the outcomes we care about, it doesn’t matter.\n\n\n\nCode\ndef dice_even_odd(n=1000):\n    \"\"\"\n    Simulate rolling a die n times and return the results.\n    \n    Parameters:\n    n (int): Number of rolls to simulate.\n    \n    Returns:\n    pd.DataFrame: DataFrame with columns 'roll' and 'even_odd'.\n    \"\"\"\n    rolls = np.random.randint(1, 7, size=n)\n    is_even = rolls % 2 == 0\n    return pd.DataFrame({\"roll\": rolls, \"is_even\": is_even, \"even_odd\": np.where(is_even, \"even\", \"odd\")})\n\ndice_rolls = dice_even_odd(1000)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nsns.histplot(dice_rolls[\"roll\"], bins=np.arange(1, 8) - 0.5, discrete=True, ax=ax[0])\nax[0].set_xlabel(\"Die Roll\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Die Rolls\")\nax[0].set_xticks(np.arange(1, 7))\nax[0].grid(axis='y')\n\nsns.histplot(dice_rolls[\"even_odd\"], discrete=True, shrink=0.8, ax=ax[1])\nax[1].set_xlabel(\"Even or Odd\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Even and Odd Rolls\")\nax[1].grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\n\nStatistical modeling generally uses this kind of trick. We attempt to find a simple model that captures the essential features of the data generating process.\nWe don’t always know as much about the data generating process as we do for a coin flip or a die roll. In fact, often we don’t know anything about it at all! In these cases, the approach is a little more “guess and check”. We start with a simple model, and see how well it describes the data. If it doesn’t work, we try a more complex model or a different model altogether.\n\nChallenges with finite samples\nIt can be hard to tell if you have a good model or not. With any finite dataset, it is quite likely that the data will not perfectly match the model. This is because the model describes probabilistic tendencies of the data. But in any small sample we will likely see deviations from the idealized outcomes described by the model.\nThink about this in the extreme: if you flip a coin only once, you will either get heads or tails. So your observed proportion of heads will be either 0 or 1, which is very different from the expected proportion of 0.5. In fact it is impossible to observe the expected proportion of heads in a single flip! This is true for a single observation of basically any random variable that takes on more than one value – it is close to impossible to learn anything about the variable’s tendencies from just one observation.\nEven with a small number of observations, it is quite difficult to tell if a model is a good fit for the data.\nLet’s say you and your roommate are always arguing about who should take out the trash. You decide to flip a coin to decide who takes it out (you are notorious for always choosing tails, because you think it’s good luck). You decide to do a best-of-ten series, so you flip the coin 10 times and record the number of heads. It turns out that you get 3 heads and 7 tails. Your roommate is furious: “That’s not fair! You always get tails! I bet you rigged the coin!”\nIs your roommate justified in being suspicious? What is the probability of getting 3 heads in 10 flips of a fair coin? What is the probability of getting 3 heads in 10 flips of a biased coin that has a 25% chance of landing on heads?\n\n\n\n\n\n\n\n\nAs this exercise shows, with a small number of observations you have some information that can help you distinguish between the two models, but it is not enough to be confident in your conclusion.\nWith more data though, suddenly the evidence becomes much stronger. The probability of getting a badly imbalanced distribution of heads and tails becomes tiny as the number of flips increases.\nIn general this is great news – the more data you have, the more confident you can be in your conclusions. It’s probably not worth flipping a coin 1000 times to decide who takes out the trash, but if you did, you would be able to tell with much more certainty whether the coin is fair or not.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "href": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Convergence for large sample sizes",
    "text": "Convergence for large sample sizes\nWhen the size of a dataset is large enough, we get some guarantees.\n\nLaw of Large Numbers\nThe Law of Large Numbers (LLN) states that as the sample size increases, the sample mean will converge to the population mean. In other words, if we take enough samples, the average of those samples will be close to the true average of the population.\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with expected value \\(\\mathbb{E}[X]\\), then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges to \\(\\mathbb{E}[X]\\) as \\(n\\) approaches infinity: \\[\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1\\]\nPrecisely, this states that the probability that the sample mean converges to the population mean is 1. 1\n\n\n\nYou don’t just have to take this for granted – let’s see the LLN in action.\nWe will simulate flipping a biased coin \\(n\\) times, and plot the proportion of heads as we increase the number of flips. We will see that the proportion converges to 0.25 as the number of flips increases.\n\n\nCode\n# set seed\nnp.random.seed(56)\n# flip a fair coin n times\nsamples_sizes = [10, 100, 1000, 1e4, 1e5, 1e6]\nproportion_heads = []\nfor n in samples_sizes:\n    # Simulate flipping a fair coin n times\n    flips = np.random.binomial(1, 0.25, size=int(n))\n    proportion_heads.append(np.mean(flips))\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.lineplot(x=samples_sizes, y=proportion_heads, marker='o', ax=ax)\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nax.set_title(\"Proportion of Heads in Coin Flips\")\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s just one experiment (flip \\(n\\) coins a single time). But this is a random process, so we can simulate it many times and see how the proportion of heads changes.\n\n\nCode\nnp.random.seed(56)\nsamples_sizes = [10, 100, 1000, 1e4, 1e5]\nsim_results = []\n\nfor n in samples_sizes: # outer loop is over sample size\n    for _ in range(1000):  # inner loop runs the simulation 1000 times, for each sample size\n        # sample n flips from a fair coin\n        flips = np.random.binomial(1, 0.25, size=int(n))\n        # calculate the proportion of heads\n        proportion_heads = np.mean(flips)\n        # store the result\n        sim_results.append(pd.Series({\"n\": n, \"proportion_heads\": proportion_heads}))\n# gather all the results (a list of Series) into a DataFrame\nsim_results = pd.concat(sim_results, axis=1).T\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# plot the proportion from individual simulations as points\nsns.stripplot(data=sim_results, \n              x=\"n\", y=\"proportion_heads\", \n              jitter=False, alpha=0.3, ax=ax, native_scale=True, marker='o', size=4)\n# plot the mean proportion of heads across simulations\nsns.lineplot(data=sim_results, \n             x=\"n\", y=\"proportion_heads\", \n             errorbar=\"sd\", marker='o', ax=ax, \n             label='Mean Proportion of Heads')\n# make ticks nicer\nax.set_xticks([10, 100, 1000, 10000, 100000])\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# histogram of the proportion of heads for each sample size\nsns.FacetGrid(sim_results, col=\"n\", col_wrap=2, height=4, aspect=1.2, sharex=False, sharey=False) \\\n    .map(sns.histplot, \"proportion_heads\", bins=np.linspace(0, 1, 41)) \\\n    .set_axis_labels(\"Proportion of Heads\", \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the variability across simulations decreases as the number of flips increases. This is what we mean when we say that the sample mean converges to the population mean. It’s not just close to the true mean on average, it is also more consistent across different samples/simulations.\nWe’re going to take advantage of this fact all the time. Basically any time you compare groups, or competing hypotheses, or evaluate predictive models, you end up taking the average of some quantity. The Law of Large Numbers tells us that as the sample size increases, the average will converge to the true value.\nAgain, what does this mean for your argument with your roommate? As you gather more and more data (coin flips), the proportion of heads (which is itself a sample mean) will converge to the true proportion of heads (the population mean). With enough flips, it becomes very clear whether the coin is rigged or not.\n\n\n\n\n\n\nUnbiased Estimators\n\n\n\nNotice in the simulation above that even when the sample mean was highly variable in small samples, it was always still centered around the true population mean. This is a crucial property for any estimator: it is said to be unbiased if the expected value of the estimator is equal to the true value of the parameter being estimated. In other words, on average, the estimator will give you the correct answer.\nThe LLN gives you a different guarantee: that as the sample size increases, the variance of the estimator decreases, so it becomes more and more likely that the estimator will be close to the true value.\n\n\n\n\nCentral Limit Theorem\nThe Central Limit Theorem states that the sample mean of \\(n\\) independent and identically distributed random variables will converge to a normal distribution, regardless of the distribution of the individual variables. This means that even if a variable is not normally distributed, if you average a bunch of them together, the sample mean will be approximately normally distributed.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges in distribution to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity: \\[\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\n\n\nConsider rolling a die – there is an equal chance (1/6) of getting each of the values between 1 and 6.\nNote that the expected value of each roll of the die is 3.5 – we can compute this as \\[E\\left[X\\right]=1\\left(\\frac{1}{6}\\right)+2\\left(\\frac{1}{6}\\right)+3\\left(\\frac{1}{6}\\right)+4\\left(\\frac{1}{6}\\right)+5\\left(\\frac{1}{6}\\right)+6\\left(\\frac{1}{6}\\right)=3.5\\]\nSo the mean of the distribution of dice values should be 3.5, but as you can see below it’s definitely not normal – values are distributed evenly from 1 to 6 rather than clustering closer to the mean.\n\n\nCode\n# plot histogram of 100 rolls of a die\nnp.random.seed(42)  # for reproducibility\nrolls = pd.DataFrame({'roll': np.random.randint(1, 7, 100)})\n\nsns.histplot(data=rolls, x='roll', bins=6, discrete=True, edgecolor=\"black\", stat='proportion')\nplt.axvline(x=rolls['roll'].mean(), color='red', linestyle='--', label='Mean')\nplt.title(\"Histogram of 100 dice rolls\")\nplt.xlabel(\"Roll\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBut what happens if we average the values of the 100 dice rolls in our sample? We can simulate this process many times (roll a dice 100 times, take the average, repeat) to get a distribution of the average.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) * np.sqrt(n)  # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\sqrt{n}(\\bar{X} - \\mathbb{E}[X])$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see the distribution of the sum is approximately normal! It has the mean we would expect (3.5) and clearly values are clustered symmetrically around the mean.\nFor the purposes of the class it’s not necessary to understand why this happens, just that it happens. To give a bit of intuition though, we basically think that with more samples distributions will starts to cluster around their means.\nWhy is this useful? Well, the normal distribution is a well studied distribution with many useful properties. It is symmetric, easy to work with mathematically, and is useful for approximating the distribution of many real-world processes. The normal distribution is decently good description of most data that clusters around its mean.\nThe CLT tells us that even if we don’t know the distribution of the individual variables, if we average enough of them together, we magically know the distribution of the sample mean.\nRemember earlier in the lecture, we talked about how statistical models are useful as long as they are a faithful description of the outcome probabilities for a DGP? Well, the CLT gives us a guarantee that for a huge class of DGPs, we can use the normal distribution to model the sample mean.\n\nStandard errors\nThe CLT tells us that the sample mean will converge to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity. This means that the standard deviation of the sample mean (called the standard error) is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\).\nWhat does it mean for the standard error to decrease as the sample size increases? It means that as we collect more data, our estimate of the mean becomes more precise. It’s basically the same point as the Law of Large Numbers!\nIn the above example we multiplied the deviations of the sample mean (\\(\\bar{X}-\\mathbb{E}[X]\\)) by \\(\\sqrt{n}\\) to put the distributions on the same scale, which makes it easier to visualize their shapes (so we can see that they become increasingly “normal”). But without that scaling, the distributions get increasingly narrow as \\(n\\) increases, which is exactly what the standard error describes.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\bar{X} - \\mathbb{E}[X]$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#footnotes",
    "href": "notebooks/lecture-04.html#footnotes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n This is technically the Strong Law of Large Numbers, stating that the sample mean converges almost surely to the population mean. There is also a Weak Law of Large Numbers which states that the sample mean converges in probability to the population mean, which is a weaker condition.↩︎",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "assignments/assignment-00.html",
    "href": "assignments/assignment-00.html",
    "title": "Assignment 00",
    "section": "",
    "text": "print(\"Hello, World!\")\n\nHello, World!"
  }
]