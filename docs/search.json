[
  {
    "objectID": "slides/lecture-02-slides.html",
    "href": "slides/lecture-02-slides.html",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n\\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability",
    "href": "slides/lecture-02-slides.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\\]\nBy definition this quantity cannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and it must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs).\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)). If we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say “there’s a 100% chance that something will happen”.\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events \\(H\\) and \\(T\\) cover 100% of the possible outcomes. So we can write: \\[\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\\]\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event \\(A\\), the event either happens or it doesn’t. So we can compute the probability of the event not occurring as: \\[\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\\]\n\nProbability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\).\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows: \\[\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\\]\n\n\nAddition and multiplication rules (and / or)\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that \\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as \\(B\\) above, so we can use the same calculation: \\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\).\n\n\n\n\n\n\nNote on notation\n\n\n\n\n\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively. The notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively. So we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads. Technically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs).\n\n\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events \\(A\\) and \\(B\\), the following rules hold:\n\nAddition rule: For any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\n\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram. \n\n\n\n\nMultiplication rule: For any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset.\n\n\n\n\n\n\n\nConditional probability\n\n\n\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”. The conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day.\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\nSo the conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\\]\n\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”\n\n\n\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n\nIndependence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\nHow does this relate to the multiplication rule? If \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\). That is, knowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply \\(\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\).\n\n\nComplicated counting\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes.\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let’s think about the case where the first flip is heads. The probability of getting a head on the first flip is \\(\\frac{1}{2}\\), and the probability of getting tails on the 4 other flips is also \\(\\frac{1}{2}\\) each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips: \\[\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}\\]\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting “exactly one head”. For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions: \\[\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16\\]\nThere are two common types of outcomes we want to count: permutations and combinations.\nA combination is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are choosing an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\\[\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nIt is clear here that there are 5 possible “slots” where we can place a head.\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still applies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\nLet’s take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices. \\[\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nNow let’s consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\\[\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n\\]\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads. This gives us a total of \\(5 \\cdot 4 = 20\\) ways to choose 2 flips to be heads out of 5 flips.\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order – where flip 1 was heads first).\nThis illustrates the key distinction between combinations and permutations. A permutation is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. If you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting.\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign “slots” in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length \\(n\\), the number of permutations is: \\[\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n\\]\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\nNamely, the number of combinations of \\(k\\) items from a set of \\(n\\) items is given by the formula: \\[\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n\\]\nThis formula counts all of the possible permutations of the sequence, and then divides by the number of ways to arrange the \\(k\\) items that are selected (which is \\(k!\\)) and the number of ways to arrange the remaining \\(n-k\\) items (which is \\((n-k)!\\)).\nIn our example, we have \\(n = 5\\) (the number of flips) and \\(k = 2\\) (the number of heads). So the number of combinations of 2 heads from 5 flips is: \\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-functions",
    "href": "slides/lecture-02-slides.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\nHowever, it is often more convenient to work with probability functions. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails).\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\n\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values.\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n\n\n\n\n\n\n\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0"
  },
  {
    "objectID": "slides/lecture-02-slides.html#random-variables",
    "href": "slides/lecture-02-slides.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\).\nWe use probability functions to describe the probabilities associated with random variables. Specifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nBernoulli random variable\n\n\n\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin.\n\n\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUniform random variable\n\n\n\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\).\n\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course."
  },
  {
    "objectID": "slides/lecture-02-slides.html#probability-distributions-and-histograms",
    "href": "slides/lecture-02-slides.html#probability-distributions-and-histograms",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nWe call the probability function for a random variable a probability distribution, which describes how the probabilities are distributed across the possible values of the random variable.\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point.\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. We can use a bar chart to visualize the counts of each value.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.figure(figsize=(8, 5))\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\", rwidth=0.8)\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let’s check it out:\n\n# load in the dice rolls data\ndice_rolls_df = pd.read_csv(\"../data/dice_rolls.csv\")\nprint(\"Total number of dice rolls:\", len(dice_rolls_df))\ndice_rolls_df.head(10)\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(dice_rolls_df['rolls'], bins=np.arange(0.5, 7.5, 1), density=True, rwidth=0.8)\nplt.title('Histogram of Dice Rolls')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.xticks(np.arange(1, 7))\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\n\n\n\n\n\nNotice that when the \\(y\\)-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a histogram to visualize the distribution of the data.\nThe histogram is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins.\n\ndf = pd.read_csv(\"../data/airbnb.csv\")\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.dropna(subset=[\"price\"])\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb['price'], bins=50, stat=\"proportion\", edgecolor='black')\nplt.title('Histogram of Airbnb Prices')\nplt.xlabel('Price (USD)')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea under a probability distribution\n\n\n\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. This is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1.\nFor discrete: \\[ \\sum_{x} f(x) = 1 \\] For continuous: \\[ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\]\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable \\(X\\) falls between \\(a\\) and \\(b\\), we can compute the area under the PDF from \\(a\\) to \\(b\\): \\[ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx \\]"
  },
  {
    "objectID": "slides/lecture-02-slides.html#expectation",
    "href": "slides/lecture-02-slides.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play.\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you’re not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero.\n\n\n\n\n\n\nGambling warning: the house always wins\n\n\n\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n\n\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\mathbb{E}[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\nSo in our roulette example, you can either lose $1 (with 90% probability) or win $9 (with 10% probability). The expectation would be: \\[\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win $10 on one game and lose $1 on the other 9 games. The average outcome is: \\[\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as \\(\\bar{X}\\), and referred to as the sample mean. \\[\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nThis approximation becomes more accurate as the number of samples \\(n\\) increases. We will talk about this more in a future lecture."
  },
  {
    "objectID": "slides/lecture-02-slides.html#variance-and-standard-deviation",
    "href": "slides/lecture-02-slides.html#variance-and-standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe average is a useful summary of a random variable’s central tendency, but it does not tell us anything about how spread out the values are.\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game – the average amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\n\n\n\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can’t afford to lose all of it on a single game.\nWe need a statistic that captures the typical distance between the values of the random variable and the average value.\n\n\n\n\n\n\nWhy distance from the average?\n\n\n\n\n\nLet’s imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what – the only question is how much you win. Let’s take an example where the payouts still differ by $10: you get $5 if you “lose” and $15 if you “win”.\nIn this case, the expected value is: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n\\] So you can expect to win $6 per game on average.\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\nThe answer is simple: we subtract the average from each value of \\(X\\): \\[X' = X - \\mathbb{E}[X]\\] This gives us a new random variable \\(X'\\) that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\\[\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0\\]\nor more generally: \\[\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0\\]\n\n\n\nSo let’s compute exactly that - the distance from the average. The formula for distance between two vectors \\(x\\) and \\(y\\) is: \\[\nd^2 = \\sum_{i} (x_i - y_i)^2\n\\] where \\(x_i\\) and \\(y_i\\) are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange (\\(a^2 + b^2 = c^2\\)).\nIn our case, we want to compute the distance between the values of the random variable \\(X\\) and the average value \\(\\mathbb{E}[X]\\). \\[\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n\\]\nNow we’re getting somewhere! However, this is adding up all of the squared distances – that means that the more values we have, the larger the distance will be. This is not quite right – instead we want to compute the average distance from the mean in order to get a sense of how spread out the values typically are.\nSo we need to divide by the number of values: \\[\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n\\]\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. If we replace the average with the expectation, we get the formula for the variance of a random variable \\(X\\): \\[\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n\\]\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if \\(X\\) represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the standard deviation:\n\\[\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n\\]\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset: \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\]\n\n\n\n\n\n\nSample variance vs. population variance\n\n\n\n\n\nTechnically, the formula above is an imperfect estimate of the population standard deviation. It’s in general a little bit too small, because the sample mean \\(\\bar{X}\\) does not perfectly represent the population mean \\(\\mathbb{E}[X]\\). We can correct for this by dividing by \\(n-1\\) instead of \\(n\\): \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\] This is called the sample standard deviation.\nWhy is the initial estimate too small? In a small dataset, the sample mean “overfits” the data, meaning it is closer to the individual data points than the true population mean. Let’s think about this in terms of coin flips. If we flip a coin once, the sample mean is either \\(\\hat{X}=0\\) or 1, depending on whether we got heads or tails. But the true population mean is \\(\\mathbb{E}[X]=\\frac{1}{2}\\). If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either \\((1-1)^2\\) or \\((0-0)^2\\)).\nBy contrast, the true (population) standard deviation is \\(\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\\), which is larger than the estimate using the sample mean.\nThis bias in computing the standard deviation gets smaller as the sample size \\(n\\) increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the \\(n-1\\) correction to get a more accurate estimate of the population standard deviation."
  },
  {
    "objectID": "slides/lecture-02-slides.html#common-probability-distributions",
    "href": "slides/lecture-02-slides.html#common-probability-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Common probability distributions",
    "text": "Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. It is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nProbability Function\nParameters\nMean\nVariance\n\n\n\n\nBernoulli\nDiscrete\n\\(f(x) = p^x (1-p)^{1-x}\\)\n\\(p \\in [0, 1]\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\nDiscrete\n\\(f(x) = \\binom{n}{x} p^x (1-p)^{n-x}\\)\n\\(n \\in \\mathbb{N}, p \\in [0, 1]\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nPoisson\nDiscrete\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\nUniform\nContinuous\n\\(f(x) = \\frac{1}{b-a}\\)\n\\(a &lt; b\\)\n\\(\\frac{a+b}{2}\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\n\nNormal\nContinuous\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\nExponential\nContinuous\n\\(f(x) = \\lambda e^{-\\lambda x}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\n\nThe plots below show the probability functions for each of these distributions.\n\nfig, ax = plt.subplots(3, 2, figsize=(12, 12), sharex=False, sharey=True)\nax = ax.flatten()\n# Bernoulli Distribution\nx = np.arange(0, 2)\np = 0.5\nax[0].bar(x, stats.bernoulli.pmf(x, p), width=0.4, color='blue', alpha=0.7)\nax[0].set_title('Bernoulli Distribution (p=0.5)')\nax[0].set_xticks(x)\nax[0].set_xticklabels(['0', '1'])\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Probability')\n\n# Binomial Distribution\nn = 10\nx = np.arange(0, n + 1)\np = 0.5\nax[1].bar(x, stats.binom.pmf(x, n, p), width=0.4, color='green', alpha=0.7)\nax[1].set_title('Binomial Distribution (n=10, p=0.5)')\nax[1].set_xticks(x)\nax[1].set_xlabel('Number of Successes')\nax[1].set_ylabel('Probability')\n\n# Poisson Distribution\nlambda_ = 5\nx = np.arange(0, 15)\nax[2].bar(x, stats.poisson.pmf(x, lambda_), width=0.4, color='orange', alpha=0.7)\nax[2].set_title('Poisson Distribution (λ=5)')\nax[2].set_xticks(x)\nax[2].set_xlabel('Number of Events')\nax[2].set_ylabel('Probability') \n\n# Uniform Distribution\na, b = 0, 10\nx = np.linspace(a, b, 1000)\nax[3].plot(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan')\nax[3].set_title('Uniform Distribution (a=0, b=10)')\nax[3].set_xlabel('Value')\nax[3].set_ylabel('Probability Density')\nax[3].fill_between(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan', alpha=0.2)\n\n# Normal Distribution\nmu, sigma = 0, 1\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nax[4].plot(x, stats.norm.pdf(x, mu, sigma), color='purple')\nax[4].set_title('Normal Distribution (μ=0, σ=1)')\nax[4].set_xlabel('Value')\nax[4].set_ylabel('Probability Density')\nax[4].fill_between(x, stats.norm.pdf(x, mu, sigma), color='purple', alpha=0.2)\n\n# Exponential Distribution\nlambda_ = 1\nx = np.linspace(0, 10, 1000)\nax[5].plot(x, stats.expon.pdf(x, scale=1/lambda_), color='red')\nax[5].set_title('Exponential Distribution (λ=1)')\nax[5].set_xlabel('Value')\nax[5].set_ylabel('Probability Density')\nax[5].fill_between(x, stats.expon.pdf(x, scale=1/lambda_), color='red', alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/lecture-02-slides.html#summary",
    "href": "slides/lecture-02-slides.html#summary",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. Probability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\nIn particular, we covered:\n\nThe basic definition of probability and how to compute it for simple events.\nThe addition and multiplication rules for calculating probabilities of multiple events.\nThe concept of independence and how it affects probabilities.\nRandom variables and their probability distributions\nThe expectation (or expected value) of a random variable\nVariance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#course-goals",
    "href": "slides/lecture-00-slides.html#course-goals",
    "title": "Why Statistics?",
    "section": "Course Goals",
    "text": "Course Goals\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#why-statistics",
    "href": "slides/lecture-00-slides.html#why-statistics",
    "title": "Why Statistics?",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data:\n\ndescription\ninference\nprediction"
  },
  {
    "objectID": "slides/lecture-00-slides.html#description",
    "href": "slides/lecture-00-slides.html#description",
    "title": "Why Statistics?",
    "section": "Description",
    "text": "Description\n\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\n\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features."
  },
  {
    "objectID": "slides/lecture-00-slides.html#description-1",
    "href": "slides/lecture-00-slides.html#description-1",
    "title": "Why Statistics?",
    "section": "Description",
    "text": "Description\n\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n\n\n3 rows × 26 columns"
  },
  {
    "objectID": "slides/lecture-00-slides.html#sample-neq-population",
    "href": "slides/lecture-00-slides.html#sample-neq-population",
    "title": "Why Statistics?",
    "section": "Sample \\(\\neq\\) Population",
    "text": "Sample \\(\\neq\\) Population\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\nPopulation\nthe entire set of data that you are interested in.\nSample\na subset of a population.\nA random sample is a sample that is selected randomly from the population.\n\n\n\n\n\n\n\n\nExample: Airbnb listings in New York City\n\n\nWe want to know the average price of Airbnb listings in New York City.\n\npopulation: all Airbnb listings in New York City\nsample: a smaller subset of those listings, which may or may not be representative of the entire population."
  },
  {
    "objectID": "slides/lecture-00-slides.html#what-is-the-population",
    "href": "slides/lecture-00-slides.html#what-is-the-population",
    "title": "Why Statistics?",
    "section": "What is the population?",
    "text": "What is the population?\nFlexible definition:\n\nAverage price of all short-term rentals in New York City? Population: all rentals (not just Airbnb listings) in New York City.\n\n\nOften, the population is actually more abstract or theoretical\n\nAverage price of all possible Airbnb listings in New York City? Population: all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more."
  },
  {
    "objectID": "slides/lecture-00-slides.html#quiz-restaurant-survey",
    "href": "slides/lecture-00-slides.html#quiz-restaurant-survey",
    "title": "Why Statistics?",
    "section": "Quiz: restaurant survey",
    "text": "Quiz: restaurant survey"
  },
  {
    "objectID": "slides/lecture-00-slides.html#inference",
    "href": "slides/lecture-00-slides.html#inference",
    "title": "Why Statistics?",
    "section": "Inference",
    "text": "Inference\nWhat if we want to answer questions about a population based on a sample?\nThis is where inference comes in.\n\nUse the given sample to infer something about the population.\n\n\nHow do we do this if we can’t ever see the entire population?\n\nNeed a link which connects the sample to the population\nTreat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\n\n\n\n\nThere is always a DGP\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population.\n\nEncompasses all the factors that influence the data (incl. the mechanisms and relationships between variables).\nThere has to be a DGP, even if we don’t know what it is.\nThe DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown.\n\nWe can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\n\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!"
  },
  {
    "objectID": "slides/lecture-00-slides.html#statistical-models",
    "href": "slides/lecture-00-slides.html#statistical-models",
    "title": "Why Statistics?",
    "section": "Statistical models",
    "text": "Statistical models\nWhen the full DGP is too complicated / unknown, we use a model\n\nsimplified mathematical representation of the DGP\nallows us to make inferences about the population based on the sample\nultimately sort of a guess – about where your data come from."
  },
  {
    "objectID": "slides/lecture-00-slides.html#example-airbnb-listings.",
    "href": "slides/lecture-00-slides.html#example-airbnb-listings.",
    "title": "Why Statistics?",
    "section": "Example: Airbnb listings.",
    "text": "Example: Airbnb listings.\n\nAssume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs.\nProbability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.\n\n\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: “If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\n\nquestion about the probability of the sample, given a certain model of the DGP\nit intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings."
  },
  {
    "objectID": "slides/lecture-00-slides.html#evaluating-models",
    "href": "slides/lecture-00-slides.html#evaluating-models",
    "title": "Why Statistics?",
    "section": "Evaluating models",
    "text": "Evaluating models\nWhat should we do now?\n\nNow that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model.\nModel is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous."
  },
  {
    "objectID": "slides/lecture-00-slides.html#inference-requires-domain-knowledge",
    "href": "slides/lecture-00-slides.html#inference-requires-domain-knowledge",
    "title": "Why Statistics?",
    "section": "Inference requires domain knowledge",
    "text": "Inference requires domain knowledge\n\n\n\n\n\n\n\nDon’t try this at home!\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data.\nThis requires domain knowledge and subject matter expertise.\n\n\n\n\nIn the Airbnb example:\n\nAssuming that all boroughs are equally likely to produce listings is a pretty bad assumption\n\nManhattan sees vastly more tourism than the other boroughs\nBrooklyn and Queens have by far the most residents according to recent census data."
  },
  {
    "objectID": "slides/lecture-00-slides.html#prediction",
    "href": "slides/lecture-00-slides.html#prediction",
    "title": "Why Statistics?",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data: we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. Basic idea of the model:\n\nwe assume the features of the listing (e.g., price) are related to the probability of it being in a certain borough\n\ne.g., perhaps more expensive listings are more likely to be in Manhattan\n\n\n\n\n\n\n\n\n\n\nFitting a model\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nCoin flip has a single parameter: the probability of landing on heads.\n\nIf you turn the knob to 0.5, you get a fair coin;\nif you turn it to 1.0, you get a coin that always lands on heads;\nif you turn it to 0.0, you get a coin that always lands on tails.\n\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data."
  },
  {
    "objectID": "slides/lecture-00-slides.html#predicting-the-borough-of-a-listing",
    "href": "slides/lecture-00-slides.html#predicting-the-borough-of-a-listing",
    "title": "Why Statistics?",
    "section": "Predicting the borough of a listing",
    "text": "Predicting the borough of a listing\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================"
  },
  {
    "objectID": "slides/lecture-00-slides.html#evaluating-predictions",
    "href": "slides/lecture-00-slides.html#evaluating-predictions",
    "title": "Why Statistics?",
    "section": "Evaluating predictions",
    "text": "Evaluating predictions\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\n\n\n\n\n\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously.\nFor now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be.\nFor example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance."
  },
  {
    "objectID": "slides/lecture-00-slides.html#model-predictions-distribution",
    "href": "slides/lecture-00-slides.html#model-predictions-distribution",
    "title": "Why Statistics?",
    "section": "Model predictions (distribution)",
    "text": "Model predictions (distribution)\nNow let’s take a look at the distribution of the model’s predictions.\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP."
  },
  {
    "objectID": "slides/lecture-00-slides.html#summary",
    "href": "slides/lecture-00-slides.html#summary",
    "title": "Why Statistics?",
    "section": "Summary",
    "text": "Summary\n3 objectives of data analysis: description, inference, and prediction.\nHopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. (Don’t worry, we’ll get there!)\nUp next:\n\nbasic programming concepts that are important for data science.\nAfter that we will learn some foundational concepts in probability that will help us think about data and models more rigorously.\n\n\nFrom there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learned any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments."
  },
  {
    "objectID": "assignments/assignment-00.html",
    "href": "assignments/assignment-00.html",
    "title": "Assignment 00",
    "section": "",
    "text": "print(\"Hello, World!\")\n\nHello, World!\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"A\": [1, 2, 3, 4, 5]})\ndf[\"B\"] = df[\"A\"] ** 2\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1\n\n\n1\n2\n4\n\n\n2\n3\n9\n\n\n3\n4\n16\n\n\n4\n5\n25\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.pointplot(data=df, x=\"A\", y=\"B\")"
  },
  {
    "objectID": "notebooks/lecture-09.html",
    "href": "notebooks/lecture-09.html",
    "title": "Lecture 09: Regression Cont.",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 10, 100)\ny = 2 * x + 1 + np.random.normal(0, 1, size=x.shape)\nplt.scatter(x, y, label='Data')\nplt.plot(x, 2 * x + 1, color='red', label='True Line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Linear Regression Example')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n\nfrom shiny import *\n\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"N\", 0, 100, 40),\n    ui.output_text_verbatim(\"txt\"),\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"The value of n*2 is {input.n() * 2}\"\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "notebooks/lecture-07.html",
    "href": "notebooks/lecture-07.html",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "",
    "text": "There is one more frequently applicable sampling-based method that is super helpful in statistical inference: permutation (or randomization) tests.\nPermutation tests, like the bootstrap, are non-parametric – meaning they do not rely on assumptions about the underlying distribution of the data. Like the bootstrap, permutation tests rely on resampling – only instead of resampling with replacement, we resample without replacement.\nSay we have two samples, \\(X\\) and \\(Y\\), and we want to test if they have different underlying distributions (e.g. different means). Then, our null hypothesis (\\(H_0\\)) is that the two samples are drawn from the same distribution.\nSo, we can combine the two samples into one larger sample, \\(Z = X \\cup Y\\), and then randomly split \\(Z\\) into two new samples, \\(X'\\) and \\(Y'\\), of the same size as the original samples. We can then compute the test statistic (e.g. the difference in means) for the new samples and repeat this process many times to build a distribution of test statistics under the null hypothesis.\nThe only assumption we need to make is that the two samples are exchangeable under the null hypothesis. This means that, if the null hypothesis is true, the two samples could be shuffled without changing the underlying distribution.\nnp.random.permutation is a useful function for this. It randomly permutes the elements of an array, which we can use to create our new samples. Let’s define a function to perform a permutation test for\n\n\nCode\ndef permutation_test(test_func, x, y, num_permutations=10000, rng=None, one_sided=True):\n    # Compute the observed test statistic\n    observed_stat = test_func(x, y)\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n    count = 0\n\n    if rng is None:\n        rng = np.random.default_rng()\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the test statistic for the permuted samples\n        permuted_stat = test_func(x_perm, y_perm)\n\n        # Compare the permuted statistic to the observed statistic\n        if one_sided:\n            if permuted_stat &gt;= observed_stat:\n                count += 1\n        else:\n            if np.abs(permuted_stat) &gt;= np.abs(observed_stat):\n                count += 1\n\n    # Compute the p-value\n    p_value = count / num_permutations\n    return p_value\n\n\nWe can apply this to our NBA data to test if SGA and Giannis have different scoring rates. This is quite similar to the bootstrap hypothesis test we did in the previous lecture, but instead of resampling with replacement, we will resample without replacement.\nBoth versions work, but the permutation test tends to be more powerful.\n\n\n\n\n\n\nStatistical Power\n\n\n\n\nPower\n\nThe probability of correctly rejecting the null hypothesis when it is false.\n\n\nA more “powerful” test is more likely to detect a true effect. The power of a test is often written as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false).\nWe usually want our tests to have high power, so we can detect true effects when they exist. We try to balance this against the risk of falsely rejecting the null hypothesis (Type I error) when it is actually true.\n\n\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace(\n    {\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan}\n)\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nrng = np.random.default_rng(42) \n# run the permutation test\np_value = permutation_test(\n    lambda x, y: np.mean(x) - np.mean(y), # lambda functions can be used inline without naming the function\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    rng=rng\n)\nprint(\n    f\"P-value for the hypothesis that SGA scores more points than Giannis: {p_value:.4f}\"\n)\n\n\nP-value for the hypothesis that SGA scores more points than Giannis: 0.0336",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-07.html#permutation-tests",
    "href": "notebooks/lecture-07.html#permutation-tests",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "",
    "text": "There is one more frequently applicable sampling-based method that is super helpful in statistical inference: permutation (or randomization) tests.\nPermutation tests, like the bootstrap, are non-parametric – meaning they do not rely on assumptions about the underlying distribution of the data. Like the bootstrap, permutation tests rely on resampling – only instead of resampling with replacement, we resample without replacement.\nSay we have two samples, \\(X\\) and \\(Y\\), and we want to test if they have different underlying distributions (e.g. different means). Then, our null hypothesis (\\(H_0\\)) is that the two samples are drawn from the same distribution.\nSo, we can combine the two samples into one larger sample, \\(Z = X \\cup Y\\), and then randomly split \\(Z\\) into two new samples, \\(X'\\) and \\(Y'\\), of the same size as the original samples. We can then compute the test statistic (e.g. the difference in means) for the new samples and repeat this process many times to build a distribution of test statistics under the null hypothesis.\nThe only assumption we need to make is that the two samples are exchangeable under the null hypothesis. This means that, if the null hypothesis is true, the two samples could be shuffled without changing the underlying distribution.\nnp.random.permutation is a useful function for this. It randomly permutes the elements of an array, which we can use to create our new samples. Let’s define a function to perform a permutation test for\n\n\nCode\ndef permutation_test(test_func, x, y, num_permutations=10000, rng=None, one_sided=True):\n    # Compute the observed test statistic\n    observed_stat = test_func(x, y)\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n    count = 0\n\n    if rng is None:\n        rng = np.random.default_rng()\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the test statistic for the permuted samples\n        permuted_stat = test_func(x_perm, y_perm)\n\n        # Compare the permuted statistic to the observed statistic\n        if one_sided:\n            if permuted_stat &gt;= observed_stat:\n                count += 1\n        else:\n            if np.abs(permuted_stat) &gt;= np.abs(observed_stat):\n                count += 1\n\n    # Compute the p-value\n    p_value = count / num_permutations\n    return p_value\n\n\nWe can apply this to our NBA data to test if SGA and Giannis have different scoring rates. This is quite similar to the bootstrap hypothesis test we did in the previous lecture, but instead of resampling with replacement, we will resample without replacement.\nBoth versions work, but the permutation test tends to be more powerful.\n\n\n\n\n\n\nStatistical Power\n\n\n\n\nPower\n\nThe probability of correctly rejecting the null hypothesis when it is false.\n\n\nA more “powerful” test is more likely to detect a true effect. The power of a test is often written as \\(1 - \\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject the null hypothesis when it is false).\nWe usually want our tests to have high power, so we can detect true effects when they exist. We try to balance this against the risk of falsely rejecting the null hypothesis (Type I error) when it is actually true.\n\n\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace(\n    {\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan}\n)\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nrng = np.random.default_rng(42) \n# run the permutation test\np_value = permutation_test(\n    lambda x, y: np.mean(x) - np.mean(y), # lambda functions can be used inline without naming the function\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    rng=rng\n)\nprint(\n    f\"P-value for the hypothesis that SGA scores more points than Giannis: {p_value:.4f}\"\n)\n\n\nP-value for the hypothesis that SGA scores more points than Giannis: 0.0336",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-07.html#there-is-really-only-one-test",
    "href": "notebooks/lecture-07.html#there-is-really-only-one-test",
    "title": "Lecture 07: Permutation and Computational Hypothesis Testing",
    "section": "There is really only one test!",
    "text": "There is really only one test!\nThe more statistics you learn and the more you are exposed to work in quantitative fields, the more you will see a wide variety of complicated statistical techniques and methods.\nUltimately they all represent the same process: 1. Compute a test statistic on the observed data. 2. Choose a null hypothesis / model. Either specify the null distribution explicitly or use a simulation-based method to generate a distribution of test statistics under the null hypothesis. 3. Compute a p-value by comparing the observed test statistic to the distribution of test statistics under the null hypothesis.\nMost of the literature in classical statistics focuses on mathematically deriving analytical solutions for 2 and 3. All of the fancy named statistical tests you see are variations with different test statistics and null hypotheses.\nCheck out this blog post by Allen Downey for more on this idea and explanation of the advantages of simulation-based methods for hypothesis testing.\n\nExample: Two-sample t-test\nOne of the most common statistical tests is the independent two-sample \\(t\\)-test, which tests if two independent samples have different means. This is used in many fields, including psychology, medicine, and social sciences – basically anywhere you want to compare two groups.\nRemembering the \\(t\\)-test is not an important point of this course – we introduce it here because it is used so frequently in practice that it is worth understanding the basic idea behind it (and that it is just a special case of the general hypothesis testing framework we have been discussing).\nWe learned already that (by the Central Limit Theorem) as the sample size increases, the sampling distribution of the sample mean becomes approximately normal, even if the underlying distribution is not normal. The exact distribution of the sample mean, however, is called the t-distribution. It is quite like the normal distribution, but has heavier tails (more likely to produce extreme values). See the plots below for an illustration of how the t-distribution converges to the standard normal distribution as the sample size increases.\n\n\nCode\nx = np.linspace(-4, 4, 100)\nstats.t.pdf(\n    x,\n    df=10,\n    loc=0,\n    scale=1\n)\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\n# Plotting the t-distribution and normal distribution for comparison\nfor i, sample_size in enumerate([2, 10, 30]):\n    df = 2 * sample_size - 2\n    sns.lineplot(\n        x=x,\n        y=stats.t.pdf(x, df=df, loc=0, scale=1),\n        label=f\"$t$ (df={df})\",\n        ax=ax[i]\n    )\n    sns.lineplot(\n        x=x,\n        y=stats.norm.pdf(x, loc=0, scale=1),\n        label=\"Normal\",\n        ax=ax[i]\n    )\n    ax[i].grid(True)\n    ax[i].set_title(f\"$t$ vs Normal ($n={sample_size}$)\")\n    ax[i].set_xlabel(\"x\")\n    ax[i].set_ylabel(\"Density\")\n    sns.move_legend(ax[i], loc=\"upper left\")\n    \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s create two independent samples from uniform distributions with different means and equal variance. We’ll perform a two-sample t-test to see if they are significantly different. In other words, we will test the null hypothesis that the two samples have the same mean against the alternative hypothesis that they have different means.\nWe’ll use the built-in scipy.stats.ttest_ind function to perform the t-test, but we will also manually compute the t-statistic and p-value to illustrate the process. We will also sample from the t-distribution to compare our results. Finally, we will perform a permutation test to see how it compares.\n\n\nCode\n# simulate a t-distribution\nrng = np.random.default_rng(43)  # set a random seed for reproducibility\n\n# take two samples from different uniform distributions\nsamples_a = rng.uniform(low=-1, high=3, size=10)\nsamples_b = rng.uniform(low=-3, high=1, size=10)\n\nt_parametric = stats.ttest_ind(\n    samples_a,\n    samples_b,\n    equal_var=True\n)\nprint(f\"Parametric t-test statistic: {t_parametric.statistic:.4f}, p-value: {t_parametric.pvalue:.6f}\")\n\n# calculate the t-statistic manually\n\ndiff_means = np.mean(samples_a) - np.mean(samples_b)\n# assume they have the same variance -- use the \"pooled\" or averaged variance\npooled_var = (np.var(samples_a, ddof=1) + np.var(samples_b, ddof=1))\npooled_std_error = np.sqrt(pooled_var / len(samples_a))\nt_stat= diff_means / pooled_std_error\nt_abs = np.abs(t_stat)\nprint(f\"Manual t-statistic: {t_stat:.4f}\")\n# two-tailed test: 1 - (CDF(t_statistic) - CDF(-t_statistic))\np_value_t_pdf = 1 - (stats.t.cdf(\n    t_abs,\n    df=len(samples_a) + len(samples_b) - 2\n) - stats.t.cdf(\n    -t_abs,\n    df=len(samples_a) + len(samples_b) - 2\n))\nprint(f\"Theoretical p-value from PDF of t: {p_value_t_pdf:.6f}\")\n\n# Sample from a t-distribution\nnum_samples = 10000\nt_samples = rng.standard_t(\n    df=len(samples_a) + len(samples_b) - 2,\n    size=num_samples\n)\n# calculate the p-value from the simulated t-distribution\np_value_simulated = np.mean(np.abs(t_samples) &gt;= t_abs)\nprint(f\"Simulated p-value by sampling from t: {p_value_simulated:.6f}\")\n\n\n## # Permutation test for t-statistic\ndef permutation_test_t(\n    x, y, num_permutations=10000, rng=None,\n):\n    # Compute the observed t-statistic\n    observed_stat = stats.ttest_ind(x, y, equal_var=True).statistic\n\n    # Combine the two samples\n    combined = np.concatenate([x, y])\n\n    if rng is None:\n        rng = np.random.default_rng()\n    permuted_stats = []\n    for _ in range(num_permutations):\n        # Permute the combined array\n        permuted = rng.permutation(combined)\n\n        # Split the permuted array into two new samples\n        x_perm = permuted[:len(x)]\n        y_perm = permuted[len(x):]\n\n        # Compute the t-statistic for the permuted samples\n        permuted_stat = stats.ttest_ind(x_perm, y_perm, equal_var=True).statistic\n\n        # Compare the permuted statistic to the observed statistic\n        permuted_stats.append(permuted_stat)\n    # Compute the p-value\n    permuted_stats = np.array(permuted_stats)\n    p_value = np.mean(np.abs(permuted_stats) &gt;= np.abs(observed_stat))\n\n    return permuted_stats, p_value\n\n# Run the permutation test\npermuted_stats, p_value = permutation_test_t(\n    samples_a,\n    samples_b,\n    num_permutations=10000,\n    rng = rng\n)\nprint(f\"Permutation test p-value: {p_value:.6f}\")\n\n\nParametric t-test statistic: 2.6958, p-value: 0.014783\nManual t-statistic: 2.6958\nTheoretical p-value from PDF of t: 0.014783\nSimulated p-value by sampling from t: 0.014400\nPermutation test p-value: 0.015700\n\n\nAll of these results are quite similar! It’s not a coincidence – they are all effectively the same test. We’re comparing the observed t-statistic to the distribution of t-statistics under the null hypothesis that the two samples are drawn from the same distribution.\nThis might be easier to appreciate visually:\n\n\nCode\nfig, ax = plt.subplots(1, 3, figsize=(12, 5), sharey=True, sharex=True)\n# plot the t-distribution and highlight the observed t-statistic\nax[0].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), label='t-distribution')\nax[0].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[0].fill_between(\n    x,\n    stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2),\n    where=np.abs(x) &gt;= t_abs,\n    alpha=0.3,\n    color='red',\n    label='Area for p-value'\n)\nax[0].set_title('t-distribution PDF')\nax[0].set_xlabel('$t$-value')\nax[0].set_ylabel('Density')\nax[0].legend(loc=\"upper left\", fontsize='x-small', frameon=False)\n\nsns.histplot(t_samples, bins=30, ax=ax[1], stat=\"density\", color=\"gray\", alpha=0.5)\nax[1].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), linestyle='--', label='t-distribution')   \nax[1].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[1].set_title('Sampled t-distribution')\nax[1].set_xlabel('$t$-value')\nax[1].set_ylabel('Density')\n\nsns.histplot(permuted_stats, bins=30, ax=ax[2], stat=\"density\", color=\"gray\", alpha=0.5)\nax[2].plot(x, stats.t.pdf(x, df=len(samples_a) + len(samples_b) - 2), linestyle='--', label='t-distribution')\nax[2].axvline(t_abs, color='red', linestyle='--', label='Observed t-statistic')\nax[2].set_title('Permutation Test Statistics')\nax[2].set_xlabel('$t$-value')\nax[2].set_ylabel('Density')\n\n\nText(0, 0.5, 'Density')\n\n\n\n\n\n\n\n\n\nSee how close the sampling distributions are to the theoretical t-distribution? This is why all the tests are so similar.",
    "crumbs": [
      "Home",
      "Lecture 07: Permutation Tests"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html",
    "href": "notebooks/lecture-05.html",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "",
    "text": "In the introductory lecture, we talked about the role of inference: drawing more general conclusions (about a population) from specific observations (a sample). Now that we have covered the most important building blocks, we can start to actually talk about how to make statistical inferences.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "href": "notebooks/lecture-05.html#a-worked-example-the-nbas-most-valuable-player-mvp",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "A worked example: The NBA’s most valuable player (MVP)",
    "text": "A worked example: The NBA’s most valuable player (MVP)\n“Shai Gilgeous Alexander (SGA) is the best player in the NBA” is a very broad claim that is hard to test. What makes a player “the best”? Does stating that they scored the most points make them the best? That his team won the most games?\nLet’s try to instead design a hypothesis that is more specific and testable. We can choose a specific metric to evaluate players, such as points per game (PPG). You can care about other metrics, like assists or rebounds, but let’s just focus on PPG for now.\nTo make the claim more specific and testable, we’ll start off with some assumptions: - Every player’s scoring follows a distribution with some mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Becuase PPG is an average over the points scored in each game, the CLT tells us that the distribution of average PPG is approximately normal (even if “points scored” is not). Specifically, PPG is a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\), where \\(n\\) is the number of games played. - The player with the highest \\(\\mu\\) is the best scorer in the league (i.e. assume that teammates, coaches, conferences, etc. are all negligible).\nTo make the claim that SGA is a better scorer than his competitors, we need to show how unlikely it is that he has the highest scoring average if his skill level is not actually the highest in the league.\n\\(H_0\\): \\(\\mu_{\\text{SGA}} \\leq c\\)\n\\(H_1\\): \\(\\mu_{\\text{SGA}} &gt; c\\)\nAll of a sudden, this is way more familiar, specific, and testable. We can now use data to compute the \\(p\\)-value for this hypothesis test.\nShai scored 32.7 PPG in 76 games the 2024-2025 season, which is the highest average in the league. The next-highest scorer was Giannis Antetokounmpo with 30.4 PPG in 72 games.\nWe will use data from the 2024-2025 NBA season, courtesy of Basketball Reference, to evaluate our hypothesis.\n\n\nCode\n# read the data for Shai Gilgeous-Alexander and Giannis Antetokounmpo\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace({\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan})\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\nshai_sample_mean = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].mean()\nshai_sample_std = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].std()\nprint(f\"Shai's PPG is {shai_sample_mean:.2f} with a standard deviation of {shai_sample_std:.2f}.\")\n\ngiannis_sample_mean = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].mean()\ngiannis_sample_std = compare_df.query(\"player == 'Giannis Antetokounmpo'\")[\"PTS\"].std()\nprint(f\"Giannis's PPG is {giannis_sample_mean:.2f} with a standard deviation of {giannis_sample_std:.2f}.\")\n\n# plot the distribution of PPG for both players\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(data=compare_df, x=\"PTS\", hue=\"player\", stat=\"density\", common_norm=False, bins=20, ax=ax)\nax.axvline(shai_sample_mean, color=\"blue\", linestyle=\"--\", label=f\"Shai's PPG: {shai_sample_mean:.2f}\")\nax.axvline(giannis_sample_mean, color=\"orange\", linestyle=\"--\", label=f\"Giannis's PPG: {giannis_sample_mean:.2f}\")\nax.set_title(\"Distribution of Points Per Game (PPG) for Shai and Giannis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nplt.show()\n\n\nShai's PPG is 32.68 with a standard deviation of 7.54.\nGiannis's PPG is 30.39 with a standard deviation of 7.32.\n\n\n\n\n\n\n\n\n\nNow what is the null distribution? Our null hypothesis is that SGA’s scoring ability is not higher than his competitors’. So let’s say that under the null, SGA’s inherent scoring ability is equal to Giannis’s observed PPG of 30.4. That’s the mean of our normal distribution given by CLT. For the standard error, we can use the sample standard deviation of Shai’s points scored and divide it by the square root of the number of games he played (76).\nNow the question is: what is the probability of SGA scoring at least 32.7 PPG if his true scoring ability is actually 30.4 PPG?\nWe can just use the normal distribution to compute this probability!\n\n\n\n\n\n\nThe Cumulative Distribution Function (CDF)\n\n\n\n\n\nThe CDF of a normal distribution gives us the probability that a random variable is less than or equal to a certain value. In other words, it computes are under a probability density function (PDF) up to a certain point.\nTo compute the probability of a random variable being greater than a certain value, we can just subtract the CDF from 1. \\[\nP(X &gt; x) = 1 - P(X \\leq x) = 1 - \\text{CDF}(x) = 1 - \\int_{-\\infty}^{x} f(t) dt\n\\]\n\n\n\n\n\nCode\n## PDF of normal\nshai_n_games = compare_df.query(\"player == 'Shai Gilgeous-Alexander'\")[\"PTS\"].count()\np_value_clt = 1 - stats.norm.cdf(\n    x=32.7, \n    loc=30.4, # the mean of the null distribution (Giannis's PPG)\n    scale=shai_sample_std/np.sqrt(shai_n_games)) \nprint(\"p-value from CLT:\", p_value_clt)\n# visualize the p-value\nx = np.linspace(27, 35, 1000)\nnull_pdf = stats.norm.pdf(x, loc=30.4, scale=shai_sample_std/np.sqrt(shai_n_games))\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x, null_pdf, label=r\"Normal Distribution given $H_0:$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: 32.7\")\nax.fill_between(x, null_pdf, where=(x &gt;= 32.7), alpha=0.4, label=\"Area for p-value\")\nax.set_title(\"PPG distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Probability Density\")\nax.legend()\nplt.show()\n\n\np-value from CLT: 0.00391019858958408\n\n\n\n\n\n\n\n\n\nThis analysis tells us that if Shai’s true scoring ability is 30.4 PPG, the probability of him scoring at least 32.7 PPG over 76 games is very small.\nWe can also try simulating the null distribution through individual game scores to see how likely it is that SGA would score at least 32.7 PPG if his true scoring ability is actually 30.4 PPG. That is, we’ll simulate 76 games of scoring, we each game is drawn from a distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai’s points scored. We’ll repeat this simulation many times to get a distribution of PPG scores under the null hypothesis.\n\n\nCode\nrng = np.random.default_rng(42)  # for reproducibility\nn_sims = 5000\nshai_simulated_ppg = []\nfor _ in range(n_sims):\n    # simulate 76 games of scoring, each game is drawn from a distribution with \n    # mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai's points scored\n    # Variance of continuous uniform distribution on [a, b] is (b-a)^2 / 12\n    low_minus_high = np.sqrt(12 * shai_sample_std**2)\n    simulated_scores = rng.uniform(low=30.4 - low_minus_high/2, high=30.4 + low_minus_high/2, size=76)\n    simulated_ppg = np.mean(simulated_scores)\n    shai_simulated_ppg.append(simulated_ppg)\n# compute the p-value from the simulated PPG scores\nshai_simulated_ppg = np.array(shai_simulated_ppg)\np_value_simulated = np.mean(shai_simulated_ppg &gt;= 32.7)\nprint(\"p-value from simulation:\", p_value_simulated)\n\n# plot the distribution of simulated PPG scores\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(shai_simulated_ppg, bins=30, stat=\"density\", ax=ax, label=\"Simulated PPG under $H_0$\")\nax.axvline(32.7, color=\"red\", linestyle=\"--\", label=f\"Observed PPG: {32.7}\")\nax.set_title(\"Simulated PPG Distribution under the Null Hypothesis\")\nax.set_xlabel(\"Points Per Game (PPG)\")\nax.set_ylabel(\"Density\")\nax.legend()\nplt.show()\n\n\np-value from simulation: 0.004\n\n\n\n\n\n\n\n\n\nFor this second approach, we didn’t use the CLT at all! Instead, we relied on simulation to understand the distribution of Shai’s scoring under the null hypothesis. And yet, the results are almost identical to the ones we got using the normal approximation!\nCan you think of any limitations of the approach we took? What assumptions did we make that might not be right? Next lecture we’ll look at approaches that do not make any assumptions about the distribution of the data, and instead rely on resampling techniques to evaluate hypotheses.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#errors-in-hypothesis-testing",
    "href": "notebooks/lecture-05.html#errors-in-hypothesis-testing",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Errors in hypothesis testing",
    "text": "Errors in hypothesis testing\nThere are two types of errors that can occur in hypothesis testing:\n\nFalse positive: reject the null hypothesis when it is actually true.\nFalse negative: fail to reject the null hypothesis when it is actually false.\n\nThese errors are also known as Type I and Type II errors, respectively.\nThis is a bit easier undestand if you tabulate the possible outcomes of a hypothesis test:\n\n\n\n\n\n\n\n\n\nReject \\(H_0\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nType I error (false positive)\nCorrect decision\n\n\n\\(H_0\\) is false\nCorrect decision\nType II error (false negative)\n\n\n\nThe convention in statistics is to denote the probability of a Type I error (false positive) as \\(\\alpha\\) and the probability of a Type II error (false negative) as \\(\\beta\\).",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-05.html#summary",
    "href": "notebooks/lecture-05.html#summary",
    "title": "Lecture 05: Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we studied hypothesis testing – a framework for evaluating claims about data and making data-driven decisions under uncertainty. We learned about the need to formalize claims and quantify uncertainty due to sampling variability, how to establish a null hypothesis, the meaning of the \\(p\\)-value, and how to compute it based on sampling distributions.\nIn the next lecture, we will think about what to do when we don’t have any idea about the underlying distribution of the data, which makes it hard to generate sampling distributions.",
    "crumbs": [
      "Home",
      "Lecture 05: Hypothesis Testing"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html",
    "href": "notebooks/lecture-03.html",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "",
    "text": "Now that we have a good understanding of the basics of probability, we can start to explore how we deal with randomness computationally.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "href": "notebooks/lecture-03.html#sampling-from-probability-distributions",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Sampling from probability distributions",
    "text": "Sampling from probability distributions\n\nA sample is a subset of data drawn from a more general population. That population can be thought of as a probability distribution – this distribution essentially describes how likely you are to observe different values when you sample from it.\nWe will quickly review some important concepts related to sampling.\n\nIndependent and identically distributed (IID) sampling\nWhen we sample from a probability distribution, we often assume that the samples are independent and identically distributed (IID). This means that each sample is drawn from the same distribution and that the samples do not influence each other.\nCoin flips are a good example of IID sampling. If you flip a fair coin multiple times, each flip has the same probability of being heads or tails (this is the “identically distributed” part), and the outcome of one flip does not affect the outcome of another (this is the “independent” part). The same is true for rolling a die!\nWe often apply this concept to more complex random processes as well, where we do not have such a clear understanding of the underlying process. For example, if we are sampling the heights of people in a city, we might assume that each person’s height is drawn from the same distribution (the distribution of heights in that city) and that one person’s height does not affect another’s. Whether or not the IID assumption holds in practice is an important question to consider when analyzing data – for example, do you think that the heights of people in a family are independent of each other?\n\n\nSampling with and without replacement\nAnother important concept in sampling is the distinction between sampling with replacement and sampling without replacement.\n\nSampling with replacement means that after we draw a sample from the population, we put it back before drawing the next sample. This means that the same object / instance can be selected multiple times.\nSampling without replacement means that once we draw a sample, we do not put it back before drawing the next sample. This means that each individual can only be selected once. This can introduce dependencies between samples, as the population changes after each draw.",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-03.html#simulating-a-random-sample",
    "href": "notebooks/lecture-03.html#simulating-a-random-sample",
    "title": "Lecture 03: Sampling and Simulation",
    "section": "Simulating a random sample",
    "text": "Simulating a random sample\nWe can simulate a random process by sampling from a corresponding probability distribution.\n\n\n\n\n\n\nNote\n\n\n\nProgrammatic random sampling is not truly random, but rather “pseudo-random.” This means that the numbers generated are determined by an initial value called a “seed”. If you use the same seed, you will get the same sequence of random numbers. This is useful for reproducibility in experiments and simulations.\nIf you don’t specify a seed, the random number generator (RNG) will use a default seed that is typically based on the current date and time, which means that you will get different results each time you run the code.\n\n\nThere are built-in functions in many programming languages, including Python, that allow us to sample from common probability distributions. For example, in Python’s NumPy library, we can use numpy.random module to sample from various distributions like uniform, normal, binomial, etc.\n\n\n\n\n\n\nNormal distribution\n\n\n\nThe normal distribution is one of the most commonly used probability distributions in statistics. It is useful for modeling lots of real-world data, especially when the data tends to cluster around a mean (or average) value. The normal distribution is defined by two parameters: the mean (average) and the standard deviation (which measures how spread out the data is around the mean).\n\n\nFor example, to sample 100 values from a normal distribution with mean 0 and standard deviation 1, you can use:\n\n\nCode\nrng = np.random.default_rng(seed=42)  # Create a random number generator with a fixed seed\nsamples = rng.normal(loc=0, scale=1, size=100)\nprint(\"Samples:\\n\", samples)\nplt.figure(figsize=(8, 5))\nplt.hist(samples, bins=10, density=True)\nplt.show()\n\n\nSamples:\n [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519 -1.30217951\n  0.1278404  -0.31624259 -0.01680116 -0.85304393  0.87939797  0.77779194\n  0.0660307   1.12724121  0.46750934 -0.85929246  0.36875078 -0.9588826\n  0.8784503  -0.04992591 -0.18486236 -0.68092954  1.22254134 -0.15452948\n -0.42832782 -0.35213355  0.53230919  0.36544406  0.41273261  0.430821\n  2.1416476  -0.40641502 -0.51224273 -0.81377273  0.61597942  1.12897229\n -0.11394746 -0.84015648 -0.82448122  0.65059279  0.74325417  0.54315427\n -0.66550971  0.23216132  0.11668581  0.2186886   0.87142878  0.22359555\n  0.67891356  0.06757907  0.2891194   0.63128823 -1.45715582 -0.31967122\n -0.47037265 -0.63887785 -0.27514225  1.49494131 -0.86583112  0.96827835\n -1.68286977 -0.33488503  0.16275307  0.58622233  0.71122658  0.79334724\n -0.34872507 -0.46235179  0.85797588 -0.19130432 -1.27568632 -1.13328721\n -0.91945229  0.49716074  0.14242574  0.69048535 -0.42725265  0.15853969\n  0.62559039 -0.30934654  0.45677524 -0.66192594 -0.36305385 -0.38173789\n -1.19583965  0.48697248 -0.46940234  0.01249412  0.48074666  0.44653118\n  0.66538511 -0.09848548 -0.42329831 -0.07971821 -1.68733443 -1.44711247\n -1.32269961 -0.99724683  0.39977423 -0.90547906]\n\n\n\n\n\n\n\n\n\nIf you have a dataset and you want to sample from it, you can use the numpy.random.choice function to randomly select elements from the dataset (with or without replacement). If your dataset is in a pandas DataFrame, you can also use the sample method to randomly select rows from the DataFrame.\n\n\nCode\n# Sampling without replacement\nrng = np.random.default_rng(seed=42)\nsubsample = rng.choice(samples, size=10, replace=False)\nprint(\"Sample:\\n\", subsample)\n\n# another way to sample; note RNG can be different in different packages\npd.DataFrame(samples, columns=[\"Sample\"]).sample(n=10, replace=False, random_state=42)\n\n\nSample:\n [-1.32269961 -1.13328721 -0.01680116 -1.68286977  0.54315427 -1.68733443\n  0.85797588 -0.85304393 -0.04992591 -0.36305385]\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n83\n-0.381738\n\n\n53\n-0.319671\n\n\n70\n-1.275686\n\n\n45\n0.218689\n\n\n44\n0.116686\n\n\n39\n0.650593\n\n\n22\n1.222541\n\n\n80\n0.456775\n\n\n10\n0.879398\n\n\n0\n0.304717\n\n\n\n\n\n\n\nIf you want to sample from a custom distribution, you can also use the numpy.random.choice function to sample from a list of values with specified probabilities.\nHere’s an example of how to sample 100 dice rolls with a rigged die that has a 50% chance of rolling a 6, and a 10% chance of rolling each of the other numbers (1-5):\n\n\nCode\npossible_rolls = [1, 2, 3, 4, 5, 6]\nprobabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\nrng.choice(possible_rolls, p=probabilities, size=100)\n\n\narray([4, 6, 6, 6, 5, 3, 6, 1, 6, 6, 6, 4, 6, 6, 6, 2, 5, 1, 2, 6, 6, 6,\n       4, 4, 5, 2, 2, 5, 3, 6, 5, 6, 6, 4, 6, 6, 4, 3, 6, 2, 2, 1, 6, 6,\n       6, 6, 5, 6, 2, 2, 6, 5, 6, 6, 6, 6, 6, 4, 1, 5, 3, 5, 6, 3, 1, 3,\n       3, 6, 6, 6, 6, 5, 6, 2, 1, 1, 6, 5, 2, 6, 2, 6, 5, 4, 4, 6, 4, 1,\n       2, 6, 6, 6, 3, 6, 6, 6, 5, 3, 1, 6])\n\n\n\nSimulating more complex processes\nSometimes real-world processes are complex, and the samples we take are not independent. The simplest version of non-independence is sampling without replacement.\nConsider dealing poker hands from a standard deck of cards. When you deal a hand, you draw cards one at a time, and each card drawn affects the next card that can be drawn (because you do not put the card back into the deck).\n\n\nCode\n# Make a deck of cards (Ace is 1, King is 13)\ndeck = np.arange(1, 14).repeat(4)  # 4 suits, each with cards 1 to 13\nprint(\"Deck of cards before shuffling:\\n\", deck)\ndeck = np.random.permutation(deck)\nprint(\"Deck of cards after shuffling:\\n\", deck)\n# deal 2 cards to each of 4 players\nrng = np.random.default_rng(seed=21)\n# Get flat indices of 8 cards from deck\n# we want to know exactly which cards are dealt\nchosen_indices = rng.choice(len(deck), size=8, replace=False)\nhands = deck[chosen_indices].reshape(4, 2)\nprint(\"Hands dealt to players:\\n\", hands)\n# remove the dealt cards from the deck\nremaining_deck = np.delete(deck, chosen_indices)\nprint(\"Remaining cards in the deck:\\n\", remaining_deck)\nboard = np.random.choice(remaining_deck, size=5, replace=False)\nprint(\"Community cards on the board:\\n\", board)\n\n\nDeck of cards before shuffling:\n [ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6\n  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12\n 13 13 13 13]\nDeck of cards after shuffling:\n [ 3  7 11  8 13  9  1  6  1 12  3  6  9  5 10  9  9  2 10  7  2 11  4  8\n  4 12  6 10  2 12  7  6  3 13  8  4  5  2  3 13  4  1  5  5  8 11 13 11\n 12 10  7  1]\nHands dealt to players:\n [[10 13]\n [ 4  5]\n [ 2 12]\n [ 4 10]]\nRemaining cards in the deck:\n [ 3  7 11  8  9  1  6  1 12  3  6  9 10  9  9  7  2 11  8  4 12  6 10  2\n  7  6  3 13  8  5  2  3 13  4  1  5  5  8 11 13 11 12  7  1]\nCommunity cards on the board:\n [ 2  7 11  7  8]\n\n\nBut it can get even more complex than that. In many real-world scenarios, the process of generating data involves multiple steps or conditions that affect the outcome.\nIn these cases simulation might not be as straightforward as sampling from a single distribution (which takes just one or two lines of code). We then tend to write loops that simulate the process step by step, keeping track of the state of things as we go along.\nLet’s consider an example of a musician busking for money in Rittenhouse Square. The musician’s earnings might depend on various factors like the weather and and the number of passersby. To keep it simple, let’s assume that the musician earns $3 for every passerby who stops to listen. Of course, not every passerby will stop – let’s pretend every passerby has the same 20% chance of stopping.\nThe musician might want to know how much money they can expect to earn in a day of busking. We can simulate this process by generating a random number of passersby and then calculating the earnings based on the stopping probability.\n\n\n\n\n\n\nPoisson distribution\n\n\n\nThe Poisson distribution is commonly used to model the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It assumes that the events occur independently and at a constant average rate. In our example, we can use the Poisson distribution to model the number of passersby in a given time period (e.g., one hour of busking).\n\n\n\n\nCode\nn_days = 5\n# simulate whether it rains each day\nrng = np.random.default_rng(seed=42)\nrain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\ntotal_earnings = 0 # initialize a variable to keep track of total earnings\nfor day in range(n_days):\n    # For each day, decide if it rains based on the probability\n    did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n    print(f\"Day {day + 1} ({rain_probabilities[day]:.2%} chance): {'Rain' if did_it_rain else 'No rain'}\")\n    # Based on the outcome, the number of passersby changes\n    if did_it_rain:\n        passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n    else:\n        passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n    print(f\"\\t Number of passersby: {passersby}\")\n    # Simulate the number who stop to listen to the busker\n    listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n    print(f\"\\t Number of listeners: {listeners}\")\n    # Compute the busker's daily earnings\n    earnings = 3 * listeners  # $3 per listener\n    print(f\"\\t Daily earnings: ${earnings}\")\n    print(\"-\" * 40)\n\n    total_earnings += earnings\n\nprint(f\"Total earnings over {n_days} days: ${total_earnings}\")\n\n\nDay 1 (54.18% chance): No rain\n     Number of passersby: 211\n     Number of listeners: 41\n     Daily earnings: $123\n----------------------------------------\nDay 2 (30.72% chance): No rain\n     Number of passersby: 226\n     Number of listeners: 54\n     Daily earnings: $162\n----------------------------------------\nDay 3 (60.10% chance): Rain\n     Number of passersby: 51\n     Number of listeners: 13\n     Daily earnings: $39\n----------------------------------------\nDay 4 (48.82% chance): Rain\n     Number of passersby: 56\n     Number of listeners: 17\n     Daily earnings: $51\n----------------------------------------\nDay 5 (6.59% chance): No rain\n     Number of passersby: 212\n     Number of listeners: 34\n     Daily earnings: $102\n----------------------------------------\nTotal earnings over 5 days: $477\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe key here is to re-use the logic of the weekly busking simulation in a loop that runs 1000 times. Each time we run the simulation, we get a different weekly outcome based on the random number generator. By averaging these outcomes, we can get a good estimate of the expected earnings over a week of busking.\nimport numpy as np\ndef busk_one_week(rng, n_days=7):\n    \"\"\"Simulate the earnings of a busker in Rittenhouse Square over the course of a week.\n    Args:\n        rng: A NumPy random number generator.\n        n_days: The number of days to simulate (default is 7).\n    Returns:\n        total_earnings: The total earnings over the week.\n    \"\"\"\n    rain_probabilities = rng.uniform(0., 0.7, size=n_days)\n\n    total_earnings = 0 # initialize a variable to keep track of total earnings\n    for day in range(n_days):\n        # For each day, decide if it rains based on the probability\n        did_it_rain = rng.binomial(n=1, p=rain_probabilities[day])\n        # Based on the outcome, the number of passersby changes\n        if did_it_rain:\n            passersby = rng.poisson(lam=50)  # fewer passersby when it rains\n        else:\n            passersby = rng.poisson(lam=200) # more passersby when it doesn't rain\n        # Simulate the number who stop to listen to the busker\n        listeners = rng.binomial(n=passersby, p=0.2)  # 20% of passersby stop\n        # Compute the busker's daily earnings\n        earnings = 3 * listeners  # $3 per listener\n\n        total_earnings += earnings\n\n    return total_earnings\n\nn_simulations = 1000\nrandom_seed = 33\n\nrng = np.random.default_rng(seed=random_seed)\n\n# Use the above function to simulate the expected earnings of a busker in Rittenhouse Square over the course of a week.\n# Run the simulation 1000 times and calculate the average earnings over 1000 simulations.\nearnings_by_week = [busk_one_week(rng) for _ in range(n_simulations)]\naverage_earnings = np.mean(earnings_by_week)\naverage_earnings",
    "crumbs": [
      "Home",
      "Lecture 03: Sampling and Simulation"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html",
    "href": "notebooks/lecture-01.html",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "",
    "text": "This lecture will be, intentionally, a bit of a whirlwind. That’s because with the advent of large language models (LLMs) like ChatGPT, Claude, Gemini, etc. knowing how to program in specific languages like Python is becoming less important. You don’t need that much practice or to focus on the syntax of a specific language.\nInstead, the important thing is to understand the core concepts involved in programming, which are largely universal across languages. This high-level understanding will allow you to use LLMs effectively to write code in any language, including Python. If you don’t understand the concepts, you won’t be able to identify when the LLM is making mistakes or producing suboptimal code.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#variables-and-types",
    "href": "notebooks/lecture-01.html#variables-and-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables and types",
    "text": "Variables and types\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\n\n\n\n\n\n\nFunctions act on variables\n\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable.\n\n\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations.\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger (int)\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat (float)\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString (str)\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList (list)\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary (dict)\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean (bool)\nTrue or False values, e.g., True, False\n\n\n\nWe will discuss a few important ones in more detail\n\n\n\n\n\n\nEverything is an object\n\n\n\nIn Python, everything is an object. This means that even basic data types like integers and strings are treated as objects with methods and properties. For example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\nSee the later section on Object-Oriented Programming for more details.\n\n\n\nTypecasting\nTypecasting is the process of converting a variable from one type to another. Not all type conversions are allowed, but some common ones include:\n\n\n\n\n\n\n\n\nFrom Type\nTo Type\nExample\n\n\n\n\nInteger\nFloat\nfloat(5) → 5.0\n\n\nFloat\nInteger\nint(3.14) → 3\n\n\nString\nInteger\nint('42') → 42\n\n\nString\nFloat\nfloat('3.14') → 3.14\n\n\nList\nString\n''.join(['S', 'h', 'a', 'l', 'o', 'm']) → 'Shalom'\n\n\nString\nList\nlist('Shalom') → ['S', 'h', 'a', 'l', 'o', 'm']\n\n\nBoolean\nInteger\nint(True) → 1, int(False) → 0\n\n\nInteger\nBoolean\nbool(1) → True, bool(0) → False, bool(-1) → True\n\n\n\nThere are also some implicit type conversions that happen automatically in Python, such as when you perform arithmetic operations between integers and floats. For example:\n\nx = 5        # Integer\ny = 2.0      # Float\nresult = x + y  # Implicit conversion to float\nresult, type(result)  # result is now a float\n\n(7.0, float)",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#lists",
    "href": "notebooks/lecture-01.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\nWe often need to store multiple values together. The most basic way to achieve this is with a list. A list is an ordered collection of items that can be of any type, including other lists. “Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\n1print(my_list[0])\n\n\n1\n\nA code annotation\n\n\n\n\n1\n\n\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\n[1, 2, 3]\n['apple', 'banana']\n[1, 3, 'banana']\n['banana', 'apple', 3, 2, 1]\n\n\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#arrays-numpy",
    "href": "notebooks/lecture-01.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\nPackages\n\n\n\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array().\n\n\n\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]\n\n\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]\n\n\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\nTypecasting works in NumPy arrays as well via the astype() method.\n\nnp.array([1, 2, 3, 4.1], dtype=float).astype(int)  # Convert float array to int array\n\narray([1, 2, 3, 4])\n\n\n\nAdvanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]\n\n\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [ 1  2  3  4  5  6  7  8  9 10]\nBoolean mask: [False False  True  True  True  True  True  True  True  True]\nFiltered array: [ 3  4  5  6  7  8  9 10]",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dictionaries",
    "href": "notebooks/lecture-01.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}\n\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25\n\n\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#dataframes",
    "href": "notebooks/lecture-01.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns). Think of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago\n\n\n\n\n\n\n\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#conditional-logic",
    "href": "notebooks/lecture-01.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false.\n\n\n\nHere’s an example of how to use these statements. Play around with the code below to see how it works. You can change the value of age to see how the output changes based on different conditions.\n\n\n\n\n\n\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false.\n\n\n\n\n\n\nBoolean expressions\n\n\n\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#loops",
    "href": "notebooks/lecture-01.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops.\n\nFor Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one.\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4. The enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n\n\n\n\nWhile Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#functions-and-functional-programming",
    "href": "notebooks/lecture-01.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result). You can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters. Here’s an example:\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\n1 Technically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing.Probably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]\n\n\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#object-oriented-programming",
    "href": "notebooks/lecture-01.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and \\ behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created.\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse\n\n\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality.\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\n\n\n\n\n\n\n\n\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-01.html#summary",
    "href": "notebooks/lecture-01.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems.",
    "crumbs": [
      "Home",
      "Lecture 01: Programming Basics"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Title: Understanding Uncertainty: An Introduction to Statistics through Data Generating Processes and Computational Simulations\nInstructor: Joey Rudoler\nClassroom: Academic Research Building (ARB), Room 401\nCourse Schedule:\n\nTentative schedule, subject to change.\n\n\nDate\nTopic\nAssignment\n\n\n\n\n7/09\nIntro, Why Statistics? (slides)\nAssignment 0\n\n\n7/10\nProgramming (slides)\n\n\n\n7/11\nProgramming cont.\n\n\n\n7/14\nProbability (slides)\n\n\n\n7/15\nProbability cont.\n\n\n\n7/16\nSampling and simulation (slides)\n\n\n\n7/17\nData Generating Processes and Statistical Models (slides)\n\n\n\n7/18\nHypothesis Testing (slides)\n\n\n\n7/21\nHypothesis Testing cont.\n\n\n\n7/22\nConfidence Intervals, bootstrapping (slides)\n\n\n\n7/23\nPermutation tests (slides)\n\n\n\n7/24\nRegression (slides)\n\n\n\n7/25\nRegression cont.\n\n\n\n7/28\nClassification\n\n\n\n7/29\nCrash course: Deep Learning and AI\n\n\n\n7/30\nAdvanced Data Visualization\n\n\n\n7/31\nParadoxes and Statistical Gotchas\n\n\n\n8/01\nGuest Lecture / Final Projects\n\n\n\n8/04\nGuest Lecture / Final Projects\n\n\n\n8/05\nGuest Lecture / Final Projects\n\n\n\n8/06\nGuest Lecture / Final Projects\n\n\n\n8/07\nFinal Project Presentations\n\n\n\n8/08\nWrap-up and course evaluation"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nDescription\nDue Date\n\n\n\n\nAssignment 0\n\n\n\n\nAssignment 1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "understanding-uncertainty",
    "section": "",
    "text": "This website contains course materials for the Understanding Uncertainty course, which is a part of the US-Israel Academic Bridge Fellowship hosted at the University of Pennsylvania.\n\n\n\nSomething from DALL-E’s imagination"
  },
  {
    "objectID": "notebooks/lecture-00.html",
    "href": "notebooks/lecture-00.html",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#welcome",
    "href": "notebooks/lecture-00.html#welcome",
    "title": "Lecture 00",
    "section": "",
    "text": "So begins “Understanding Uncertainty”, a course in statistical thinking and data science.\nThis is lecture 0. See the syllabus for an overview of the course.\nOur basic goals for the course are:\n\nto build a strong intuition about data, where it comes from, and what questions it can answer.\nto learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#why-statistics",
    "href": "notebooks/lecture-00.html#why-statistics",
    "title": "Lecture 00",
    "section": "Why statistics?",
    "text": "Why statistics?\nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#description",
    "href": "notebooks/lecture-00.html#description",
    "title": "Lecture 00",
    "section": "Description",
    "text": "Description\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand.\nLet’s load in some data and take a look at it.\nThe dataset contains Airbnb listings in New York City, including prices, locations, and other features.\n\n\nCode\n# from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.rename(columns={\"neighbourhood_group\": \"borough\"})\nairbnb = airbnb.dropna(subset=[\"borough\", \"price\", \"long\", \"lat\"])\nairbnb[\"borough\"] = airbnb[\"borough\"].str.lower()\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"manhatan\", \"manhattan\")\nairbnb[\"borough\"] = airbnb[\"borough\"].str.replace(\"brookln\", \"brooklyn\")\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n# print the first 5 rows\nairbnb[:5]\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_identity_verified\nhost_name\nborough\nneighbourhood\nlat\nlong\ncountry\n...\nservice_fee\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\nreview_rate_number\ncalculated_host_listings_count\navailability_365\nhouse_rules\nlicense\n\n\n\n\n0\n1001254\nClean & quiet apt home by the park\n80014485718\nunconfirmed\nMadaline\nbrooklyn\nKensington\n40.64749\n-73.97237\nUnited States\n...\n$193\n10.0\n9.0\n10/19/2021\n0.21\n4.0\n6.0\n286.0\nClean up and treat the home the way you'd like...\nNaN\n\n\n1\n1002102\nSkylit Midtown Castle\n52335172823\nverified\nJenna\nmanhattan\nMidtown\n40.75362\n-73.98377\nUnited States\n...\n$28\n30.0\n45.0\n5/21/2022\n0.38\n4.0\n2.0\n228.0\nPet friendly but please confirm with me if the...\nNaN\n\n\n2\n1002403\nTHE VILLAGE OF HARLEM....NEW YORK !\n78829239556\nNaN\nElise\nmanhattan\nHarlem\n40.80902\n-73.94190\nUnited States\n...\n$124\n3.0\n0.0\nNaN\nNaN\n5.0\n1.0\n352.0\nI encourage you to use my kitchen, cooking and...\nNaN\n\n\n3\n1002755\nNaN\n85098326012\nunconfirmed\nGarry\nbrooklyn\nClinton Hill\n40.68514\n-73.95976\nUnited States\n...\n$74\n30.0\n270.0\n7/5/2019\n4.64\n4.0\n1.0\n322.0\nNaN\nNaN\n\n\n4\n1003689\nEntire Apt: Spacious Studio/Loft by central park\n92037596077\nverified\nLyndon\nmanhattan\nEast Harlem\n40.79851\n-73.94399\nUnited States\n...\n$41\n10.0\n9.0\n11/19/2018\n0.10\n3.0\n1.0\n289.0\nPlease no smoking in the house, porch or on th...\nNaN\n\n\n\n\n5 rows × 26 columns\n\n\n\nNow there’s a lot you can do, but let’s start by visualizing the prices of listings.\n\n\nCode\n# plot a histogram of the price column\nplt.figure(figsize=(10, 5))\nplt.hist(airbnb['price'], bins=50)\nplt.title('Prices of Airbnb Listings in NYC')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\nCode\nairbnb[\"price\"].describe()\n\n\ncount    102316.000000\nmean        625.291665\nstd         331.677344\nmin          50.000000\n25%         340.000000\n50%         624.000000\n75%         913.000000\nmax        1200.000000\nName: price, dtype: float64\n\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the geopandas library to plot the locations of listings on a map of New York City.\n\n\nCode\nimport geopandas as gpd\nfrom geodatasets import get_path\n# load the shapefile of NYC neighborhoods\nnyc_neighborhoods = gpd.read_file(get_path('nybb'))\nnyc_neighborhoods = nyc_neighborhoods.to_crs(epsg=4326)  # convert to WGS84\n# plot the neighborhoods with airbnb listings\nnyc_neighborhoods.plot(figsize=(8, 8), color='white', edgecolor='black')\n# plot the airbnb listings on top of the neighborhoods\n# use the 'long' and 'lat' columns to create a GeoDataFrame\nairbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb['long'], airbnb['lat']), crs='EPSG:4326')\n# set the coordinate reference system to WGS84\nairbnb_gdf.plot(ax=plt.gca(), column=\"price\", markersize=3, alpha=0.05, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\nplt.title('Airbnb Listings in NYC')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics only describe the data.\nWhy is this limiting? After all, we like data – it tells us things about the world and it’s objective and quantifiable.\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\nLet’s look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small “sample” or subset of the data?\n\n\nCode\n# separately plot 3 samples of airbnb listings\nfig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(4):\n    # sample 1000 listings\n    sample = airbnb.sample(100, random_state=i)\n    # plot the neighborhoods with airbnb listings\n    nyc_neighborhoods.plot(ax=ax[i], color='white', edgecolor='black')\n    # plot the airbnb listings on top of the neighborhoods\n    # use the 'long' and 'lat' columns to create a GeoDataFrame\n    airbnb_gdf_sample = gpd.GeoDataFrame(sample, geometry=gpd.points_from_xy(sample['long'], sample['lat']), crs='EPSG:4326')\n    # set the coordinate reference system to WGS84\n    airbnb_gdf_sample.plot(ax=ax[i], column=\"price\", markersize=3, alpha=0.8, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})\n    ax[i].set_title(f'Airbnb Listings in NYC (Sample {i+1})\\n Average Price: ${sample[\"price\"].mean():.2f}')\n    ax[i].set_xlabel('Longitude')\n    ax[i].set_ylabel('Latitude')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the samples differ from one another. They have different geography and different prices. This means you can’t just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n\n\n\n\n\n\nSample vs. Population\n\n\n\n\n\nA population is the entire set of data that you are interested in. A sample is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.\nNote that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.\nOften, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.\n\n\n\nDescriptive statistics are useful for understanding the data at hand, but they don’t necessarily tell us much about the world outside of the data. For that, we need to do something more.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#inference",
    "href": "notebooks/lecture-00.html#inference",
    "title": "Lecture 00",
    "section": "Inference",
    "text": "Inference\nSo what if we want to answer questions about a population based on a sample? This is where inference comes in. Specifically, we want to use the given sample to infer something about the population.\nHow do we do this if we can’t ever see the entire population? The answer is that we need a link which connects the sample to the population – specifically, we can explicitly treat the sample as the outcome of a data-generating process (DGP).\n\n\n\n\n\n\nThere is always a DGP\n\n\n\n\n\nA data-generating process (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.\nThere has to be a DGP, even if we don’t know what it is. The DGP is the process that generates the data we observe.\nThe full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n\n\n\nOf course, we don’t necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\nThis is where the model comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess – a guess about where your data come from.\nFor example, we might assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. (So the probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.)\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n\n\n\nCode\n# number of listings per borough\nborough_counts = airbnb['borough'].value_counts().rename_axis('borough').reset_index(name='count')\nborough_counts['borough'] = borough_counts['borough'].str.title()  # capitalize\n# normalize the counts to proportions\nborough_counts['proportion'] = borough_counts['count'] / borough_counts['count'].sum()\n\nplt.figure(figsize=(8, 5))\nplt.bar(borough_counts['borough'], borough_counts['proportion'])\nplt.axhline(y=1/5, color='r', linestyle='--', label='Equal Proportion (20%)')\nplt.title('Proportion of Airbnb Listings per Borough')\nplt.xlabel('Borough', fontsize=14)\nplt.ylabel('Proportion', fontsize=14)\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe inference question we want to ask is roughly:\n“If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?”\nNotice that this is a question about the probability of the sample, given a certain model of the DGP. In our Airbnb example above, it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\nWhat should we do now? Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. After all, the model is just a “guess” about the DGP, while the sample is real data that we have observed.\n\n\n\n\n\n\nUnlikely data or unlikely model?\n\n\n\n\n\nThere are two main culprits when we see a sample that is unlikely under our model:\n\nThe sample! Think of this as “luck of the draw”. This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there’s actually a 3% chance of this happening); if you flip a coin 100 times, there’s virtually no chance that you’ll get all tails (less than 10-30 chance).\nThe model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won’t go away just by collecting more data.\n\n\n\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n\n\n\n\n\nDon’t try this at home!\n\n\n\n\n\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story.\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. This requires domain knowledge and subject matter expertise. In the Airbnb example, you would want to know something about the five boroughs of New York City before starting your analysis. Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption (Manhattan sees vastly more tourism than the other boroughs, and Brooklyn and Queens have by far the most residents according to recent census data).\nStatistical inference is a powerful tool, but it is not a substitute for understanding the data and the context in which it was collected. Modeling might be guesswork, but it is best if it is informed guesswork.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#prediction",
    "href": "notebooks/lecture-00.html#prediction",
    "title": "Lecture 00",
    "section": "Prediction",
    "text": "Prediction\nPrediction is the process of using a model to make predictions about unseen (or future) data.\nBack to the Airbnb data, we might want to predict which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\nTo that end we will fit a predictive model to the data. We won’t go into the details of the model we use here, but the basic idea is that we assume the features of the listing (e.g., price) is related to the probability of it being in a certain borough (perhaps more expensive listings are more likely to be in Manhattan, for example).\n\n\n\n\n\n\nFitting a model\n\n\n\n\n\nModels generally have parameters, which are adjustable values that affect the model’s behavior. Think of them like “knobs” you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker.\nThe model for a coin flip, for example, has a single parameter: the probability of landing on heads. If you turn the knob to 0.5, you get a fair coin; if you turn it to 1.0, you get a coin that always lands on heads; if you turn it to 0.0, you get a coin that always lands on tails.\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by minimizing some kind of error function, which provides a measure of how well the model fits the data.\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix\nimport seaborn as sns\n\n# prepare the data for linear regression\nfeature_columns = [\"price\", \"room_type\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"review_rate_number\"]\nairbnb_clean = airbnb.dropna(subset=feature_columns + [\"borough\"])\nX = airbnb_clean[feature_columns]\n# convert categorical variables to lowercase\nX.loc[:, \"room_type\"] = X[\"room_type\"].str.lower()\ny = airbnb_clean[\"borough\"].values.reshape(-1)\n# convert categorical variables to dummy variables\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\nX[\"rating_interaction\"] = X[\"reviews_per_month\"] * X[\"review_rate_number\"]\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n# fit the logistic regression model\nmodel = LogisticRegression(max_iter=1000, solver=\"newton-cholesky\")\nmodel.fit(X_train, y_train)\n# make predictions on the test set\ny_pred = model.predict(X_test)\n# calculate the prediction accuracy\naccuracy = np.mean(y_pred == y_test)\nprint(f\"{40*'='}\")\nprint(f'Prediction Accuracy: {accuracy:.2%}')\nprint(f\"{40*'='}\")\n# calculate the confusion matrix\nconfusion_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=airbnb['borough'].unique())\n# confusion_matrix = [confusion_matrix[i].T for i in range(len(confusion_matrix))] \nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\nax = ax.flatten()\nax[-1].axis('off')  # turn off the last subplot since we have only 5 boroughs\n# plot the confusion matrix for each borough\nfor i, borough in enumerate(airbnb['borough'].unique()):\n    sns.heatmap(confusion_matrix[i], annot=True, fmt='d', cmap='Blues', ax=ax[i], cbar=False)\n    ax[i].set_xlabel('Predicted', fontsize=10)\n    ax[i].set_ylabel('Actual', fontsize=10)\n    ax[i].set_xticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\n    ax[i].set_yticklabels(['Not ' + borough, borough], rotation=30, fontsize=10)\nplt.tight_layout()\n_ = plt.show()\n\n\n========================================\nPrediction Accuracy: 45.38%\n========================================\n\n\n\n\n\n\n\n\n\nOk, so the model is around 45% accurate at predicting the borough of a listing.\n\n\n\n\n\n\nWhat is a “good” prediction rate?\n\n\n\n\n\nFor discussion / reflection: What is a “good” prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. For now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. For example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team’s performance.\n\n\n\nNow let’s take a look at the distribution of the model’s predictions.\n\n\nCode\n# plot the counts of predicted listings per borough side by side with the actual counts\nimport seaborn as sns\nplot_df = pd.DataFrame({\n    'predicted': pd.Series(y_pred.flatten()),\n    'actual': pd.Series(y_test.flatten())\n})\nplot_df = plot_df.melt(var_name='type', value_name='borough')\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.countplot(x='borough', hue='type', data=plot_df, ax=ax, alpha=0.7)\nsns.move_legend(ax, loc=\"center right\", title=None)\nplt.title('Predicted vs Actual Airbnb Listings per Borough')\nplt.xlabel('Borough')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n\n\n\n\n\nPrediction and inference can interact\n\n\n\n\n\nPrediction and inference are closely related, and they can often be done simultaneously.\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-00.html#summary",
    "href": "notebooks/lecture-00.html#summary",
    "title": "Lecture 00",
    "section": "Summary",
    "text": "Summary\nIn this introductory lecture we talked about the 3 objectives of data analysis: description, inference, and prediction. Hopefully you now have a better understanding of what statistics is supposed to help you do with data. Of course, we haven’t actually gone into any of the details of how to do anything. Don’t worry, we’ll get there!\nNext up, we’ll cover some of the basic programming concepts that are important for data science. After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. From there, the sky is the limit! We’ll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\nSince we haven’t learned any programming or statistics yet, we won’t have any real exercises for this lecture. There’s just a quick Assignment 0 to make sure you are set up to run Python code for future assignments.",
    "crumbs": [
      "Home",
      "Lecture 00: Why Statistics?"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html",
    "href": "notebooks/lecture-02.html",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n\\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability",
    "href": "notebooks/lecture-02.html#probability",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nMost of you are probably familiar with the basic intuition of probability: essentially it measures how likely an event is to occur.\nIn mathematical terms, the probability \\(\\P\\) of an event \\(A\\) is defined as:\n\\[\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\\]\nBy definition this quantity cannot be negative (\\(\\P(A) = 0\\) means \\(A\\) never occurs), and it must be less than or equal to 1 (\\(\\P(A) = 1\\) means \\(A\\) always occurs).\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads (\\(H\\)) and tails (\\(T\\)). If we let \\(A\\) be the event that the coin lands on heads, then we can compute the probability of \\(A\\) as follows:\n\\[\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\\]\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say “there’s a 100% chance that something will happen”.\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events \\(H\\) and \\(T\\) cover 100% of the possible outcomes. So we can write: \\[\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\\]\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event \\(A\\), the event either happens or it doesn’t. So we can compute the probability of the event not occurring as: \\[\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\\]\n\nProbability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\).\nIf we let \\(B\\) be the event that at least one coin lands on heads, we can compute the probability of \\(B\\) as follows: \\[\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\\]\n\n\nAddition and multiplication rules (and / or)\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event \\(C = \\{HH\\}\\))?\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that \\(\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}\\).\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as \\(B\\) above, so we can use the same calculation: \\(\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}\\).\n\n\n\n\n\n\nNote on notation\n\n\n\n\n\nIn the above, we used \\(H_1\\) and \\(H_2\\) to denote heads on the first and second flips, respectively. The notation \\(H_1 ~\\text{and}~ H_2\\) means both flips are heads, while \\(H_1 ~\\text{or}~ H_2\\) means at least one flip is heads.\nIn probability theory, we often use the symbols \\(\\cap\\) and \\(\\cup\\) to denote “and” and “or” respectively. So we could also write \\(\\P(H_1 \\cap H_2)\\) for the probability of both flips being heads, and \\(\\P(H_1 \\cup H_2)\\) for the probability of at least one flip being heads. Technically, this is set notation where \\(\\cap\\) means intersection (the event where both \\(H_1\\) and \\(H_2\\) occur), while \\(\\cup\\) means union (the event where either \\(H_1\\) or \\(H_2\\) occurs).\n\n\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events \\(A\\) and \\(B\\), the following rules hold:\n\nAddition rule: For any two events \\(A\\) and \\(B\\), the probability of either \\(A\\) or \\(B\\) occurring is given by: \\[\n\\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n\\] This last term, \\(\\P(A \\cap B)\\), is necessary to avoid double counting the outcomes where both \\(A\\) and \\(B\\) occur.\nNote that if \\(A\\) and \\(B\\) are mutually exclusive (i.e., they cannot both occur at the same time), then \\(\\P(A \\cap B) = 0\\), and the formula simplifies to: \\[  \\P(A \\cup B) = \\P(A) + \\P(B)\\]\n\n\n\n\n\n\n\nVisualizing sets of events\n\n\n\n\n\nThe following image illustrates the addition rule for two events \\(A\\) and \\(B\\) using a Venn diagram. \n\n\n\n\nMultiplication rule: For any two events \\(A\\) and \\(B\\), the probability of both \\(A\\) and \\(B\\) occurring is given by: \\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)\\] where \\(\\P(B | A)\\) is the conditional probability of \\(B\\) given that \\(A\\) has occurred. This means you first consider the outcomes where \\(A\\) occurs, and then look at the probability of \\(B\\) within that subset.\n\n\n\n\n\n\n\nConditional probability\n\n\n\n\n\nThe notation \\(\\P(B | A)\\) is read as “the probability of \\(B\\) given \\(A\\)”. It represents the probability of event \\(B\\) occurring under the condition that event \\(A\\) has already occurred.\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event \\(A\\) is “it is hot outside”, and the event \\(B\\) is “I buy ice cream”. The conditional probability \\(\\P(B | A)\\) would be higher than \\(\\P(B)\\) on a typical day.\nLet’s think about this in the context of our coin flips. If we know that the first flip is heads (\\(H_1\\)), then only two outcomes are possible (\\(HH\\) and \\(HT\\)) instead of four (\\(HH\\), \\(HT\\), \\(TH\\), \\(TT\\)).\nSo the conditional probability \\(\\P(H_2 | H_1)\\), which is the probability of the second flip being heads given that the first flip was heads, is: \\[\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\\]\n\n\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, “What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)”\n\n\n\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n\nIndependence\nTwo events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of the other.\nHow does this relate to the multiplication rule? If \\(A\\) and \\(B\\) are independent, then the conditional probability \\(\\P(B | A)\\) is simply \\(\\P(B)\\). That is, knowing that \\(A\\) has occurred does not change the probability of \\(B\\) occurring.\nThis means that for independent events, the multiplication rule simplifies to:\n\\[\\P(A \\cap B) = \\P(A) \\cdot \\P(B)\\]\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply \\(\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\).\n\n\nComplicated counting\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes.\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let’s think about the case where the first flip is heads. The probability of getting a head on the first flip is \\(\\frac{1}{2}\\), and the probability of getting tails on the 4 other flips is also \\(\\frac{1}{2}\\) each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips: \\[\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}\\]\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting “exactly one head”. For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions: \\[\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16\\]\nThere are two common types of outcomes we want to count: permutations and combinations.\nA combination is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are choosing an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\\[\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nIt is clear here that there are 5 possible “slots” where we can place a head.\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still applies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\nLet’s take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices. \\[\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n\\]\nNow let’s consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\\[\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n\\]\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads. This gives us a total of \\(5 \\cdot 4 = 20\\) ways to choose 2 flips to be heads out of 5 flips.\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order – where flip 1 was heads first).\nThis illustrates the key distinction between combinations and permutations. A permutation is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. If you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting.\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign “slots” in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length \\(n\\), the number of permutations is: \\[\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n\\]\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\nNamely, the number of combinations of \\(k\\) items from a set of \\(n\\) items is given by the formula: \\[\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n\\]\nThis formula counts all of the possible permutations of the sequence, and then divides by the number of ways to arrange the \\(k\\) items that are selected (which is \\(k!\\)) and the number of ways to arrange the remaining \\(n-k\\) items (which is \\((n-k)!\\)).\nIn our example, we have \\(n = 5\\) (the number of flips) and \\(k = 2\\) (the number of heads). So the number of combinations of 2 heads from 5 flips is: \\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-functions",
    "href": "notebooks/lecture-02.html#probability-functions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability functions",
    "text": "Probability functions\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck.\nHowever, it is often more convenient to work with probability functions. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] where \\(x\\) is the outcome of the coin flip (0 for heads, 1 for tails).\n\n\n\n\n\n\nFunctions map inputs to outputs\n\n\n\n\n\nFunctions are just a “map” that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range \\([0, 1]\\).\n\n\n\nThis might seem a bit redundant because we’re just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values.\nFor example, if we have a die with six sides, we can define a probability function \\(f\\) as follows: \\[\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with \\(k\\) sides, we can define a probability function \\(f\\) as follows:\n\\[\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\] This is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n\n\n\n\n\n\n\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0\n\ndef f(x, k):\n    if 1 &lt;= x &lt;= k:\n        return 1 / k\n    else:\n        return 0",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#random-variables",
    "href": "notebooks/lecture-02.html#random-variables",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like \\(X\\) or \\(Y\\). The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like \\(x\\) or \\(y\\).\nWe use probability functions to describe the probabilities associated with random variables. Specifically, a probability function \\(f\\) for a random variable \\(X\\) gives the probability that \\(X\\) takes on a specific value \\(x\\).\nFor example, let \\(X\\) be a random variable that represents the outcome of flipping a fair coin. The probability function for \\(X\\) would be: \\[\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nBernoulli random variable\n\n\n\n\n\nThe above is an example of a Bernoulli random variable, which takes on the value 1 with probability \\(p\\) and the value 0 with probability \\(1 - p\\). In our case, \\(p = \\frac{1}{2}\\) for a fair coin.\n\n\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let \\(Y\\) be a random variable that represents the height of a person in centimeters. Let’s assume that every person’s height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for \\(Y\\) would be: \\[\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\nUniform random variable\n\n\n\n\n\nThe above is an example of a uniform random variable, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is \\(\\frac{1}{50}\\).\n\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "href": "notebooks/lecture-02.html#probability-distributions-and-histograms",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nWe call the probability function for a random variable a probability distribution, which describes how the probabilities are distributed across the possible values of the random variable.\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a probability mass function (PMF), which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a probability density function (PDF), which gives the density of probability at each point.\nLet’s say we have a random variable \\(X\\), but we don’t know the exact probability function. Instead, we have a set of observed data points \\(\\{x_1, x_2, \\ldots, x_n\\}\\) that we believe are individual realizations of \\(X\\). In other words, we have a sample of data that we think is representative of the underlying random variable.\nHow can we visualize this data to understand the distribution of \\(X\\)? The simplest solution is to just plot how many times each value occurs in the data. We can use a bar chart to visualize the counts of each value.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([0, 0, 1, 1, 0])\n\nplt.figure(figsize=(8, 5))\nplt.hist(x, bins=np.arange(-0.5, 2.5, 1), density=False, align=\"mid\", rwidth=0.8)\nplt.xticks([0, 1])\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let’s check it out:\n\n# load in the dice rolls data\ndice_rolls_df = pd.read_csv(\"../data/dice_rolls.csv\")\nprint(\"Total number of dice rolls:\", len(dice_rolls_df))\ndice_rolls_df.head(10)\n\nTotal number of dice rolls: 1000\n\n\n\n\n\n\n\n\n\nrolls\n\n\n\n\n0\n1\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n3\n\n\n5\n6\n\n\n6\n1\n\n\n7\n5\n\n\n8\n2\n\n\n9\n1\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(dice_rolls_df['rolls'], bins=np.arange(0.5, 7.5, 1), density=True, rwidth=0.8)\nplt.title('Histogram of Dice Rolls')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.xticks(np.arange(1, 7))\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\n\n\n\n\n\n\nNotice that when the \\(y\\)-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a histogram to visualize the distribution of the data.\nThe histogram is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or “bins”) along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins.\n\ndf = pd.read_csv(\"../data/airbnb.csv\")\n# import the data on Airbnb listings in the New York City\nairbnb = pd.read_csv(\"../data/airbnb.csv\")\n# data cleaning\nairbnb = airbnb.dropna(subset=[\"price\"])\n# format the price column\nairbnb['price'] = airbnb['price'].replace({'\\\\$': '', ',': ''}, regex=True).astype(float)\n\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb['price'], bins=50, stat=\"proportion\", edgecolor='black')\nplt.title('Histogram of Airbnb Prices')\nplt.xlabel('Price (USD)')\nplt.ylabel('Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea under a probability distribution\n\n\n\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. This is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1.\nFor discrete: \\[ \\sum_{x} f(x) = 1 \\] For continuous: \\[ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\]\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable \\(X\\) falls between \\(a\\) and \\(b\\), we can compute the area under the PDF from \\(a\\) to \\(b\\): \\[ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx \\]",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#expectation",
    "href": "notebooks/lecture-02.html#expectation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Expectation",
    "text": "Expectation\nWe are often interested in the average value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play.\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you’re not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero.\n\n\n\n\n\n\nGambling warning: the house always wins\n\n\n\n\n\nActually, at real casinos, the games are designed so that “the house always wins” in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance – they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\nIn the short term this is hardly noticeable – you’re actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n\n\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\nThe expectation (or expected value) of a random variable \\(X\\) is gives the average value of \\(X\\) over many instances. It is denoted as \\(\\mathbb{E}[X]\\) or \\(\\mu_X\\). The expectation is calculated as follows: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n\\] where \\(f(x)\\) is the probability function of \\(X\\). For continuous random variables, the sum is replaced with an integral: \\[\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nThe way to think about this is that the expectation is a weighted average of all possible values of \\(X\\), where the weights are the probabilities of each value.\nSo in our roulette example, you can either lose $1 (with 90% probability) or win $9 (with 10% probability). The expectation would be: \\[\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win $10 on one game and lose $1 on the other 9 games. The average outcome is: \\[\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n\\]\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as \\(\\bar{X}\\), and referred to as the sample mean. \\[\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nThis approximation becomes more accurate as the number of samples \\(n\\) increases. We will talk about this more in a future lecture.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "href": "notebooks/lecture-02.html#variance-and-standard-deviation",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe average is a useful summary of a random variable’s central tendency, but it does not tell us anything about how spread out the values are.\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game – the average amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\n\n\n\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can’t afford to lose all of it on a single game.\nWe need a statistic that captures the typical distance between the values of the random variable and the average value.\n\n\n\n\n\n\nWhy distance from the average?\n\n\n\n\n\nLet’s imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what – the only question is how much you win. Let’s take an example where the payouts still differ by $10: you get $5 if you “lose” and $15 if you “win”.\nIn this case, the expected value is: \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n\\] So you can expect to win $6 per game on average.\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\nThe answer is simple: we subtract the average from each value of \\(X\\): \\[X' = X - \\mathbb{E}[X]\\] This gives us a new random variable \\(X'\\) that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\\[\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0\\]\nor more generally: \\[\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0\\]\n\n\n\nSo let’s compute exactly that - the distance from the average. The formula for distance between two vectors \\(x\\) and \\(y\\) is: \\[\nd^2 = \\sum_{i} (x_i - y_i)^2\n\\] where \\(x_i\\) and \\(y_i\\) are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange (\\(a^2 + b^2 = c^2\\)).\nIn our case, we want to compute the distance between the values of the random variable \\(X\\) and the average value \\(\\mathbb{E}[X]\\). \\[\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n\\]\nNow we’re getting somewhere! However, this is adding up all of the squared distances – that means that the more values we have, the larger the distance will be. This is not quite right – instead we want to compute the average distance from the mean in order to get a sense of how spread out the values typically are.\nSo we need to divide by the number of values: \\[\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n\\]\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. If we replace the average with the expectation, we get the formula for the variance of a random variable \\(X\\): \\[\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n\\]\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if \\(X\\) represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the standard deviation:\n\\[\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n\\]\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset: \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\]\n\n\n\n\n\n\nSample variance vs. population variance\n\n\n\n\n\nTechnically, the formula above is an imperfect estimate of the population standard deviation. It’s in general a little bit too small, because the sample mean \\(\\bar{X}\\) does not perfectly represent the population mean \\(\\mathbb{E}[X]\\). We can correct for this by dividing by \\(n-1\\) instead of \\(n\\): \\[\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n\\] This is called the sample standard deviation.\nWhy is the initial estimate too small? In a small dataset, the sample mean “overfits” the data, meaning it is closer to the individual data points than the true population mean. Let’s think about this in terms of coin flips. If we flip a coin once, the sample mean is either \\(\\hat{X}=0\\) or 1, depending on whether we got heads or tails. But the true population mean is \\(\\mathbb{E}[X]=\\frac{1}{2}\\). If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either \\((1-1)^2\\) or \\((0-0)^2\\)).\nBy contrast, the true (population) standard deviation is \\(\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\\), which is larger than the estimate using the sample mean.\nThis bias in computing the standard deviation gets smaller as the sample size \\(n\\) increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the \\(n-1\\) correction to get a more accurate estimate of the population standard deviation.",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#common-probability-distributions",
    "href": "notebooks/lecture-02.html#common-probability-distributions",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Common probability distributions",
    "text": "Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. It is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nProbability Function\nParameters\nMean\nVariance\n\n\n\n\nBernoulli\nDiscrete\n\\(f(x) = p^x (1-p)^{1-x}\\)\n\\(p \\in [0, 1]\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\nDiscrete\n\\(f(x) = \\binom{n}{x} p^x (1-p)^{n-x}\\)\n\\(n \\in \\mathbb{N}, p \\in [0, 1]\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nPoisson\nDiscrete\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\nUniform\nContinuous\n\\(f(x) = \\frac{1}{b-a}\\)\n\\(a &lt; b\\)\n\\(\\frac{a+b}{2}\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\n\nNormal\nContinuous\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\nExponential\nContinuous\n\\(f(x) = \\lambda e^{-\\lambda x}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\n\nThe plots below show the probability functions for each of these distributions.\n\nfig, ax = plt.subplots(3, 2, figsize=(12, 12), sharex=False, sharey=True)\nax = ax.flatten()\n# Bernoulli Distribution\nx = np.arange(0, 2)\np = 0.5\nax[0].bar(x, stats.bernoulli.pmf(x, p), width=0.4, color='blue', alpha=0.7)\nax[0].set_title('Bernoulli Distribution (p=0.5)')\nax[0].set_xticks(x)\nax[0].set_xticklabels(['0', '1'])\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Probability')\n\n# Binomial Distribution\nn = 10\nx = np.arange(0, n + 1)\np = 0.5\nax[1].bar(x, stats.binom.pmf(x, n, p), width=0.4, color='green', alpha=0.7)\nax[1].set_title('Binomial Distribution (n=10, p=0.5)')\nax[1].set_xticks(x)\nax[1].set_xlabel('Number of Successes')\nax[1].set_ylabel('Probability')\n\n# Poisson Distribution\nlambda_ = 5\nx = np.arange(0, 15)\nax[2].bar(x, stats.poisson.pmf(x, lambda_), width=0.4, color='orange', alpha=0.7)\nax[2].set_title('Poisson Distribution (λ=5)')\nax[2].set_xticks(x)\nax[2].set_xlabel('Number of Events')\nax[2].set_ylabel('Probability') \n\n# Uniform Distribution\na, b = 0, 10\nx = np.linspace(a, b, 1000)\nax[3].plot(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan')\nax[3].set_title('Uniform Distribution (a=0, b=10)')\nax[3].set_xlabel('Value')\nax[3].set_ylabel('Probability Density')\nax[3].fill_between(x, stats.uniform.pdf(x, loc=a, scale=b-a), color='cyan', alpha=0.2)\n\n# Normal Distribution\nmu, sigma = 0, 1\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nax[4].plot(x, stats.norm.pdf(x, mu, sigma), color='purple')\nax[4].set_title('Normal Distribution (μ=0, σ=1)')\nax[4].set_xlabel('Value')\nax[4].set_ylabel('Probability Density')\nax[4].fill_between(x, stats.norm.pdf(x, mu, sigma), color='purple', alpha=0.2)\n\n# Exponential Distribution\nlambda_ = 1\nx = np.linspace(0, 10, 1000)\nax[5].plot(x, stats.expon.pdf(x, scale=1/lambda_), color='red')\nax[5].set_title('Exponential Distribution (λ=1)')\nax[5].set_xlabel('Value')\nax[5].set_ylabel('Probability Density')\nax[5].fill_between(x, stats.expon.pdf(x, scale=1/lambda_), color='red', alpha=0.2)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-02.html#summary",
    "href": "notebooks/lecture-02.html#summary",
    "title": "Lecture 02: Probability and Random Variables",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. Probability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\nIn particular, we covered:\n\nThe basic definition of probability and how to compute it for simple events.\nThe addition and multiplication rules for calculating probabilities of multiple events.\nThe concept of independence and how it affects probabilities.\nRandom variables and their probability distributions\nThe expectation (or expected value) of a random variable\nVariance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!",
    "crumbs": [
      "Home",
      "Lecture 02: Probability and Random Variables"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html",
    "href": "notebooks/lecture-04.html",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#data-generating-processes",
    "href": "notebooks/lecture-04.html#data-generating-processes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "",
    "text": "In the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example).\nThis leads us to a useful way of thinking about data: all data is generated by some underlying process. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! “Some process” is a bit of a catch-all: of course the data doesn’t just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about data generating processes (DGPs) is the key to analyzing data.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#statistical-models",
    "href": "notebooks/lecture-04.html#statistical-models",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\nLet’s return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a Bernoulli distribution, which describes the probability of each outcome. Specifically,\n\\[P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}\\]\nwhere (X) is the outcome of the coin flip, (p) is the probability of heads, and (1 - p) is the probability of tails. If we assume a fair coin, then (p = 0.5).\nNow clearly, this model does not capture the complexity of a real-world coin flip, which is of course influenced by many factors such as the weight of the coin, the force of the flip, air resistance, etc. Statistical models are always reductive in this sense. But the important thing is that a Bernoulli distribution really does do a good job of describing the outcomes of a coin flip. As long as that is the case, we can use the model to make predictions about the data generated by the process.\nTo see this, consider the following process: you roll a fair die 100 times. For each roll, you record whether the number you rolled was even or odd. Since there are three even numbers (2, 4, 6) and three odd numbers (1, 3, 5), and each number has an equal probability of being rolled, the probability of rolling an even number is 0.5 and the probability of rolling an odd number is also 0.5. This is statistically identical to flipping a fair coin! So we can use the same Bernoulli distribution to model the outcomes of this process, even though it completely ignore the details of the die rolling process itself. We lose information about the specific numbers rolled, but as long as we accurately capture the probability of the outcomes we care about, it doesn’t matter.\n\n\n\nCode\ndef dice_even_odd(n=1000):\n    \"\"\"\n    Simulate rolling a die n times and return the results.\n    \n    Parameters:\n    n (int): Number of rolls to simulate.\n    \n    Returns:\n    pd.DataFrame: DataFrame with columns 'roll' and 'even_odd'.\n    \"\"\"\n    rolls = np.random.randint(1, 7, size=n)\n    is_even = rolls % 2 == 0\n    return pd.DataFrame({\"roll\": rolls, \"is_even\": is_even, \"even_odd\": np.where(is_even, \"even\", \"odd\")})\n\ndice_rolls = dice_even_odd(1000)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nsns.histplot(dice_rolls[\"roll\"], bins=np.arange(1, 8) - 0.5, discrete=True, ax=ax[0])\nax[0].set_xlabel(\"Die Roll\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Die Rolls\")\nax[0].set_xticks(np.arange(1, 7))\nax[0].grid(axis='y')\n\nsns.histplot(dice_rolls[\"even_odd\"], discrete=True, shrink=0.8, ax=ax[1])\nax[1].set_xlabel(\"Even or Odd\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Even and Odd Rolls\")\nax[1].grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\n\nStatistical modeling generally uses this kind of trick. We attempt to find a simple model that captures the essential features of the data generating process.\nWe don’t always know as much about the data generating process as we do for a coin flip or a die roll. In fact, often we don’t know anything about it at all! In these cases, the approach is a little more “guess and check”. We start with a simple model, and see how well it describes the data. If it doesn’t work, we try a more complex model or a different model altogether.\n\nChallenges with finite samples\nIt can be hard to tell if you have a good model or not. With any finite dataset, it is quite likely that the data will not perfectly match the model. This is because the model describes probabilistic tendencies of the data. But in any small sample we will likely see deviations from the idealized outcomes described by the model.\nThink about this in the extreme: if you flip a coin only once, you will either get heads or tails. So your observed proportion of heads will be either 0 or 1, which is very different from the expected proportion of 0.5. In fact it is impossible to observe the expected proportion of heads in a single flip! This is true for a single observation of basically any random variable that takes on more than one value – it is close to impossible to learn anything about the variable’s tendencies from just one observation.\nEven with a small number of observations, it is quite difficult to tell if a model is a good fit for the data.\nLet’s say you and your roommate are always arguing about who should take out the trash. You decide to flip a coin to decide who takes it out (you are notorious for always choosing tails, because you think it’s good luck). You decide to do a best-of-ten series, so you flip the coin 10 times and record the number of heads. It turns out that you get 3 heads and 7 tails. Your roommate is furious: “That’s not fair! You always get tails! I bet you rigged the coin!”\nIs your roommate justified in being suspicious? What is the probability of getting 3 heads in 10 flips of a fair coin? What is the probability of getting 3 heads in 10 flips of a biased coin that has a 25% chance of landing on heads?\n\n\n\n\n\n\n\n\nAs this exercise shows, with a small number of observations you have some information that can help you distinguish between the two models, but it is not enough to be confident in your conclusion.\nWith more data though, suddenly the evidence becomes much stronger. The probability of getting a badly imbalanced distribution of heads and tails becomes tiny as the number of flips increases.\nIn general this is great news – the more data you have, the more confident you can be in your conclusions. It’s probably not worth flipping a coin 1000 times to decide who takes out the trash, but if you did, you would be able to tell with much more certainty whether the coin is fair or not.",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "href": "notebooks/lecture-04.html#convergence-for-large-sample-sizes",
    "title": "Lecture 04: Data Generating Processes and Statistical Models",
    "section": "Convergence for large sample sizes",
    "text": "Convergence for large sample sizes\nWhen the size of a dataset is large enough, we get some guarantees.\n\nLaw of Large Numbers\nThe Law of Large Numbers (LLN) states that as the sample size increases, the sample mean will converge to the population mean. In other words, if we take enough samples, the average of those samples will be close to the true average of the population.\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with expected value \\(\\mathbb{E}[X]\\), then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges to \\(\\mathbb{E}[X]\\) as \\(n\\) approaches infinity: \\[\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1\\]\nPrecisely, this states that the probability that the sample mean converges to the population mean is 1. 1\n\n\n\n1  This is technically the Strong Law of Large Numbers, stating that the sample mean converges almost surely to the population mean. There is also a Weak Law of Large Numbers which states that the sample mean converges in probability to the population mean, which is a weaker condition.You don’t just have to take this for granted – let’s see the LLN in action.\nWe will simulate flipping a biased coin \\(n\\) times, and plot the proportion of heads as we increase the number of flips. We will see that the proportion converges to 0.25 as the number of flips increases.\n\n\nCode\n# set seed\nnp.random.seed(56)\n# flip a fair coin n times\nsamples_sizes = [10, 100, 1000, 1e4, 1e5, 1e6]\nproportion_heads = []\nfor n in samples_sizes:\n    # Simulate flipping a fair coin n times\n    flips = np.random.binomial(1, 0.25, size=int(n))\n    proportion_heads.append(np.mean(flips))\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.lineplot(x=samples_sizes, y=proportion_heads, marker='o', ax=ax)\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nax.set_title(\"Proportion of Heads in Coin Flips\")\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s just one experiment (flip \\(n\\) coins a single time). But this is a random process, so we can simulate it many times and see how the proportion of heads changes.\n\n\nCode\nnp.random.seed(56)\nsamples_sizes = [10, 100, 1000, 1e4, 1e5]\nsim_results = []\n\nfor n in samples_sizes: # outer loop is over sample size\n    for _ in range(1000):  # inner loop runs the simulation 1000 times, for each sample size\n        # sample n flips from a fair coin\n        flips = np.random.binomial(1, 0.25, size=int(n))\n        # calculate the proportion of heads\n        proportion_heads = np.mean(flips)\n        # store the result\n        sim_results.append(pd.Series({\"n\": n, \"proportion_heads\": proportion_heads}))\n# gather all the results (a list of Series) into a DataFrame\nsim_results = pd.concat(sim_results, axis=1).T\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# plot the proportion from individual simulations as points\nsns.stripplot(data=sim_results, \n              x=\"n\", y=\"proportion_heads\", \n              jitter=False, alpha=0.3, ax=ax, native_scale=True, marker='o', size=4)\n# plot the mean proportion of heads across simulations\nsns.lineplot(data=sim_results, \n             x=\"n\", y=\"proportion_heads\", \n             errorbar=\"sd\", marker='o', ax=ax, \n             label='Mean Proportion of Heads')\n# make ticks nicer\nax.set_xticks([10, 100, 1000, 10000, 100000])\nax.set_xscale('log')\nax.set_xlabel(\"Number of Flips (log scale)\")\nax.set_ylabel(\"Proportion of Heads\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# histogram of the proportion of heads for each sample size\nsns.FacetGrid(sim_results, col=\"n\", col_wrap=2, height=4, aspect=1.2, sharex=False, sharey=False) \\\n    .map(sns.histplot, \"proportion_heads\", bins=np.linspace(0, 1, 41)) \\\n    .set_axis_labels(\"Proportion of Heads\", \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the variability across simulations decreases as the number of flips increases. This is what we mean when we say that the sample mean converges to the population mean. It’s not just close to the true mean on average, it is also more consistent across different samples/simulations.\nWe’re going to take advantage of this fact all the time. Basically any time you compare groups, or competing hypotheses, or evaluate predictive models, you end up taking the average of some quantity. The Law of Large Numbers tells us that as the sample size increases, the average will converge to the true value.\nAgain, what does this mean for your argument with your roommate? As you gather more and more data (coin flips), the proportion of heads (which is itself a sample mean) will converge to the true proportion of heads (the population mean). With enough flips, it becomes very clear whether the coin is rigged or not.\n\n\n\n\n\n\nUnbiased Estimators\n\n\n\nNotice in the simulation above that even when the sample mean was highly variable in small samples, it was always still centered around the true population mean. This is a crucial property for any estimator: it is said to be unbiased if the expected value of the estimator is equal to the true value of the parameter being estimated. In other words, on average, the estimator will give you the correct answer.\nThe LLN gives you a different guarantee: that as the sample size increases, the variance of the estimator decreases, so it becomes more and more likely that the estimator will be close to the true value.\n\n\n\n\nCentral Limit Theorem\nThe Central Limit Theorem states that the sample mean of \\(n\\) independent and identically distributed random variables will converge to a normal distribution, regardless of the distribution of the individual variables. This means that even if a variable is not normally distributed, if you average a bunch of them together, the sample mean will be approximately normally distributed.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then the sample mean \\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) converges in distribution to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity: \\[\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\n\n\n\nConsider rolling a die – there is an equal chance (1/6) of getting each of the values between 1 and 6.\nNote that the expected value of each roll of the die is 3.5 – we can compute this as \\[E\\left[X\\right]=1\\left(\\frac{1}{6}\\right)+2\\left(\\frac{1}{6}\\right)+3\\left(\\frac{1}{6}\\right)+4\\left(\\frac{1}{6}\\right)+5\\left(\\frac{1}{6}\\right)+6\\left(\\frac{1}{6}\\right)=3.5\\]\nSo the mean of the distribution of dice values should be 3.5, but as you can see below it’s definitely not normal – values are distributed evenly from 1 to 6 rather than clustering closer to the mean.\n\n\nCode\n# plot histogram of 100 rolls of a die\nnp.random.seed(42)  # for reproducibility\nrolls = pd.DataFrame({'roll': np.random.randint(1, 7, 100)})\n\nsns.histplot(data=rolls, x='roll', bins=6, discrete=True, edgecolor=\"black\", stat='proportion')\nplt.axvline(x=rolls['roll'].mean(), color='red', linestyle='--', label='Mean')\nplt.title(\"Histogram of 100 dice rolls\")\nplt.xlabel(\"Roll\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBut what happens if we average the values of the 100 dice rolls in our sample? We can simulate this process many times (roll a dice 100 times, take the average, repeat) to get a distribution of the average.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) * np.sqrt(n)  # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\sqrt{n}(\\bar{X} - \\mathbb{E}[X])$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see the distribution of the sum is approximately normal! It has the mean we would expect (3.5) and clearly values are clustered symmetrically around the mean.\nFor the purposes of the class it’s not necessary to understand why this happens, just that it happens. To give a bit of intuition, though: with more samples, averages tend to cluster around the true mean of the distribution.\nWhy is this useful? Well, the normal distribution is a well studied distribution with many useful properties. It is symmetric, easy to work with mathematically, and is useful for approximating the distribution of many real-world processes. The normal distribution is decently good description of most data that clusters around its mean.\nThe CLT tells us that even if we don’t know the distribution of the individual variables, if we average enough of them together, we magically know the distribution of the sample mean.\nRemember earlier in the lecture, we talked about how statistical models are useful as long as they are a faithful description of the outcome probabilities for a DGP? Well, the CLT gives us a guarantee that for a huge class of DGPs, we can use the normal distribution to model the sample mean.\n\nStandard errors\nThe CLT tells us that the sample mean will converge to a normal distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\) as \\(n\\) approaches infinity. This means that the standard deviation of the sample mean (called the standard error) is equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\).\nWhat does it mean for the standard error to decrease as the sample size increases? It means that as we collect more data, our estimate of the mean becomes more precise. It’s basically the same point as the Law of Large Numbers!\nIn the above example we multiplied the deviations of the sample mean (\\(\\bar{X}-\\mathbb{E}[X]\\)) by \\(\\sqrt{n}\\) to put the distributions on the same scale, which makes it easier to visualize their shapes (so we can see that they become increasingly “normal”). But without that scaling, the distributions get increasingly narrow as \\(n\\) increases, which is exactly what the standard error describes.\n\n\nCode\nnp.random.seed(42)  # for reproducibility\nfig, ax = plt.subplots(3, 3, figsize=(12, 12), sharex=True, sharey=False)\nax = ax.flatten()\n\nfor i, n in enumerate([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n    # Simulate (5000 times) rolling a die n times and taking the average\n    sum_rolls_data = (np.random.randint(1, 7, size=(5000, n)).mean(axis=1) - 3.5) # Centering around the mean (3.5) and scaling by sqrt(n)\n    sum_rolls = pd.DataFrame({'sum_roll': sum_rolls_data})\n\n    # Plot the histogram\n    sns.histplot(data=sum_rolls, x='sum_roll', edgecolor=\"black\", stat='proportion', ax=ax[i])\n    ax[i].set_title(f\"$n={n}$ rolls of a die\")\n    ax[i].set_xlabel(r\"$\\bar{X} - \\mathbb{E}[X]$\")\n    ax[i].set_ylabel(\"Proportion\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 04: Data Generating Processes"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html",
    "href": "notebooks/lecture-06.html",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "",
    "text": "We have talked about quantifying uncertainty with \\(p\\)-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking “how likely is it that the average is \\(\\mu=0\\)?” we might ask “what is a plausible range of values for the average?”\nThis range is called a confidence interval. Like in hypothesis testing, we can choose a risk level \\(\\alpha\\) (e.g., 5%). The confidence interval, like a hypothesis test, is computed based on a sample – so sometimes it is incorrect!\nMathematically, we the confidence interval for a parameter \\(\\theta\\) is defined as the range of values bounded by \\([L, U]\\) that are likely to contain the true value of the parameter with probability \\(1-\\alpha\\): \\[\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n\\]\nThere is an important distinction to make here: \\(\\theta\\) is not the thing that changes! In fact, we can’t compute \\(\\theta\\) - it is a theoretical property of the population, which we can’t access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\nThe sample mean \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\). Meaning, we can assume that the sample mean is typically close to the population mean.\nThe variance of the sample mean is \\(\\sigma^2/n\\), where \\(\\sigma^2\\) can be estimated from the sample.\nThe distribution of the sample mean is symmetric.\n\n\n\nCode\n# annotated plot of the sample mean distribution\nfix, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-3, 3, 100)\npdf = stats.norm.pdf(x, loc=0, scale=1)\nax.plot(x, pdf, label=\"Sample Mean Distribution\")\nax.fill_between(x, pdf, alpha=0.1, color=\"blue\")\n# ax.axvline(0, color=\"black\", linestyle=\"--\")\n# ax.axvline([1, -1], color=\"red\", linestyle=\"--\", label=r\"$\\mu \\pm \\frac{\\sigma}{\\sqrt{n}}$\")\n# ax.annotate(r\"$\\mu$\", xy=(0, 0.1), xytext=(0.5, 0.2),\n#             arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\"),\n#             fontsize=18, color=\"black\")\nax.set_title(r\"Sample Mean $\\bar{X}$ Distribution\")\nax.set_xlabel(\"Sample Mean\")\nax.set_ylabel(\"Probability Density\")\nax.set_xticks(np.arange(-3, 4, 1))\nax.set_xticklabels([r\"$\\mu-3\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu-2\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu-\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu$\", r\"$\\mu+\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu+2\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu+3\\frac{\\sigma}{\\sqrt{n}}$\"])\nax.grid(True, linestyle=\"--\", axis='x', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within \\(k\\) standard deviations of the mean?\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range \\([\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]\\) as: \\[\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n\\] where we made the substitution \\(u = (x - \\mu) \\sqrt{n}/\\sigma\\).\nWe can set this integral to be equal to the probability \\(1 - \\alpha\\) to find the value of \\(z\\) that corresponds to a given confidence level.\n\\[\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n\\]\nSolving this integral for a chosen value of \\(\\alpha\\) gives us the value of \\(z\\) that corresponds to that confidence level. We denote it as \\(z_{\\alpha/2}\\), because the upper limit corresponds to the \\((1 - \\frac{\\alpha}{2})\\) quantile of the distribution.\n\n\n\n\n\n\nStandard normal distribution quantiles\n\n\n\n\n\nThe standard normal distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So \\((1 - \\alpha)\\) is the probability that a sample from the standard normal distribution falls within \\([-z, z]\\).\nThis is the \\((1 - \\frac{\\alpha}{2})\\) because the standard normal distribution is symmetric around 0. So the probability \\(\\alpha\\) of the sample falling outside of \\([-z, z]\\) is split equally between the two tails of the distribution. So there is a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling below \\(-z\\) and a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling above \\(z\\).\n\n\n\nIf you evaluate this integral, you find that the probability of the sample mean being within \\(z\\) standard errors 1 of the mean is, for various values of \\(z\\):\n\n\n\n\\(z\\)\nProbability\n\n\n\n\n1\n0.6827\n\n\n2\n0.9545\n\n\n3\n0.9973\n\n\n4\n&gt; 0.9999\n\n\n\nWe use this logic to define a confidence interval for the population mean \\(\\mu\\) as: \\[\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#confidence-intervals",
    "href": "notebooks/lecture-06.html#confidence-intervals",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "",
    "text": "We have talked about quantifying uncertainty with \\(p\\)-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking “how likely is it that the average is \\(\\mu=0\\)?” we might ask “what is a plausible range of values for the average?”\nThis range is called a confidence interval. Like in hypothesis testing, we can choose a risk level \\(\\alpha\\) (e.g., 5%). The confidence interval, like a hypothesis test, is computed based on a sample – so sometimes it is incorrect!\nMathematically, we the confidence interval for a parameter \\(\\theta\\) is defined as the range of values bounded by \\([L, U]\\) that are likely to contain the true value of the parameter with probability \\(1-\\alpha\\): \\[\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n\\]\nThere is an important distinction to make here: \\(\\theta\\) is not the thing that changes! In fact, we can’t compute \\(\\theta\\) - it is a theoretical property of the population, which we can’t access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\nThe sample mean \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\). Meaning, we can assume that the sample mean is typically close to the population mean.\nThe variance of the sample mean is \\(\\sigma^2/n\\), where \\(\\sigma^2\\) can be estimated from the sample.\nThe distribution of the sample mean is symmetric.\n\n\n\nCode\n# annotated plot of the sample mean distribution\nfix, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-3, 3, 100)\npdf = stats.norm.pdf(x, loc=0, scale=1)\nax.plot(x, pdf, label=\"Sample Mean Distribution\")\nax.fill_between(x, pdf, alpha=0.1, color=\"blue\")\n# ax.axvline(0, color=\"black\", linestyle=\"--\")\n# ax.axvline([1, -1], color=\"red\", linestyle=\"--\", label=r\"$\\mu \\pm \\frac{\\sigma}{\\sqrt{n}}$\")\n# ax.annotate(r\"$\\mu$\", xy=(0, 0.1), xytext=(0.5, 0.2),\n#             arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\"),\n#             fontsize=18, color=\"black\")\nax.set_title(r\"Sample Mean $\\bar{X}$ Distribution\")\nax.set_xlabel(\"Sample Mean\")\nax.set_ylabel(\"Probability Density\")\nax.set_xticks(np.arange(-3, 4, 1))\nax.set_xticklabels([r\"$\\mu-3\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu-2\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu-\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu$\", r\"$\\mu+\\frac{\\sigma}{\\sqrt{n}}$\",\n                    r\"$\\mu+2\\frac{\\sigma}{\\sqrt{n}}$\", r\"$\\mu+3\\frac{\\sigma}{\\sqrt{n}}$\"])\nax.grid(True, linestyle=\"--\", axis='x', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within \\(k\\) standard deviations of the mean?\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range \\([\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]\\) as: \\[\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n\\] where we made the substitution \\(u = (x - \\mu) \\sqrt{n}/\\sigma\\).\nWe can set this integral to be equal to the probability \\(1 - \\alpha\\) to find the value of \\(z\\) that corresponds to a given confidence level.\n\\[\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n\\]\nSolving this integral for a chosen value of \\(\\alpha\\) gives us the value of \\(z\\) that corresponds to that confidence level. We denote it as \\(z_{\\alpha/2}\\), because the upper limit corresponds to the \\((1 - \\frac{\\alpha}{2})\\) quantile of the distribution.\n\n\n\n\n\n\nStandard normal distribution quantiles\n\n\n\n\n\nThe standard normal distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So \\((1 - \\alpha)\\) is the probability that a sample from the standard normal distribution falls within \\([-z, z]\\).\nThis is the \\((1 - \\frac{\\alpha}{2})\\) because the standard normal distribution is symmetric around 0. So the probability \\(\\alpha\\) of the sample falling outside of \\([-z, z]\\) is split equally between the two tails of the distribution. So there is a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling below \\(-z\\) and a probability of \\(\\frac{\\alpha}{2}\\) of the sample falling above \\(z\\).\n\n\n\nIf you evaluate this integral, you find that the probability of the sample mean being within \\(z\\) standard errors 1 of the mean is, for various values of \\(z\\):\n\n\n\n\\(z\\)\nProbability\n\n\n\n\n1\n0.6827\n\n\n2\n0.9545\n\n\n3\n0.9973\n\n\n4\n&gt; 0.9999\n\n\n\nWe use this logic to define a confidence interval for the population mean \\(\\mu\\) as: \\[\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n\\]",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#uncertainty-without-models",
    "href": "notebooks/lecture-06.html#uncertainty-without-models",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Uncertainty without models",
    "text": "Uncertainty without models\nSo far, we’ve emphasized the importance of developing models of data-generating processes in order to quantify randomness and uncertainty. If you know something about the underlying process that generates your data, you can pick an appropriate statistical model and use it to sample from the distribution, simulate new data, and evaluate hypotheses. The CLT is a powerful tool because even when we don’t know what a good model of the data-generating process is, we can often use the normal distribution as an approximation for the distribution of sample means.\nHowever, there are still many situations where we don’t know enough about the DGP to confidently specify a model. In these cases, we can still use statistical techniques to evaluate hypotheses about the data. The idea is that if we don’t have any knowledge about the DGP, we rely on the only information we have: the data itself.\n\nResampling methods\nHow did we get a dataset in the first place? Recall that every dataset is a sample from some underlying distribution.\nWhat is the main difference between a sample and a population? A sample is a subset of the population, and it is usually much smaller than the population. The sample might not cover all the “edge cases” or rare events that are present in the population, or certain outcomes might be overrepresented (you could flip a fair coin 10 times and get 10 heads, even if it is unlikely). But as we have seen, the bigger the sample is the more it tends to represent the population well.\n\n\nCode\n# sample data from a normal distribution\nrng = np.random.default_rng(42)  # for reproducibility\nfig, ax = plt.subplots(1, 3, figsize=(10, 4))\nfor i, n_samples in enumerate([10, 100, 1000]):\n    sample = rng.normal(loc=0, scale=1, size=n_samples)\n    ax[i].hist(sample, bins=30, density=True, alpha=0.5)\n    ax[i].set_title(f\"Sample of {n_samples} from N(0, 1)\")\n    ax[i].set_xlabel(\"Value\")\n    ax[i].set_ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf the sample starts to look like the population… then maybe we can just use the sample itself as a proxy for the population? This is the idea behind resampling methods. When the sample is pretty large, we can sub-sample from the original dataset as though it were the population, and use the sub-samples to estimate properties of the population.\nLet’s try this with the example above. Treating each of the histograms as a proxy for the population, we will sub-sample (with replacement) from the sample 1000 times to create new samples, and see what they look like.\n\n\nCode\nrng = np.random.default_rng(42)  # for reproducibility\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\nfor i, n_samples in enumerate([10, 100, 1000]):\n    sample = rng.normal(loc=0, scale=1, size=n_samples)\n    # choose samples at random with replacement\n    resample = rng.choice(sample, size=10000, replace=True)\n    sns.histplot(resample, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Resampled', ax=ax[i])\n    sns.histplot(sample, bins=30, stat=\"probability\", alpha=0.1, color='blue', label='Original', ax=ax[i])\n    ax[i].set_title(f\"10000 subsamples of {n_samples} samples\", fontsize=12)\n    ax[i].set_xlabel(\"Value\")\n    ax[i].set_ylabel(\"Probability\")\nplt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs it turns out, the distribution of the resampled data is very similar to the original sample. So if the original sample is similar to the true population distribution, the resampled distribution will retain that similarity.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#bootstrapping",
    "href": "notebooks/lecture-06.html#bootstrapping",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping is a specific type of resampling method that allows us to estimate the sampling distribution of any test statistic (e.g., mean, median, variance) by repeatedly resampling with replacement from the observed data. This bootstrapped distribution can then be used to compute confidence intervals or perform hypothesis tests without making any assumptions about the underlying distribution of the data!\nBootstrapping works as follows:\n\nGiven a dataset of size \\(n\\), draw a sample of size \\(n\\) from the original dataset with replacement.\nCompute the test statistic (e.g., mean, median, variance) on the bootstrapped sample.\nRepeat steps 1 and 2 a large number of times (e.g., 1000 times) to create a distribution of the test statistic.\nUse the distribution of the test statistic to compute confidence intervals or perform hypothesis tests.\n\n\n\n\n\n\n\nWhy sample with replacement?\n\n\n\n\n\nThe idea behind the bootstrap is to take independent samples from the original dataset. It is supposed to simulate i.i.d. sampling from the population. Sampling with replacement allows us to create new samples such that taking a sample does not affect the next sample, and every bootstrapped sample comes from the same proxy distribution.\nThink about what would happen if we sampled without replacement: since the sample is the same size as the original dataset, we would end up with the same sample every time! There are only \\(n\\) datapoints in the sample, so if we sample without replacement, we will always get the same sample back.\n\n\n\nLet’s generate a sample from a normal distribution with known parameters \\(\\mu=3.2\\) and \\(\\sigma=1.5\\). Then, we will use bootstrapping to estimate the mean and variance of the population from the sample.\n\n\nCode\nrng = np.random.default_rng(42)\n\nsample_size = 1000\nn_bootstraps = 1000\n\noriginal_sample = rng.normal(loc=3.2, scale=1.5, size=sample_size)\n\nbootstrapped_means = []\nbootstrapped_std = []\nfor _ in range(n_bootstraps):\n    resample = rng.choice(original_sample, size=sample_size, replace=True)\n    bootstrapped_means.append(np.mean(resample))\n    bootstrapped_std.append(np.std(resample, ddof=1))\nbootstrapped_means = np.array(bootstrapped_means)\nbootstrapped_std = np.array(bootstrapped_std)\nprint(f\"Bootstrapped mean: {np.mean(bootstrapped_means):.2f}\")\nprint(f\"Bootstrapped standard deviation: {np.mean(bootstrapped_std):.2f}\")\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(bootstrapped_means, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Bootstrapped Means', ax=ax[0])\nsns.histplot(bootstrapped_std, bins=30, stat=\"probability\", alpha=0.5, color='orange', label='Bootstrapped Std Devs', ax=ax[1])\nax[0].axvline(np.mean(bootstrapped_means), color='red', linestyle='--', label='Mean of Means')\nax[1].axvline(np.mean(bootstrapped_std), color='red', linestyle='--', label='Mean of Std Devs')\nax[0].legend()\nax[1].legend()\nax[0].set_title(\"Bootstrapped Means Distribution\")\nax[1].set_title(\"Bootstrapped Std Devs Distribution\")\nax[0].set_xlabel(\"Mean Value\")\nax[1].set_xlabel(\"Standard Deviation Value\")\nax[0].set_ylabel(\"Probability\")\nax[1].set_ylabel(\"Probability\")\nplt.show()\n\n\nBootstrapped mean: 3.16\nBootstrapped standard deviation: 1.48\n\n\n\n\n\n\n\n\n\nHere is the real power of bootstrapping, though: because we have the whole distribution of the resampled data, we can compute confidence intervals any statistic by looking at at different percentiles of the resampled distribution. For example, we can compute a 95% confidence interval for the mean by taking the 2.5th and 97.5th percentiles of the resampled means.\n\n\nCode\nlower_bound_mean = np.percentile(bootstrapped_means, 2.5)\nupper_bound_mean = np.percentile(bootstrapped_means, 97.5)\nprint(f\"95% Confidence Interval for the Mean: ({lower_bound_mean:.2f}, {upper_bound_mean:.2f})\")\nlower_bound_std = np.percentile(bootstrapped_std, 2.5)\nupper_bound_std = np.percentile(bootstrapped_std, 97.5)\nprint(f\"95% Confidence Interval for the Standard Deviation: ({lower_bound_std:.2f}, {upper_bound_std:.2f})\")\n\n# plot the distribution of bootstrapped means and std with confidence intervals\nfig, ax = plt.subplots(1, 2, figsize=(10, 6))\nsns.histplot(bootstrapped_means, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Bootstrapped Means', ax=ax[0])\nax[0].axvline(lower_bound_mean, color='blue', linestyle='--', label='Lower Bound (2.5th Percentile)')\nax[0].axvline(upper_bound_mean, color='green', linestyle='--', label='Upper Bound (97.5th Percentile)')\nax[0].set_title(\"Bootstrapped Means Distribution\")\nax[0].set_xlabel(\"Mean Value\")\nax[0].set_ylabel(\"Probability\")\nax[0].legend()\n\nsns.histplot(bootstrapped_std, bins=30, stat=\"probability\", alpha=0.5, color='orange', label='Bootstrapped Std', ax=ax[1])\nax[1].axvline(lower_bound_std, color='red', linestyle='--', label='Lower Bound (2.5th Percentile)')\nax[1].axvline(upper_bound_std, color='blue', linestyle='--', label='Upper Bound (97.5th Percentile)')\nax[1].set_title(\"Bootstrapped Std Distribution\")\nax[1].set_xlabel(\"Standard Deviation Value\")\nax[1].set_ylabel(\"Probability\")\nax[1].legend()\nplt.show()\n\n\n95% Confidence Interval for the Mean: (3.07, 3.25)\n95% Confidence Interval for the Standard Deviation: (1.41, 1.55)\n\n\n\n\n\n\n\n\n\n\nUnderstanding the meaning of a confidence interval\nThe definition of a confidence interval is a bit confusing, but bootstrapping can actually help us demonstrate what it means.\nThe uncertainty in the confidence interval does NOT come from uncertainty in the value of the parameter we are estimating. In our example, we designed the distribution and we know that the true mean is 3.2, so there is no uncertainty about the value of the parameter. The uncertainty comes from the fact that we are estimating the parameter based on a sample, and the sample is not necessarily representative of the population.\nTo illustrate this, let’s repeatedly sample from our distribution and run the bootstrapping procedure on each sample. In the plot below, each interval represents a 95% confidence interval for the mean of a different sample. The highlighted intervals are the ones that failed to capture the true mean of 3.2.\n\n\nCode\ndef bootstrap_mean_ci(sample, n_iterations=1000, alpha=0.05, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    n = len(sample)\n    means = []\n    for _ in range(n_iterations):\n        resample = rng.choice(sample, size=n, replace=True)\n        means.append(np.mean(resample))\n    lower_bound = np.percentile(means, 100 * alpha / 2)\n    upper_bound = np.percentile(means, 100 * (1 - alpha / 2))\n    return lower_bound, upper_bound\n\n\n## Do a bootstrap on 1000 samples from the same distribution, see what proportion of CIs contain the mean\nrng = np.random.default_rng(42)\nn_sims = 1000\nsample_size = 1000\n# store the intervals (lower, upper)\nci_list = []\n# track which intervals do not contain the true mean\nviolating_ci_idx = []\nfor i in range(n_sims):\n    sample = rng.normal(loc=3.2, scale=1.5, size=sample_size)\n    lower_bound, upper_bound = bootstrap_mean_ci(\n        sample, n_iterations=1000, alpha=0.05, rng=rng\n    )\n    ci_list.append((i, lower_bound, upper_bound))\n    # check if the true mean is within the confidence interval\n    if not (3.2 &gt;= lower_bound and 3.2 &lt;= upper_bound):\n        # print(f\"Iteration {i}: CI does not contain the true mean 3.2\")\n        violating_ci_idx.append(i)\nprint(\n    f\"Proportion of CIs that do not contain the true mean: {len(violating_ci_idx) / n_sims:.4f}\"\n)\n\n# plot the first 5 violating CIs\nfig, ax = plt.subplots(figsize=(10, 6))\nfor i in range(violating_ci_idx[5] + 1):\n    if i in violating_ci_idx:\n        alpha = 1.0\n    else:\n        alpha = 0.2\n    lower_bound, upper_bound = ci_list[i][1], ci_list[i][2]\n    ax.plot([i, i], [lower_bound, upper_bound], color=\"gray\", alpha=alpha, linewidth=2)\n    ax.scatter(\n        i, lower_bound, color=\"blue\", alpha=alpha, label=\"Lower Bound\" if i == 0 else \"\", zorder=3\n    )\n    ax.scatter(\n        i,\n        upper_bound,\n        color=\"green\",\n        alpha=alpha,\n        label=\"Upper Bound\" if i == 0 else \"\",\n        zorder=3\n    )\n\nax.axhline(3.2, color=\"black\", linestyle=\"--\", label=\"True Mean (3.2)\")\nax.set_title(\"Violating Confidence Intervals\")\nax.set_xlabel(\"Iteration\")\nax.set_ylabel(\"Confidence Interval\")\nax.legend()\nplt.show()\n\n\nProportion of CIs that do not contain the true mean: 0.0580\n\n\n\n\n\n\n\n\n\nNotice that about 5% of the intervals do not capture the true mean, which is exactly what we expect from a 95% confidence interval.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#example-variability-in-scoring-average",
    "href": "notebooks/lecture-06.html#example-variability-in-scoring-average",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Example: variability in scoring average",
    "text": "Example: variability in scoring average\nLet’s return to our analysis of NBA players’ scoring ability.\nWe can use bootstrapping to estimate confidence intervals for each player’s scoring average.\n\n\nCode\n### Data import and preparation ###\nsga_df = pd.read_csv(\"../data/sga-stats-24-25.csv\")\ngiannis_df = pd.read_csv(\"../data/giannis-stats-24-25.csv\")\n# combine the dataframes and clean up the data\nsga_df[\"player\"] = \"Shai Gilgeous-Alexander\"\ngiannis_df[\"player\"] = \"Giannis Antetokounmpo\"\ncompare_df = pd.concat([sga_df, giannis_df], ignore_index=True)\n# filter out rows where the player did not play or was inactive\ncompare_df = compare_df.replace({\"Did Not Dress\": np.nan, \"Inactive\": np.nan, \"Did Not Play\": np.nan, \"\": np.nan})\ncompare_df.dropna(subset=[\"PTS\"], inplace=True)\n# convert PTS to float/numeric and Date to datetime\ncompare_df[\"PTS\"] = compare_df[\"PTS\"].astype(float)\ncompare_df[\"Date\"] = pd.to_datetime(compare_df[\"Date\"])\n\n# print scoring averages\nprint(f\"Shai Gilgeous-Alexander's scoring average: {compare_df[compare_df['player'] == 'Shai Gilgeous-Alexander']['PTS'].mean():.2f}\")\nprint(f\"Giannis Antetokounmpo's scoring average: {compare_df[compare_df['player'] == 'Giannis Antetokounmpo']['PTS'].mean():.2f}\")\n\n### Bootstrapping confidence intervals for scoring average ###\n\nsga_lower_bound, sga_upper_bound = bootstrap_mean_ci(\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    n_iterations=1000,\n    alpha=0.05,\n    rng=np.random.default_rng(42)\n)\ngiannis_lower_bound, giannis_upper_bound = bootstrap_mean_ci(\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    n_iterations=1000,\n    alpha=0.05,\n    rng=np.random.default_rng(42)\n)\nprint(\"-\"*25)\nprint(f\"Shai Gilgeous-Alexander's 95% CI: ({sga_lower_bound:.2f}, {sga_upper_bound:.2f})\")\nprint(f\"Giannis Antetokounmpo's 95% CI: ({giannis_lower_bound:.2f}, {giannis_upper_bound:.2f})\")\n\n\n\nShai Gilgeous-Alexander's scoring average: 32.68\nGiannis Antetokounmpo's scoring average: 30.39\n-------------------------\nShai Gilgeous-Alexander's 95% CI: (30.97, 34.42)\nGiannis Antetokounmpo's 95% CI: (28.70, 32.09)\n\n\nNotice that Shai’s 95% CI does not overlap with Giannis’ scoring average, but it does overlap with Giannis’ 95% CI!\nThis actually highlights a problem with our previous analysis: we only considered the variability in SGA’s scoring average, and asked how likely it is that his scoring average was equivalent to a specific value (Giannis’ scoring average). But we didn’t consider the variability in Giannis’ scoring average!\nThink about it like this: perhaps SGA’s scoring average was randomly higher than expected, and Giannis’ scoring average was randomly lower than expected. This possibly makes it more likely that the two averages are closer together than it would appear.\nAnother way of stating this is that our null hypothesis was too weak (we chose a baseline equal to Giannis’ scoring average, but Giannis could actually be better or worse than that).\nLet’s fix this mistake! We can use bootstrapping to generate scoring average distributions for both players, and then compare the two distributions to see how much they overlap.\nThe question is: if we resample with replacement from both players’ game logs (i.e., their individual game scores), how often do we find that Giannis scores more on average than SGA?\n\n\nCode\ndef bootstrap_mean_dist(sample, n_iterations=1000, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    n = len(sample)\n    means = []\n    for _ in range(n_iterations):\n        resample = rng.choice(sample, size=n, replace=True)\n        means.append(np.mean(resample))\n    return np.array(means)\n# Generate bootstrapped distributions for both players\nsga_dist = bootstrap_mean_dist(\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"],\n    n_iterations=1000,\n    rng=np.random.default_rng(42)\n)\ngiannis_dist = bootstrap_mean_dist(\n    compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"],\n    n_iterations=1000,\n    rng=np.random.default_rng(42)\n)\n# compute the proportion of bootstrapped means where Giannis's mean is greater than or equal to SGA's mean\np_value = np.mean(giannis_dist &gt;= sga_dist)\nprint(f\"Proportion of bootstrapped samples where Giannis's mean &gt;= SGA's mean: {p_value:.4f}\")\n\n# Plot the distributions\nfig, ax = plt.subplots(2, 1, figsize=(10, 6))\nsns.histplot(sga_dist, bins=30, stat=\"probability\", alpha=0.5, color='blue', label='Shai Gilgeous-Alexander', ax=ax[0])\nsns.histplot(giannis_dist, bins=30, stat=\"probability\", alpha=0.5, color='green', label='Giannis Antetokounmpo', ax=ax[0])\nax[0].axvline(np.mean(sga_dist), color='blue', linestyle='--')\nax[0].axvline(np.mean(giannis_dist), color='green', linestyle='--')\nax[0].set_title(\"Bootstrapped Scoring Average Distributions\")\nax[0].set_xlabel(\"Scoring Average\")\nax[0].set_ylabel(\"Probability\")\nax[0].legend()\n# Plot the distribution of the difference in means\ndiff_dist = sga_dist - giannis_dist\nsns.histplot(diff_dist, bins=30, stat=\"probability\", alpha=0.5, label='Difference (SGA - Giannis)', ax=ax[1])\nax[1].fill_betweenx(\n    y=ax[1].get_ylim(),\n    x1=ax[1].get_xlim()[0],\n    x2=0,\n    alpha=0.1\n)\nax[1].axvline(0, color='purple', linestyle='--', label=r'$\\mu_{\\text{SGA}} - \\mu_{\\text{Giannis}} = 0$')\nax[1].set_title(\"Bootstrapped Difference in Scoring Averages\")\nax[1].set_xlabel(\"Difference in Scoring Average (SGA - Giannis)\")\nax[1].set_ylabel(\"Probability\")\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nProportion of bootstrapped samples where Giannis's mean &gt;= SGA's mean: 0.0340\n\n\n\n\n\n\n\n\n\nRecall that in the previous analysis, our \\(p\\)-value was 0.004! So our corrected analysis estimates that the probability that SGA’s scoring title is a “fluke” is actually an order of magnitude higher.\nAnother way to look at this is to return to the lens of hypothesis testing. What hypothesis test captures the variability of both players’ scoring? How is our null hypothesis different from the one we used before?\nEarlier we just tested if SGA’s true scoring rate was higher than a constant value (Giannis’ observed scoring average). Now, we are testing if SGA’s true scoring rate is higher than Giannis’ true scoring rate, which is a random variable that can take on different values across different samples.\n\n\\(H_0: ~\\mu_{\\text{SGA}} \\leq \\mu_{\\text{Giannis}}\\)\n\\(H_1: ~\\mu_{\\text{SGA}} &gt; \\mu_{\\text{Giannis}}\\)\n\nThis null hypothesis basically posits that in the best case, SGA’s scoring average is equal to Giannis’ scoring average (if not worse). This would mean that their scoring output is indistinguishable – if the variance of their scoring is on the same scale, then their game logs are actually being sampled from the same distribution! We can simulate this in a bootstrapping procedure by combining the two players’ game logs and resampling from the combined distribution.\nThe question is: if we resample with replacement from the combined game logs (i.e. the players have the exact same scoring distribution), how often do we see such a large advantage for SGA?\n\n\nCode\nnp.random.seed(42)  \n# number of games played by each player\nn_games_sga = len(compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"])\nn_games_giannis = len(compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"])\n# Compute the observed difference in means\nobserved_diff = (\n    compare_df[compare_df[\"player\"] == \"Shai Gilgeous-Alexander\"][\"PTS\"].mean()\n    - compare_df[compare_df[\"player\"] == \"Giannis Antetokounmpo\"][\"PTS\"].mean()\n)\nprint(f\"Observed difference in scoring averages: {observed_diff:.2f} (SGA - Giannis)\")\n\nn_bootstraps = 1000\nbootstrapped_diffs = []\n# Perform bootstrapping to compute the difference in means\nfor i in range(n_bootstraps):\n    # Bootstrap sampling for SGA\n    sga_sample = compare_df[\"PTS\"].sample(n=n_games_sga, replace=True)\n    # Bootstrap sampling for Giannis\n    giannis_sample = compare_df[\"PTS\"].sample(n=n_games_giannis, replace=True)\n    # Compute the means\n    sga_mean = sga_sample.mean()\n    giannis_mean = giannis_sample.mean()\n    diff = sga_mean - giannis_mean\n    bootstrapped_diffs.append(diff)\nbootstrapped_diffs = np.array(bootstrapped_diffs)\n# Compute the p-value\np_value = np.mean(bootstrapped_diffs &gt;= observed_diff)\nprint(f\"Bootstrapped p-value for the difference in scoring averages: {p_value:.4f}\")\n\n\nObserved difference in scoring averages: 2.30 (SGA - Giannis)\nBootstrapped p-value for the difference in scoring averages: 0.0440\n\n\nSimulation and resampling methods give us a powerful way to quantify uncertainty in a way that does not rely so heavily on assumptions about the underlying distribution of the data.\nIn the next lecture, we’ll look at another simulation-based method for hypothesis testing: permutation tests.",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-06.html#footnotes",
    "href": "notebooks/lecture-06.html#footnotes",
    "title": "Lecture 06: Confidence Intervals and Bootstrapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. the standard deviation of the sampling distribution↩︎",
    "crumbs": [
      "Home",
      "Lecture 06: Bootstrapping"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html",
    "href": "notebooks/lecture-08.html",
    "title": "Lecture 08: Linear Regression",
    "section": "",
    "text": "Now that we’ve talked about inference in depth, it is time to talk about prediction. In this lecture, we will build up to the basics of linear regression, which is a powerful tool for making predictions based on data.",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#predictions-from-patterns",
    "href": "notebooks/lecture-08.html#predictions-from-patterns",
    "title": "Lecture 08: Linear Regression",
    "section": "Predictions from patterns",
    "text": "Predictions from patterns\nHow do we make predictions? We typically look for patterns in the data and use those patterns to make informed guesses about future outcomes.\n\n\n\n\n\n\nWhy not memorize?\n\n\n\nWhy not just memorize the data? Think about a dataset like this:\n\n\n\nName\nApples / Day\n\n\n\n\nAlice\n3\n\n\nBob\n5\n\n\nCharlie\n2\n\n\n\nIf you were asked to predict how many apples a new person would eat per day, you could memorize the data and say “Alice eats 3 apples, Bob eats 5 apples, Charlie eats 2 apples.” You would be 100% correct on this dataset! But what if you were asked to predict how many apples a new person would eat? You would have no idea, because your memorized list only contains the data you have seen already. You have no way to make a prediction about a new person like Dave: | Name | Apples / Day | |——|————–| | Dave | ? |\nSo, we need a better strategy!\nMore broadly, this is called generalization: the ability to make predictions about new data based on patterns learned from existing data.\n\n\nConsider the following dataset of the heights (in inches) of a sample of adults. The dataset also records the height of their parents, so we’ll refer to the adults as “children” and their parents as “parents” in this context. 1\n\n\nCode\ngalton = pd.read_csv(\"../data/galton-stata11.tsv\", sep=\"\\t\", dtype={\"male\": bool, \"female\": bool, \"gender\": \"category\", \"family\": \"category\"})\ngalton.head() # first few rows of the dataset\n\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\ngender\nheight\nkids\nmale\nfemale\n\n\n\n\n0\n1\n78.5\n67.0\nM\n73.2\n4\nTrue\nFalse\n\n\n1\n1\n78.5\n67.0\nF\n69.2\n4\nFalse\nTrue\n\n\n2\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n3\n1\n78.5\n67.0\nF\n69.0\n4\nFalse\nTrue\n\n\n4\n2\n75.5\n66.5\nM\n73.5\n4\nTrue\nFalse\n\n\n\n\n\n\n\nLet’s say I asked you to predict the height of another child not in the dataset. Assume they’re from the same population as the children in the dataset. You have no other information about the child. How would you do it?\nThe most straightforward thing to do is ask yourself: “What is the typical height of a child in this dataset?” You could then use that typical height as your prediction. This is the simplest kind of pattern recognition: finding the average value in the dataset and using it as a prediction.\n\n\nCode\n# print the mean height of children in the dataset\nmean_height = galton[\"height\"].mean()\nprint(f\"The mean height of children in the Galton dataset is {mean_height:.2f} inches.\")\n\n# plot the distribution of heights\nplt.figure(figsize=(8, 4))\nsns.histplot(galton[\"height\"], stat=\"proportion\", bins=30)\nplt.xlabel(\"Height (inches)\")\nplt.ylabel(\"Proportion of children\")\nplt.title(\"Distribution of Heights of Children\")\nplt.axvline(mean_height, color=\"red\", linestyle=\"--\", label=\"Mean Height\")\nplt.legend()\nplt.show()\n\n\nThe mean height of children in the Galton dataset is 66.76 inches.\n\n\n\n\n\n\n\n\n\nThis isn’t bad as a first guess, and you actually can’t do any better than this if you have no other information about the child.\nBut what if you had more information? For example, say you had other information about the child (besides their own height), such as their sex (male / female) or the heights of their parents. If you think that these factors might be related to the child’s height, they should inform your prediction.\nLet’s start by just plotting the heights of the children in the dataset against the heights of their parents. This will help us visualize the relationship between the two variables.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=True, sharex=True)\n# plot the heights of children against the heights of their fathers\nsns.scatterplot(data=galton, x=\"father\", y=\"height\", ax=ax[0], alpha=0.5)\nax[0].set_xlabel(\"Father's Height (inches)\")\nax[0].set_ylabel(\"Child's Height (inches)\")\nax[0].set_title(\"Child's Height vs Father's Height\")\nax[0].axvline(galton[\"father\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean Father's Height\")\nax[0].axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nax[0].legend(loc='lower right')\n\n# plot the heights of children against the heights of their mothers\nsns.scatterplot(data=galton, x=\"mother\", y=\"height\", ax=ax[1], alpha=0.5)\nax[1].set_xlabel(\"Mother's Height (inches)\")\nax[1].set_ylabel(\"Child's Height (inches)\")\nax[1].set_title(\"Child's Height vs Mother's Height\")\nax[1].axvline(galton[\"mother\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean Mother's Height\")\nax[1].axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nax[1].legend(loc='lower right')\n\n\n\n\n\n\n\n\n\nNotice how the naive prediction we made earlier (the average height of the children), shown as a blue dashed line, totally ignores the heights of the parents. It’s constant across the entire plot, and does not capture any of the variation in the data.\nYou may, based on your own experience and understanding of the world, expect that children with taller parents tend to be taller themselves. It sure looks that way in the plot above – the taller children tend to have taller parents, and the shorter children tend to have shorter parents. It’s not always the case, by any means, but it seems to be a general trend.\nBut how much taller? We want to quantify this relationship, so that we can get a formula for predicting a child’s height based on their parents’ heights. Mathematically, we want to define a function \\(f\\) that takes the heights of the parents as input and returns the predicted height of the child as output.\n\nPreparing the data\nDealing with two separate parents is cumbersome. Can we combine the heights of the parents into a single number that represents “the height of the parents”? One way to do this is to take the average of the two heights.\nThis is problematic at the moment. Based on the plot it seems that the father’s height has a different relationship with the child’s height than the mother’s height does. The trends are similar, but the plots have different shapes. This is probably because men and women tend to have different average heights (as you can see in the plot above). Also because men are taller on average, they contribute more to the average height of the parents than women do.\nThere is a useful trick we can use to simplify this. We can put the heights of men and women in the dataset on the same scale by standardizing them.\n\\[\n\\begin{align*}\n\\text{standardized female height} = \\frac{\\text{height} - \\text{average of female heights}}{\\text{standard deviation of female heights}} \\\\\n\\text{standardized male height} = \\frac{\\text{height} - \\text{average of male heights}}{\\text{standard deviation of male heights}} \\\\\n\\end{align*}\n\\]\nThis procedure transforms all of the heights into a common scale, where the average is zero and the standard deviation is one. This process is called standardization or z-scoring.\nLet’s do this and plot the standardized heights of the parents against the heights of the children.\n\n\nCode\n# include all female and male heights (parents and children)\nmother_heights = galton[[\"mother\", \"family\"]].drop_duplicates()[\"mother\"].values\nfemale_child_heights = galton[galton[\"gender\"] == \"F\"][\"height\"].values\nfemale_heights = np.concatenate([mother_heights, female_child_heights])\n\nfather_heights = galton[[\"father\", \"family\"]].drop_duplicates()[\"father\"].values\nmale_child_heights = galton[galton[\"gender\"] == \"M\"][\"height\"].values\nmale_heights = np.concatenate([father_heights, male_child_heights])\n\n# calculate the mean and standard deviation of the heights of mothers and fathers\nfemale_mean = female_heights.mean()\nfemale_std = female_heights.std(ddof=1)\nmale_mean = male_heights.mean()\nmale_std = male_heights.std(ddof=1)\n\n# transform the heights to z-scores\ngalton[\"mother_z\"] = (galton[\"mother\"] - female_mean) / female_std\ngalton[\"father_z\"] = (galton[\"father\"] - male_mean) / male_std\ngalton.loc[galton[\"gender\"] == \"F\", \"height_z\"] = (galton.loc[galton[\"gender\"] == \"F\", \"height\"] - female_mean) / female_std\ngalton.loc[galton[\"gender\"] == \"M\", \"height_z\"] = (galton.loc[galton[\"gender\"] == \"M\", \"height\"] - male_mean) / male_std\n\n# plot the z-scores of the heights of children against the z-scores of their fathers\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharey=True, sharex=True)\nsns.scatterplot(data=galton, x=\"father_z\", y=\"height_z\", ax=ax[0], alpha=0.5)\nax[0].set_xlabel(\"Father's Height (z-score)\")\nax[0].set_ylabel(\"Child's Height (z-score)\")\nax[0].axvline(0, color=\"red\", linestyle=\"--\")\nax[0].axhline(0, color=\"blue\", linestyle=\"--\")\n\n# plot the z-scores of the heights of children against the z-scores of their mothers\nsns.scatterplot(data=galton, x=\"mother_z\", y=\"height_z\", ax=ax[1], alpha=0.5)\nax[1].set_xlabel(\"Mother's Height (z-score)\")\nax[1].set_ylabel(\"Child's Height (z-score)\")\nax[1].axvline(0, color=\"red\", linestyle=\"--\")\nax[1].axhline(0, color=\"blue\", linestyle=\"--\")  \n\n\n\n\n\n\n\n\n\nNow we have put the heights of the parents on the same scale, so we can combine them into a single number that represents “the height of the parents”. We can do this by taking the average of the standardized heights of the parents.\n\n\nCode\ngalton[\"midparent\"] = (galton[\"mother_z\"] + galton[\"father_z\"]) / 2\n\n# plot the z-scores of the heights of children against the midparent z-scores\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height_z\", alpha=0.5)\nplt.xlabel(\"Midparent Height (z-score)\")\nplt.ylabel(\"Child's Height (z-score)\")\nplt.axvline(0, color=\"red\", linestyle=\"--\")\nplt.axhline(0, color=\"blue\", linestyle=\"--\")\nplt.title(\"Child's Height vs Midparent Height (z-score)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNice! It seems like this retains the relationship between the heights of the parents and the heights of the children, but now we have a single number that represents the height of the parents.\nLet’s go back to the units we care about predicting, though. It’s ok to use standardized heights for the parents, but we want to predict the height of the child in inches.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height\", alpha=0.5)\nplt.xlabel(\"Midparent Height (Z-score)\")\nplt.ylabel(\"Child's Height (inches)\")\nplt.axvline(0, color=\"red\", linestyle=\"--\")\nplt.axhline(mean_height, color=\"blue\", linestyle=\"--\", label=\"Mean Child's Height\")\nplt.title(\"Child's Height vs Midparent Height (Z-score)\")\n\n\nText(0.5, 1.0, \"Child's Height vs Midparent Height (Z-score)\")\n\n\n\n\n\n\n\n\n\n\n\nQuantifying relationships\nBack to our question: how much taller are the children of tall parents?\nThe simplest way to quantify this relationship is to use a linear function, which is a function of the form \\(f(x) = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept. A linear function has a constant rate of change, which means that for every unit increase in the input, the output increases by a fixed amount. Fitting a linear function to data is called linear regression.\nIn our case, we choose to model the relationship between the heights of the parents and the heights of the children as a linear function.\n\\[\\text{predicted height of child} = \\text{slope} \\times \\text{midparent height} + \\text{intercept}\\]\nNote that our first model, the average height of the children, is a special case of this linear function where the slope is zero and the intercept is the average height of the children.\nWhat should the slope be? Basically, we want it to be whatever value makes the predicted heights of the children as close to the actual heights of the children as possible.\nWe can use a statistical technique called ordinary least squares (OLS) to find the best-fitting line for our data. OLS minimizes the (squared) distance between the predicted heights of the children and the actual heights of the children.\n\n\n\n\n\n\nWhy squared?\n\n\n\n\n\nOLS minimizes \\((\\text{predicted height} - \\text{actual height})^2\\). Why squared? There are two main reasons: 1. Squaring the differences ensures that we don’t have negative values canceling out positive values 2. When you square a number, it becomes larger as the number gets larger. This means that larger errors are penalized more heavily than smaller errors. So you’d rather be off by 1 inch 10 times (squared error of 10) than be off by 10 inches once (squared error of 100). This helps us find a line that is close to all of the points and avoids being too far off on any one point.\nAdvanced note: If you like calculus, the other benefit of squared errors is that they are differentiable (think about a parabola \\(y = x^2\\) - the slope is always changing, but it never has a sharp corner). This is a big difference compared to absolute errors, which have a sharp corner at zero (the slope of \\(|x|\\) is not defined at zero, where it changes from -1 to 1).\n\n\n\nLet’s fit a linear regression model to the data and see what the best-fitting line looks like.\n\n\n\n\n\n\nSpecifying regression models\n\n\n\n\n\nStatistical software packages have built-in functions for fitting many common models to data. There are a variety of packages that do pretty much the same thing, and they all have their own syntax for specifying the model you want to fit.\nSome packages accept a formula as input, which specifies the relationship between the variables. The “formula” is a string that describes the model you want to fit, and has its own syntax. In this case, we want to predict the height of the child based on the midparent height, so we use the formula height ~ midparent. The ~ symbol separates the outcome variable (the variable we want to predict) from the input variable (the variable we are using to make the prediction). The + symbol can be used to add more input variables to the model, but in this case we only have one input variable. The intercept is included by default, so we don’t need to specify it explicitly.\n\n\n\n\n\nCode\n# fit OLS and draw the prediction errors\nfrom statsmodels.formula.api import ols\n# fit a linear regression model to the data\nmodel = ols(\"height ~ midparent\", data=galton).fit()\n# print the summary of the model\npredicted_heights = model.predict(galton[\"midparent\"]).values\nerrors = galton[\"height\"] - predicted_heights\n\nselect_idx = galton[\"family\"].drop_duplicates().sample(20).index.values # avoid overplotting a few non-overlapping families\nremaining_idx = galton.index.difference(select_idx).values\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n\nsns.scatterplot(data=galton, x=\"midparent\", y=\"height\", ax=ax[0])\nsns.lineplot(x=galton[\"midparent\"], y=predicted_heights, color=\"k\", label=\"Predicted Height\", ax=ax[0])\n\nsns.scatterplot(data=galton.iloc[select_idx], x=\"midparent\", y=\"height\", alpha=1., ax=ax[1])\nsns.scatterplot(data=galton.iloc[remaining_idx], x=\"midparent\", y=\"height\", alpha=0.15, color=\"gray\", ax=ax[1])\nplt.xlabel(\"Midparent Height (Z-score)\")\nplt.ylabel(\"Child's Height (inches)\")\nsns.lineplot(x=galton[\"midparent\"], y=predicted_heights, color=\"k\", label=\"Predicted Height\", ax=ax[1])\nfor i in select_idx:\n    # draw a line from the predicted height to the actual height\n    ax[1].plot([galton[\"midparent\"].iloc[i], galton[\"midparent\"].iloc[i]], [predicted_heights[i], galton[\"height\"].iloc[i]], color=\"red\", linestyle=\"--\", alpha=.8)\n\n\n\n\n\n\n\n\n\nIn the second plot, we show the prediction errors for a few datapoints. The red lines show the difference between the predicted height of the child and the actual height of the child. OLS finds the best fit that minimizes the sum of the squared lengths of these red lines.\nSo what’s the slope that answers our question? We can extract the parameters of the model to find out. The slope is the parameter associated with the midparent height variable.\n\n\nCode\nmodel.params\n\n\nIntercept    66.765946\nmidparent     1.656486\ndtype: float64\n\n\nSo what exactly can we say?\nThe slope of a line \\(y = mx + b\\) is the change in \\(y\\) for a one-unit increase in \\(x\\). So, in our case, the slope is the change in the predicted height of the child for a one-unit increase in the midparent height.\nSo, for every one unit increase in the midparent height, the predicted height of the child increases by about 1.7 inches.\nThis probably seems confusing because the midparent heights are standardized and averaged. It means that if the parents’ heights are on average one standard deviation above the mean height, the predicted height of the child is 1.7 inches taller than the average height of the children in the dataset.\nSo how did we do in terms of prediction?\nLet’s take a look at the distribution of the errors.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.histplot(x=errors, alpha=0.5, stat=\"probability\", ax=ax)\nax.set_xlabel(\"Prediction Error (inches)\")\nax.axhline(0, color=\"gray\", linestyle=\"--\")\nax.axvline(0, color=\"gray\", linestyle=\"--\")\nax.set_title(\"Prediction Error vs Midparent Height (Z-score)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe often evaluate our predictions by studying the mean and standard deviation of the prediction errors. We want the mean to be close to zero, which means that our predictions are on average correct. You don’t want to be consistently over- or under-predicting. The standard deviation tells us how much the prediction errors vary.\n\n\nCode\nprint(f\"Mean prediction error: {np.mean(errors):.2e} \\nStandard deviation of prediction error: {np.std(errors):.2f}\")\n\n\nMean prediction error: -2.43e-14 \nStandard deviation of prediction error: 3.39\n\n\nSpecifically, the standard deviation is called the “Root Mean Squared Error” (RMSE), and it tells us how far off our predictions are from the actual values on average: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted height}_i - \\text{actual height}_i)^2}\\] So, in our case the RMSE is about 3.4. This means that on average, our predictions are off by about 3.4 inches.\nLet’s compare this to the RMSE of the naive prediction we made earlier (the average height of the children).\n\n\nCode\nnaive_rmse = np.sqrt(np.mean((galton[\"height\"] - mean_height) ** 2))\nprint(f\"Naive RMSE: {naive_rmse:.2f} inches\")\n\n\nNaive RMSE: 3.58 inches\n\n\nNotice anything else interesting about the residual distribution? It’s centered around zero, and most of the residuals close to that mean – it’s approximately normal!\nWe skipped over this important detail before, but that’s actually a key assumption of the linear regression model. The residuals (or you can think of them as “noise” – variation in the data that we can’t explain with our model) should be normally distributed.\nThis highlights the connection between linear regression and the kind of probabilistic statistical models we have been discussing in this course all along. The regression model treats the variable we are predicting (the height of the child) as a random variable itself. It has an expected value that depends in a linear way on the input variable, but it also has some inherent variability.\nSo the linear regression model predictions can be written as: \\[y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\sigma^2\\) is the variance of the residuals.\nThe consequence of this variability is that, just like our previous models, our data-driven conclusions are subject to uncertainty from sampling. We might just happen to get a sample where the children of tall parents are shorter than average, and get a slope that is negative.\nThis is why we need to be careful about interpreting the results of our regression model. We’ll return to the hypothesis testing framework in the next lecture, and show how it can be applied to regression models.",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-08.html#footnotes",
    "href": "notebooks/lecture-08.html#footnotes",
    "title": "Lecture 08: Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Galton dataset is is a classic dataset in statistics, named after Sir Francis Galton, who studied the relationship between the heights of parents and their children. The study is the origin of the term “linear regression” and is murkily involved in the eugenics movement of the early 1900s. Data from here.↩︎",
    "crumbs": [
      "Home",
      "Lecture 08: Regression Basics"
    ]
  },
  {
    "objectID": "notebooks/lecture-10.html",
    "href": "notebooks/lecture-10.html",
    "title": "Lecture 10: ANOVA",
    "section": "",
    "text": "We want to emphasize that nearly any statistical method with a closed-form solution can be replicated with a simulation.\nTake ANOVA, or analysis of variance, for example. The ANOVA test is used to determine whether there are any statistically significant differences between the means of two or more groups. It is a parametric test that assumes the data is normally distributed and that the variances of the groups are equal.\nThe exact test we compute is the F-test, which compares the variance between groups to the variance within groups. The null hypothesis is that all group means are equal, while the alternative hypothesis is that at least one group mean is different. The F-test relies on the fact that the ratio of two independent chi-squared variables (i.e. a normal variable, squared) follows an F-distribution.\nHere we will simulate the F-test by generating random data from a normal distribution and computing the F-statistic for different group means. We will see that under the null hypothesis (group means are equal), the F-statistic follows an F-distribution with degrees of freedom equal to the number of groups minus one and the total number of observations minus the number of groups.\nIn the next lecture we will compare the F-test to a non-parametric permutation test, which does not make any assumptions about the distribution of the data."
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "AI Notice: I expect that this entire assignment can be readily solved by today’s AI systems. I would encourage you to attempt the assignment yourself first, but if you find yourself stuck, you can use AI tools to help you. I have to emphasize that if you do so, it is crucial that you take the time to understand the AI’s solution and not just copy it. The goal is to learn and practice the concepts, not just to complete the assignment. If you don’t need AI to solve the assignment – great! It can still help – ask it to review your solution, critique it, or suggest improvements. This will help you learn even more about how to write good code."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-1-robust-addition",
    "href": "assignments/assignment-01.html#problem-1-robust-addition",
    "title": "Assignment 01",
    "section": "Problem 1: Robust addition",
    "text": "Problem 1: Robust addition\nWrite a function that takes in a list of numbers and returns their sum. However, if the list contains any non-numeric values, the function should ignore those values and only sum the numeric ones. The exception is if the non-numeric values can be converted to numeric types (like strings representing numbers), in which case they should be converted and included in the sum. If the list is empty or contains no numeric values, the function should return 0.\n\ndef robust_addition(numbers : list) -&gt; float:\n    # TODO: Implement the function to sum only numeric values in the list.\n    pass"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-2",
    "href": "assignments/assignment-01.html#problem-2",
    "title": "Assignment 01",
    "section": "Problem 2:",
    "text": "Problem 2:\nYou have a 1D NumPy array of student scores (0–100).\n\nWrite a function pass_rate(scores, threshold) that returns the fraction of scores ≥ threshold.\nWrite a function top_percentile(scores, p) that returns the score value at the p-th percentile (e.g. p=0.9 ⇒ 90th percentile).\n\nConstraints:\n\nUse no Python loops or list-comprehensions.\nLeverage NumPy’s casting behavior (boolean \\(\\to\\) integer).\n\n\nimport numpy as np\nfrom typing import Union\n\nArray1D = Union[np.ndarray, list[float]]\n\ndef pass_rate(scores: Array1D, threshold: float) -&gt; float:\n    \"\"\"\n    Return the proportion of entries in `scores` &gt;= threshold.\n    \"\"\"\n    arr = np.asarray(scores)\n    # TODO: compute proportion without loops\n    ...\n\ndef top_percentile(scores: Array1D, p: float) -&gt; float:\n    \"\"\"\n    Return the p-th percentile of `scores`, where 0 &lt; p &lt; 1.\n    \"\"\"\n    arr = np.asarray(scores)\n    # TODO: use a single np function\n    ..."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-3-palindrome-checker",
    "href": "assignments/assignment-01.html#problem-3-palindrome-checker",
    "title": "Assignment 01",
    "section": "Problem 3: Palindrome Checker",
    "text": "Problem 3: Palindrome Checker\nWrite a function that checks whether a given string is a palindrome (a word, phrase, number, or other sequence of characters that reads the same forward and backward, ignoring spaces, punctuation, and capitalization). The function should return True if the string is a palindrome and False otherwise.\n\ndef is_palindrome(s: str) -&gt; bool:\n    # TODO: Implement the function to check if the string is a palindrome.\n    # Ignore spaces, punctuation, and capitalization.\n    pass"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-4-multiple-data-tables-and-pandas-operations",
    "href": "assignments/assignment-01.html#problem-4-multiple-data-tables-and-pandas-operations",
    "title": "Assignment 01",
    "section": "Problem 4: Multiple data tables and Pandas operations",
    "text": "Problem 4: Multiple data tables and Pandas operations\nWe did not cover how to do all this in class, but here are some exercises to practice merging and querying data with Pandas. Please refer to the Pandas documentation, the links below, and feel free to ask AI assistants to point you to the right functions and methods.\nSome related reading: - Blog post on relational databases - Joins in SQL / relational databases - Joins in Pandas\nBelow we define the data for a set of tables related to car registrations and traffic violations. You should be able to use the Pandas library to perform various queries on this data. Try the following exercises:\n\nShow all violations for “Alice Smith” (D001).\nHint: Join drivers → registrations → violations and filter where dl_number == \"D001\".\nFind violations on cars older than 2018.\nHint: Join cars → violations and filter where year &lt; 2018.\nWhich plates have zero recorded violations?\nHint: Anti-join registrations (or cars) against violations.\nCount total violations per driver (include drivers with none).\nHint: Merge all tables, then .groupby(\"dl_number\") + .size() (or .count()).\nCompute average fine per driver, sorted descending.\nHint: Group by dl_number and use .mean() on the fine column.\nFor each car make (e.g. Ford, Toyota), what is the total number of violations?\nHint: Join cars → violations → group by make and use .size().\nWhat’s the average fine by vehicle year?\nHint: Merge cars → violations, group by year, and use .mean() on fine.\n\n\nimport pandas as pd\nfrom pandas import DataFrame\n\n# — Driver info — (10 drivers)\ndrivers = pd.DataFrame([\n    {\"dl_number\": \"D001\", \"name\": \"Alice Smith\",   \"age\": 34},\n    {\"dl_number\": \"D002\", \"name\": \"Bob Jones\",     \"age\": 28},\n    {\"dl_number\": \"D003\", \"name\": \"Carol Diaz\",    \"age\": 45},\n    {\"dl_number\": \"D004\", \"name\": \"David Lee\",     \"age\": 52},\n    {\"dl_number\": \"D005\", \"name\": \"Eva Chen\",      \"age\": 23},\n    {\"dl_number\": \"D006\", \"name\": \"Frank Moore\",   \"age\": 36},\n    {\"dl_number\": \"D007\", \"name\": \"Grace Patel\",   \"age\": 41},\n    {\"dl_number\": \"D008\", \"name\": \"Henry Zhao\",    \"age\": 29},\n    {\"dl_number\": \"D009\", \"name\": \"Ivy Nguyen\",    \"age\": 50},\n    {\"dl_number\": \"D010\", \"name\": \"Jack O'Connor\", \"age\": 31},\n])\n\n# — Car registrations (DL → plate) — (15 registrations)\nregistrations = pd.DataFrame([\n    {\"dl_number\": \"D001\", \"plate\": \"ABC-123\"},\n    {\"dl_number\": \"D001\", \"plate\": \"XYZ-999\"},\n    {\"dl_number\": \"D002\", \"plate\": \"JKL-456\"},\n    {\"dl_number\": \"D003\", \"plate\": \"MNO-321\"},\n    {\"dl_number\": \"D004\", \"plate\": \"PQR-654\"},\n    {\"dl_number\": \"D005\", \"plate\": \"STU-111\"},\n    {\"dl_number\": \"D005\", \"plate\": \"VWX-222\"},\n    {\"dl_number\": \"D006\", \"plate\": \"YZA-333\"},\n    {\"dl_number\": \"D007\", \"plate\": \"BCD-444\"},\n    {\"dl_number\": \"D007\", \"plate\": \"EFG-555\"},\n    {\"dl_number\": \"D008\", \"plate\": \"HIJ-666\"},\n    {\"dl_number\": \"D009\", \"plate\": \"KLM-777\"},\n    {\"dl_number\": \"D009\", \"plate\": \"NOP-888\"},\n    {\"dl_number\": \"D010\", \"plate\": \"QRS-999\"},\n])\n\n# — Car info (plate → make/model/year) — (14 cars)\ncars = pd.DataFrame([\n    {\"plate\": \"ABC-123\", \"make\": \"Toyota\",  \"model\": \"Camry\",   \"year\": 2018},\n    {\"plate\": \"XYZ-999\", \"make\": \"Honda\",   \"model\": \"Civic\",   \"year\": 2020},\n    {\"plate\": \"JKL-456\", \"make\": \"Ford\",    \"model\": \"Escape\",  \"year\": 2019},\n    {\"plate\": \"MNO-321\", \"make\": \"Tesla\",   \"model\": \"Model 3\", \"year\": 2021},\n    {\"plate\": \"PQR-654\", \"make\": \"Nissan\",  \"model\": \"Altima\",  \"year\": 2017},\n    {\"plate\": \"STU-111\", \"make\": \"Chevy\",   \"model\": \"Malibu\",  \"year\": 2016},\n    {\"plate\": \"VWX-222\", \"make\": \"Kia\",     \"model\": \"Soul\",    \"year\": 2022},\n    {\"plate\": \"YZA-333\", \"make\": \"BMW\",     \"model\": \"X3\",      \"year\": 2015},\n    {\"plate\": \"BCD-444\", \"make\": \"Audi\",    \"model\": \"A4\",      \"year\": 2019},\n    {\"plate\": \"EFG-555\", \"make\": \"Hyundai\", \"model\": \"Elantra\", \"year\": 2020},\n    {\"plate\": \"HIJ-666\", \"make\": \"Subaru\",  \"model\": \"Outback\", \"year\": 2018},\n    {\"plate\": \"KLM-777\", \"make\": \"Ford\",    \"model\": \"Focus\",   \"year\": 2017},\n    {\"plate\": \"NOP-888\", \"make\": \"Mazda\",   \"model\": \"CX-5\",    \"year\": 2021},\n    {\"plate\": \"QRS-999\", \"make\": \"Chevy\",   \"model\": \"Impala\",  \"year\": 2014},\n])\n\n# — Traffic violations (plate → violation) — (20 entries)\nviolations = pd.DataFrame([\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-01\", \"type\": \"speeding\",   \"fine\": 150},\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-07\", \"type\": \"red_light\",  \"fine\": 200},\n    {\"plate\": \"XYZ-999\", \"date\": \"2025-06-11\", \"type\": \"seatbelt\",  \"fine\":  75},\n    {\"plate\": \"JKL-456\", \"date\": \"2025-06-05\", \"type\": \"parking\",    \"fine\":  40},\n    {\"plate\": \"MNO-321\", \"date\": \"2025-06-09\", \"type\": \"speeding\",   \"fine\": 120},\n    {\"plate\": \"PQR-654\", \"date\": \"2025-06-10\", \"type\": \"dui\",         \"fine\": 500},\n    {\"plate\": \"STU-111\", \"date\": \"2025-06-12\", \"type\": \"speeding\",   \"fine\": 130},\n    {\"plate\": \"VWX-222\", \"date\": \"2025-06-14\", \"type\": \"parking\",    \"fine\":  55},\n    {\"plate\": \"YZA-333\", \"date\": \"2025-06-15\", \"type\": \"red_light\",  \"fine\": 250},\n    {\"plate\": \"BCD-444\", \"date\": \"2025-06-16\", \"type\": \"speeding\",   \"fine\": 160},\n    {\"plate\": \"EFG-555\", \"date\": \"2025-06-17\", \"type\": \"parking\",    \"fine\":  35},\n    {\"plate\": \"HIJ-666\", \"date\": \"2025-06-18\", \"type\": \"seatbelt\",  \"fine\":  80},\n    {\"plate\": \"KLM-777\", \"date\": \"2025-06-19\", \"type\": \"speeding\",   \"fine\": 140},\n    {\"plate\": \"NOP-888\", \"date\": \"2025-06-20\", \"type\": \"dui\",         \"fine\": 600},\n    {\"plate\": \"QRS-999\", \"date\": \"2025-06-21\", \"type\": \"parking\",    \"fine\":  45},\n    {\"plate\": \"ABC-123\", \"date\": \"2025-06-22\", \"type\": \"speeding\",   \"fine\": 155},\n    {\"plate\": \"ZZZ-000\", \"date\": \"2025-06-12\", \"type\": \"speeding\",   \"fine\": 130}, \n    {\"plate\": \"UNKNOWN\", \"date\": \"2025-06-23\", \"type\": \"red_light\",  \"fine\": 220}, \n    {\"plate\": \"NOP-888\", \"date\": \"2025-06-24\", \"type\": \"seatbelt\",  \"fine\":  85},\n    {\"plate\": \"STU-111\", \"date\": \"2025-06-25\", \"type\": \"parking\",    \"fine\":  50},\n])"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-5-list-comprehension-and-lambda-functions",
    "href": "assignments/assignment-01.html#problem-5-list-comprehension-and-lambda-functions",
    "title": "Assignment 01",
    "section": "Problem 5: List Comprehension and Lambda Functions",
    "text": "Problem 5: List Comprehension and Lambda Functions\nRead about two advanced Python features we did not cover in class: - List comprehensions - Lambda functions\nThey are very useful for writing concise code, since they allow you to create lists and functions in a single line.\n\nWrite a list comprehension that builds a staircase pattern of the word “Staircases” with 10 steps, letter by letter.\n\nExample output:\n\n['S',\n 'St',\n 'Sta',\n 'Stai',\n 'Stair',\n 'Stairc',\n 'Stairca',\n 'Staircas',\n 'Staircase',\n 'Staircases']\nWrite a lambda function that takes a list of numbers and returns a new list containing only the numbers from the original list that are divisible by 3.\nWrite a lambda function that takes a string and returns the string reversed. (e.g. “shalom” becomes “molahs”). Then, apply this function to the column of names in the drivers table to create a new column called reversed_name. (You can use the apply method on a Pandas DataFrame to apply a function to each element in a column.)"
  },
  {
    "objectID": "assignments/assignment-01.html#problem-6-object-oriented-programming-oop-with-inheritance",
    "href": "assignments/assignment-01.html#problem-6-object-oriented-programming-oop-with-inheritance",
    "title": "Assignment 01",
    "section": "Problem 6: Object-Oriented Programming (OOP) with Inheritance",
    "text": "Problem 6: Object-Oriented Programming (OOP) with Inheritance\nObjective\nPractice defining base classes, subclasses, method overriding, etc.\nBuild a mini sports‐league system. You’ll model TeamMember, Player, Coach, and Team, then extend with sport‐specific players.\nComplete the class definitions below:\n\nfrom typing import List\n\nclass TeamMember:\n    def __init__(self, name: str, age: int) -&gt; None:\n        self.name = name\n        self.age = age\n\n    def role(self) -&gt; str:\n        \"\"\"Return the member’s role name.\"\"\"\n        ...\n\nclass Player(TeamMember):\n    def __init__(self, name: str, age: int, position: str, number: int) -&gt; None:\n        super().__init__(name, age)\n        self.position = position\n        self.number = number\n\n    def role(self) -&gt; str:\n        ...\n\n    def play(self) -&gt; str:\n        \"\"\"Return a generic “is playing” message.\"\"\"\n        ...\n\nclass Coach(TeamMember):\n    def __init__(self, name: str, age: int, experience_years: int) -&gt; None:\n        super().__init__(name, age)\n        self.experience_years = experience_years\n\n    def role(self) -&gt; str:\n        ...\n\n    def train(self, player: Player) -&gt; str:\n        \"\"\"Return a training message.\"\"\"\n        ...\n\nclass Team:\n    def __init__(self, name: str) -&gt; None:\n        self.name = name\n        self.members: List[TeamMember] = []\n\n    def add_member(self, member: TeamMember) -&gt; None:\n        ...\n\n    def lineup(self) -&gt; List[str]:\n        \"\"\"List ‘Name – Role’ for each member.\"\"\"\n        ...\n\n    def game_day(self) -&gt; None:\n        \"\"\"\n        For each Player, call play();\n        for each Coach, call train() on every Player.\n        Print each message.\n        \"\"\"\n        ...\n\nOnce the methods are all defined, you can create instances of these classes and test the functionality.\nInstantiate two teams, “HaPoel” and “Maccabi”. Try adding players and coaches to each team, and then print out the team lineups with .lineup() and simulate a gameday preparation with .game_day().\nNext: Define two subclasses of Player: BasketballPlayer and FootballPlayer (European football, not American). Each subclass should have a unique play() method that prints a sport-specific message. For example, BasketballPlayer might print “Nailing a three-pointer!” while FootballPlayer might print “Goooooooooal!”."
  },
  {
    "objectID": "assignments/assignment-01.html#problem-7-functional-vs.-object-oriented-programming-for-team-statistics",
    "href": "assignments/assignment-01.html#problem-7-functional-vs.-object-oriented-programming-for-team-statistics",
    "title": "Assignment 01",
    "section": "Problem 7: Functional vs. Object-Oriented Programming for team statistics",
    "text": "Problem 7: Functional vs. Object-Oriented Programming for team statistics\nThis exervise is designed to emphasize the differences between functional and object-oriented programming paradigms.\nWe’ll define a dataset with recorded game statistics for a team of players, and then implement two different approaches to compute the team’s statistical leaders: one using functional programming and the other using object-oriented programming.\n\ndata = pd.DataFrame({\n    \"game_id\": [\"001\", \"001\", \"002\", \"002\", \"002\"],\n    \"team\": [\"HaPoel\", \"Maccabi\", \"HaPoel\", \"Maccabi\", \"HaPoel\"],\n    \"player\": [\"Messi\", \"Ronaldo\", \"Messi\", \"Wagner\", \"Neymar\"],\n    \"goals\": [1, 2, 1, 0, 1],\n    \"assists\": [0, 1, 0, 2, 1],\n    \"minutes_played\": [90, 90, 90, 90, 90]\n})\n\n# compute the scoring leader for a given team\ndef compute_scoring_leader(data, team: str) -&gt; str:\n    ...\n    \n# compute the scoring leader for a given team using OOP\nclass TeamStats:\n    def __init__(self, data: pd.DataFrame, team: str) -&gt; None:\n        ...\n    \n    def scoring_leader(self) -&gt; str:\n        ..."
  },
  {
    "objectID": "slides/lecture-01-slides.html#variables",
    "href": "slides/lecture-01-slides.html#variables",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables",
    "text": "Variables\nVariables are used to store data in a program. They can hold different types of data, such as numbers, strings (text), lists, and more.\nIt is both useful and pretty accurate to think of programmatic variables in the same way you think of algebraic variables in math. You can assign or change the value of a variable, and you can use it in calculations or operations."
  },
  {
    "objectID": "slides/lecture-01-slides.html#aside-functions",
    "href": "slides/lecture-01-slides.html#aside-functions",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Aside: Functions",
    "text": "Aside: Functions\n\n\n\n\n\n\n\nFunctions act on variables\n\n\nFunctions in programming are designed to operate on variables. They take input (variables), perform some operations, and return output. Understanding how variables work is crucial for effectively using functions.\nWe’ll explore functions in more detail later (Functions), but for now, remember that functions are named blocks of code that manipulate variables to achieve specific tasks.\nSome functions are built-in, meaning they are provided by the programming language itself, while others can be defined by the user. Built-in functions in Python include print() for displaying output, as well as type() for checking the type of a variable."
  },
  {
    "objectID": "slides/lecture-01-slides.html#creating-variables",
    "href": "slides/lecture-01-slides.html#creating-variables",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Creating Variables",
    "text": "Creating Variables\nYou can create a variable by assigning it a value using the equals sign (=).\nFor example, if you create a variable x that holds the value 5, you can use it in calculations like this:\nx = 5\ny = x + 3\nprint(y)  # Output: 8"
  },
  {
    "objectID": "slides/lecture-01-slides.html#variable-types",
    "href": "slides/lecture-01-slides.html#variable-types",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variable Types",
    "text": "Variable Types\nThe following table describes some common variable types:\n\n\n\n\n\n\n\nVariable Type\nDescription\n\n\n\n\nInteger (int)\nWhole numbers, e.g., 5, -3, 42\n\n\nFloat (float)\nDecimal numbers, e.g., 3.14, -0.001, 2.0\n\n\nString (str)\nTextual data, e.g., \"Hello, world!\", 'Python'\n\n\nList (list)\nOrdered collection of items, e.g., [1, 2, 3], ['a', 'b', 'c']\n\n\nDictionary (dict)\nKey-value pairs, e.g., {'name': 'Alice', 'age': 30}\n\n\nBoolean (bool)\nTrue or False values, e.g., True, False"
  },
  {
    "objectID": "slides/lecture-01-slides.html#variables-are-objects",
    "href": "slides/lecture-01-slides.html#variables-are-objects",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Variables are Objects",
    "text": "Variables are Objects\nIn Python, everything is an object.\n\nThis means that even basic data types like integers and strings are treated as objects with methods and properties.\nFor example, you can call methods on a string object to manipulate it, like my_string.upper() to convert it to uppercase.\n\nSee the later section on Object-Oriented Programming for more details."
  },
  {
    "objectID": "slides/lecture-01-slides.html#typecasting",
    "href": "slides/lecture-01-slides.html#typecasting",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Typecasting",
    "text": "Typecasting\nTypecasting is the process of converting a variable from one type to another. Not all type conversions are allowed, but some common ones include:\n\n\n\n\n\n\n\n\nFrom Type\nTo Type\nExample\n\n\n\n\nInteger\nFloat\nfloat(5) → 5.0\n\n\nFloat\nInteger\nint(3.14) → 3\n\n\nString\nInteger\nint('42') → 42\n\n\nString\nFloat\nfloat('3.14') → 3.14\n\n\nList\nString\n''.join(['S', 'h', 'a', 'l', 'o', 'm']) → 'Shalom'\n\n\nString\nList\nlist('Shalom') → ['S', 'h', 'a', 'l', 'o', 'm']\n\n\nBoolean\nInteger\nint(True) → 1, int(False) → 0\n\n\nInteger\nBoolean\nbool(1) → True, bool(0) → False, bool(-1) → True"
  },
  {
    "objectID": "slides/lecture-01-slides.html#implicit-type-conversions",
    "href": "slides/lecture-01-slides.html#implicit-type-conversions",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Implicit Type Conversions",
    "text": "Implicit Type Conversions\nThere are also some implicit type conversions that happen automatically in Python, such as when you perform arithmetic operations between integers and floats. For example:\n\nx = 5        # Integer\ny = 2.0      # Float\nresult = x + y  # Implicit conversion to float\nresult, type(result)  # result is now a float\n\n(7.0, float)"
  },
  {
    "objectID": "slides/lecture-01-slides.html#lists",
    "href": "slides/lecture-01-slides.html#lists",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Lists",
    "text": "Lists\n\nWe often need to store multiple values together.\nThe most basic way to achieve this is with a list.\nA list is an ordered collection of items that can be of any type, including other lists.\n\n“Ordered” means that the items have a specific sequence, and you can access them by their position (index) in the list.\n\n\nIn Python, you can create a list using square brackets []. For example:\n\nmy_list = [1, 2, 3, 'apple', 'banana']\nprint(my_list[0]) \n\n1\n\n\nYou can access items in a list using their index (a number specifying their position). In Python, indexing starts at 0, so my_list[0] refers to the first item in the list.\nIndexing also works with negative numbers, which count from the end of the list. For example, my_list[-1] refers to the last item in the list.\nThe syntax for retrieving indexes is my_list[start:end:step], where start is the index to start from, end is the index to stop before, and step is the interval between items. If you omit start, it defaults to 0; if you omit end, it defaults to the end of the list; and if you omit step, it defaults to 1.\n\n\nCode\nprint(my_list[:3]) # first three elements\nprint(my_list[3:]) # from the fourth element to the end\nprint(my_list[::2]) # every other\nprint(my_list[::-1])  # reverse the list\n\n\n[1, 2, 3]\n['apple', 'banana']\n[1, 3, 'banana']\n['banana', 'apple', 3, 2, 1]\n\n\nYou can also modify lists by adding or removing items. For example:\n\n\nCode\nmy_list.append('orange')  # Adds 'orange' to the end of the list\nprint(my_list)  # Output: [1, 2, 3, 'apple', 'banana', 'orange']\n\n\n[1, 2, 3, 'apple', 'banana', 'orange']"
  },
  {
    "objectID": "slides/lecture-01-slides.html#arrays-numpy",
    "href": "slides/lecture-01-slides.html#arrays-numpy",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Arrays (NumPy)",
    "text": "Arrays (NumPy)\nWhile lists are flexible, they can be inefficient and unreliable for many numerical operations. Arrays, provided by the core library numpy, enforce a single data type and are optimized for numerical computations. They also have lots of built-in functionality for mathematical operations.\n\n\n\n\n\n\n\nPackages\n\n\nThere is only so much functionality that can be included in a core programming language. To keep the language simple, many advanced features are provided through external packages.\nPackages are collections of pre-written code that you can import into your program to use their features. When you want to use a package, you typically import it at the beginning of your script. For example, to use NumPy, you would write:\nimport numpy as np\nnp is now what we call an alias, a shorthand for referring to the NumPy package.\nNow any time you want to use a function (we’ll discuss functions in detail later) from NumPy, you can do so by prefixing it with np.. For example, we’ll see how to create a NumPy array below using np.array().\n\n\n\n\nYou can create a NumPy array using the numpy.array() command. For example:\n\n\nCode\nimport numpy as np\nmy_array = np.array([1, 2, 3, 4, 5])\nprint(my_array)  \n\n\n[1 2 3 4 5]\n\n\nYou can perform mathematical operations on NumPy arrays, and they will be applied element-wise. For example:\n\n\nCode\nmy_array_squared = my_array ** 2\nprint(my_array_squared)  \n\n\n[ 1  4  9 16 25]\n\n\nYou can’t have mixed data types in a NumPy array, so if you try to create an array with both numbers and strings, it will convert everything to strings:\n\n\nCode\nmixed_array = np.array([1, 'two', 3.0])\nprint(mixed_array)  # Output: ['1' 'two' '3.0']\n\n\n['1' 'two' '3.0']\n\n\nTypecasting works in NumPy arrays as well via the astype() method.\n\nnp.array([1, 2, 3, 4.1], dtype=float).astype(int)  # Convert float array to int array\n\narray([1, 2, 3, 4])"
  },
  {
    "objectID": "slides/lecture-01-slides.html#advanced-indexing",
    "href": "slides/lecture-01-slides.html#advanced-indexing",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Advanced indexing",
    "text": "Advanced indexing\nNumPy arrays support complex indexing, allowing you to access and manipulate specific elements or subarrays efficiently.\nYou can actually use arrays to index other arrays, which is a powerful feature. This allows you to select specific elements based on conditions or patterns.\n\nmy_array = np.arange(1, 11)\nprint(my_array) \n# grab specific elements\nidx = [1, 1, 3, 4]\nprint(my_array[idx])\n\n[ 1  2  3  4  5  6  7  8  9 10]\n[2 2 4 5]\n\n\nOne important feature is boolean indexing, where you can use a boolean array to select elements from another array. This lets you filter data based on conditions. For example:\n\n\nCode\nmy_array = np.arange(1, 11)  # Creates a NumPy array with values from 1 to 10\nprint(\"Original array:\", my_array)\n# Create a boolean array where elements are greater than 2\nboolean_mask = my_array &gt; 2\nprint(\"Boolean mask:\", boolean_mask)\n# Use the boolean mask to filter the array\nfiltered_array = my_array[boolean_mask]\nprint(\"Filtered array:\", filtered_array) \n\n\nOriginal array: [ 1  2  3  4  5  6  7  8  9 10]\nBoolean mask: [False False  True  True  True  True  True  True  True  True]\nFiltered array: [ 3  4  5  6  7  8  9 10]"
  },
  {
    "objectID": "slides/lecture-01-slides.html#dictionaries",
    "href": "slides/lecture-01-slides.html#dictionaries",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dictionaries",
    "text": "Dictionaries\nSometimes a list or array is not enough. You may want to store data in a way that allows you to access it by a keyword rather than by an index. For example, I might have a list of people and their ages, but I want to be able to look up a person’s age by their name. In this case, I can use a dictionary.\nWe can create a dictionary using curly braces {} and separating keys and values with a colon :. Here’s an example:\n\nname_age_dict = {\n    \"Alice\": 30,\n    \"Bob\": 25,\n    \"Charlie\": 35\n}\n\nIn order to access a value in a dictionary, we use the key in square brackets []. Here’s how you can do that:\n\nname_age_dict[\"Bob\"] # this will print Bob's age\n\n25\n\n\nThe “value” in a dictionary can be of any type, including another dictionary or a list. This allows for building up complex data structures that contain named entities and their associated data.\nFor example, you might have a dictionary that contains different types of data about a person.\n\nname_age_list_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n}"
  },
  {
    "objectID": "slides/lecture-01-slides.html#dataframes",
    "href": "slides/lecture-01-slides.html#dataframes",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Dataframes",
    "text": "Dataframes\nMost of the time, data scientists work with tabular data (data organized in tables with rows and columns).\nThink of the data you typically see in spreadsheets – rows represent individual records, and columns represent attributes of those records.\nIn Python, the most common way to work with tabular data is through the pandas library, which provides a powerful data structure called a DataFrame.\n\n\nCode\nimport pandas as pd\n# Create a DataFrame with sample data\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height (cm)': [165, 180, 175],\n    'Weight (kg)': [55.1, 80.5, 70.2],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n})\ndf\n\n\n\n\n\n\n\n\n\nName\nAge\nHeight (cm)\nWeight (kg)\nCity\n\n\n\n\n0\nAlice\n25\n165\n55.1\nNew York\n\n\n1\nBob\n30\n180\n80.5\nLos Angeles\n\n\n2\nCharlie\n35\n175\n70.2\nChicago\n\n\n\n\n\n\n\nOne import thing to realize about DataFrames that each column can have a different data type. For example, one column might contain integers, another might contain strings, and yet another might contain floating-point numbers.\nHowever, all the values in a single column should be of the same type. Intuitively: since columns represent attributes, every value in a column should represent the same kind of information. It wouldn’t make sense if the “city” column of a DataFrame contained both “New York” (a string) and 42 (an integer).\nNote that this rule isn’t necessarily enforced by the DataFrame structure itself, but it’s a good practice to follow. Otherwise, you might run into issues when performing operations on the DataFrame.\n\n\nCode\nbad_df = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 'Thirty-Five'],  # Mixed types in the 'Age' column\n})\n\nbad_df[\"Age\"] * 3\n\n\n0                                   75\n1                                   90\n2    Thirty-FiveThirty-FiveThirty-Five\nName: Age, dtype: object"
  },
  {
    "objectID": "slides/lecture-01-slides.html#conditional-logic",
    "href": "slides/lecture-01-slides.html#conditional-logic",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Conditional logic",
    "text": "Conditional logic\nConditional logic allows you to make decisions in your code based on certain conditions. This is essential for controlling the flow of your program and executing different actions based on different situations.\n\nIf-elif-else statements\nThe most common way to implement conditional logic is through if, elif, and else statements:\n\n\n\n\n\n\n\nStatement Type\nDescription\n\n\n\n\nif\nChecks a condition and executes the block if it’s true.\n\n\nelif\nChecks another condition if the previous if or elif was false.\n\n\nelse\nExecutes a block if all previous conditions were false.\n\n\n\nHere’s an example of how to use these statements. Play around with the code below to see how it works. You can change the value of age to see how the output changes based on different conditions.\nage = #TODO: Set your age here\nif age &lt; 18:\n    print(\"You are a minor.\")\nelif age &lt; 65:\n    print(\"You are an adult.\")\nelif age &gt;= 120:\n    print(\"You've done your time, haven't you?\")\nelse:\n    print(\"You are a senior citizen.\")\nNote that the elif and else statements are optional. You can have just an if statement, which will execute a block of code if the condition is true and skip it if the condition is false.\n\n\n\n\n\n\n\nBoolean expressions\n\n\nBoolean expressions are conditions that evaluate to either True or False. They are often used in if statements to control the flow of the program. Common operators for creating Boolean expressions include:\n\n\n\nOperator\nDescription\n\n\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\nand , &\nLogical AND\n\n\nor, |\nLogical OR\n\n\nnot , ~\nLogical NOT"
  },
  {
    "objectID": "slides/lecture-01-slides.html#loops",
    "href": "slides/lecture-01-slides.html#loops",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Loops",
    "text": "Loops\nLoops are special constructs that allow you to repeat a block of code multiple times in sequence. They are useful when you want to perform the same operation on multiple items, such as iterating over a list or processing each row in a DataFrame.\nThe two most common types of loops are for loops and while loops.\nFor Loops\nA for loop iterates over a sequence (like a list or a string) and executes a block of code for each item in that sequence. Here’s an example:\nmy_list = [1, 2, 3, 4, 5]\nfor item in my_list:\n    print(item)\nThis will print each item in my_list one by one.\n\n\n\n\n\n\n\nUseful Python functions: range() and enumerate()\n\n\nIn Python, the range() function generates a sequence of numbers, which is often used in for loops. For example, range(5) generates the numbers 0 to 4. The enumerate() function is useful when you need both the index and the value of items in a list. It returns pairs of (index, value) for each item in the list. For example:\nmy_list = ['a', 'b', 'c']\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n\n\n\n\nWhile Loops\nA while loop continues to execute a block of code as long as a specified condition is true. Here’s an example:\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1 # Increment the count\nThis will print the numbers 0 to 4, incrementing count by 1 each time until the condition count &lt; 5 is no longer true."
  },
  {
    "objectID": "slides/lecture-01-slides.html#functions-and-functional-programming",
    "href": "slides/lecture-01-slides.html#functions-and-functional-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Functions and functional programming",
    "text": "Functions and functional programming\nFunctions are reusable blocks of code that perform a specific task. They allow you to organize your code into logical sections, making it easier to read, maintain, and reuse.\nThey work like functions in math: you can pass inputs (arguments) to a function, and it will return an output (result). You can define a function in Python using the def keyword, followed by the function name and parentheses containing any parameters. Here’s an example:\ndef add_numbers(a, b):\n    \"\"\"Adds two numbers and returns the result.\"\"\"\n    return a + b\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nFunctions can also have default values for parameters, which allows you to call them with fewer arguments than defined. For example:\ndef greet(name=\"World\"):\n    \"\"\"Greets the specified name or 'World' by default.\"\"\"\n    return f\"Hello, {name}!\"\nprint(greet())          # Output: Hello, World!\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\nFunctional programming is a style of programming that treats computer programs as the evaluation of mathematical functions. It is alternatively called value-oriented programming1 because the output of a program is just the value(s) it produces as a function of its inputs.\nProbably the core principle of functional programming is to avoid changing state and mutable data. This means that once a value is created, it should not be changed. Instead, you create new values based on existing ones.\nThat means means that functions should not have side effects – they use data passed to them and return a new value without modifying the input data. This makes it easier to reason about code, as you can understand what a function does just by looking at its inputs and outputs.\nFor example, consider the following two functions for squaring a number:\n\n\nCode\nimport numpy as np\n\ndef square_functional(input):\n    \"\"\"Returns the square of an array\"\"\"\n    return input ** 2\n\ndef square_side_effect(input):\n    \"\"\"Returns the square of an array with a side effect\"\"\"\n    input[0] = -1\n    return input ** 2  # This is a side effect, modifying the first element of input\n\na = np.array([1, 3, 5])\nb = square_functional(a)  # b will be 25, a remains 5\nprint(f\"Functional: a = {a}, b = {b}\")\nc = square_side_effect(a)  # c will be 25, a will still be 5\nprint(f\"Side Effect: a = {a}, c = {c}\")\n\n\nFunctional: a = [1 3 5], b = [ 1  9 25]\nSide Effect: a = [-1  3  5], c = [ 1  9 25]\n\n\nThere are somewhat complicated rules about what objects can be modified in place and what cannot (sometimes Python allows it, sometimes it doesn’t), but the general rule is that you should avoid modifying objects in place unless you have a good reason to do so. The main reason is that you might inadvertently change the value of an object that is being used elsewhere in your code, leading to bugs that are hard to track down. Instead, create new objects based on existing ones.\nTechnically there is a difference between functional programming and value-oriented programming that programming-language nerds care about, but for our purposes, they are the same thing."
  },
  {
    "objectID": "slides/lecture-01-slides.html#object-oriented-programming",
    "href": "slides/lecture-01-slides.html#object-oriented-programming",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Object-Oriented Programming",
    "text": "Object-Oriented Programming\nWhile you can write programs in Python using just functions, the language is really designed for object-oriented programming (OOP). OOP is a style of programming built around the concept of “objects”, which are specific instances of classes.\nA class is like a template for creating new objects. It defines the properties (attributes) and \\ behaviors (methods) that the objects created from the class will have.\nTo define a class in Python, you use the class keyword followed by the class name. Every class should have an __init__ method, which is a special method that initializes the object when it is created.\nHere’s a simple example of a class:\n\n\nCode\nclass Date():\n    \"\"\"A simple class to represent a date\"\"\"\n\n    # This is the constructor method, called when an instance is created like Date(2025, 5, 6)\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        # defined what print() should do\n        # formats the date as YYYY-MM-DD\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n    \n    # here is a method that checks if the date is in summer\n    def is_summer(self):\n        \"\"\"Check if the date is in summer (June, July, August)\"\"\"\n        return self.month in [6, 7, 8]\n\n# Create an instance of the Date class\ndate_instance = Date(2025, 5, 6)\n\nprint(date_instance)  # Output: 2025-05-06\nprint(date_instance.is_summer())  # Output: False\n\n\n2025-05-06\nFalse\n\n\nObject-oriented programming has a number of advantages, but many of them are really just about organizing code in a way that makes it easier to understand, reuse, and maintain.\nOne of the key features of OOP is inheritance, which allows you to create new classes based on existing ones. This means you can define a base class with common attributes and methods, and then create subclasses that inherit from it and add or override functionality.\nFor example, you might inherit from the base class Date to create a subclass HolidayDate that adds specific attributes or methods related to holidays:\n#| setup: true\n#| exercise: holiday_date\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __str__(self):\n        return f\"{self.year}-{self.month:02d}-{self.day:02d}\"\n#| exercise: holiday_date\nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        # TODO: Add a new attribute for the holiday name\n\n    def print_holiday(self):\n        # TODO: Print the date and the holiday name\n\nchannukah = HolidayDate(2025, 12, 7, \"Channukah\")\nprint(channukah)  # Output: 2025-12-07\nchannukah.print_holiday()  # Output: Channukah is on 2025-12-07.\n\nSolution. \nclass HolidayDate(Date):\n    def __init__(self, year, month, day, holiday_name):\n        super().__init__(year, month, day)\n        self.holiday_name = holiday_name\n\n    def print_holiday(self):\n        print(f\"{self.holiday_name} is on {self}.\")\n\nThis allows you to create specialized versions of a class without duplicating code, making your codebase cleaner and easier to maintain.\nFor the purposes of statistics and data science, classes are mostly useful because they allow you to create custom data structures that can hold both data and methods for manipulating that data. We have already seen this in the context of DataFrames – the pandas library defines a DataFrame class that has methods for manipulating tabular data. By defining and using DataFrame objects, you get access to a wide range of functionality for working with data without having to implement it yourself. For example, you can filter rows, group data, and perform aggregations (like mean, sum, etc.) using methods defined in the DataFrame class."
  },
  {
    "objectID": "slides/lecture-01-slides.html#summary",
    "href": "slides/lecture-01-slides.html#summary",
    "title": "Lecture 01: Programming Basics, Data Structures, and Data Manipulation",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered some of the core programming concepts that are important to understand when working with Python or any other programming language. In today’s assignment, you will practice these concepts by writing Python code to solve some problems."
  }
]