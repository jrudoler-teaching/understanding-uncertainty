{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b3fe5e23",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Lecture 09: Regression Cont.\"\n",
    "date: \"now\"\n",
    "format: \n",
    "    live-html:\n",
    "        toc: true\n",
    "        toc-location: right\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        code-summary: \"Code\"\n",
    "        code-block-name: \"Code\"\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "code-annotations: hover\n",
    "filters:\n",
    "  - shinylive\n",
    "pyodide:\n",
    "  packages:\n",
    "    - matplotlib\n",
    "    - numpy\n",
    "    - statsmodels\n",
    "    - pandas\n",
    "execute:\n",
    "    warning: false\n",
    "    enabled: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7031f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| hide: true\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640d3ca",
   "metadata": {},
   "source": [
    "<!-- ```{shinylive-python}\n",
    "#| standalone: true\n",
    "#| viewerHeight: 1000\n",
    "#| viewerWidth: 600\n",
    "\n",
    "from shiny import App, ui, render\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "app_ui = ui.page_fluid(\n",
    "    ui.input_slider(\"n\", \"Number of bins\", min=0, max=100, value=40, step=1),\n",
    "    ui.input_slider(\"noise\", \"Noise level\", min=0, max=1, value=0.5, step=0.01),\n",
    "    ui.input_slider(\"beta\", \"Slope of regression line\", min=-2, max=2, value=0.5, step=0.1),\n",
    "    ui.output_plot(\"plot\")\n",
    ")\n",
    "def server(input, output, session):\n",
    "    @output\n",
    "    @render.plot(alt=\"Average Y by binned X\", width=600, height=400)\n",
    "    def plot():\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "        x = rng.normal(size=1000)\n",
    "        y = input.beta() * x + input.noise() * rng.normal(size=1000)\n",
    "\n",
    "        data = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "        sns.scatterplot(x='x', y='y', data=data, alpha=0.3)\n",
    "        sns.regplot(x='x', y='y', x_bins=input.n(), data=data, ci=None, fit_reg=False, marker=\"x\", color='red')\n",
    "\n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.show()\n",
    "\n",
    "app = App(app_ui, server)\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b5869",
   "metadata": {},
   "source": [
    "## Inference in Regression\n",
    "\n",
    "Last lecture we introduced **linear regression** as a way to model the relationship between a response variable $Y$ and some predictor variable $X$. The model assumes that the relationship can be described by a linear equation:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$\n",
    "\n",
    "where $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\epsilon$ is the error term. The error term captures the variability in $Y$ that cannot be explained by $X$, and we assume it follows a normal distribution with mean 0 (i.e. the deviations from the true regression line are zero on average) and some variance $\\sigma^2$.\n",
    "\n",
    "Throughout this course, we have focused on **sampling variability** and how it affects our ability to make inferences about a population based on a sample. In the context of regression, this means that if we take different samples from the same population, we will get different estimates of the regression coefficients $\\beta_0$ and $\\beta_1$. This variability is due to the random nature of sampling and the inherent noise in the data. \n",
    "\n",
    "The app below illustrates this concept by allowing you to draw samples from a population and see how the estimated regression coefficient ($\\hat{\\beta}_1$) varies across samples. Notice that the true slope of the regression line is fixed, but the estimated slope varies due to sampling variability. \n",
    "\n",
    "You can also see how the stability of the estimate improves as the sample size increases, as well as the confidence implied by the $p$-value of the hypothesis test for the slope ($H_0: \\beta_1 = 0$). We will unpack this in more detail shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3e61e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":::{.column-screen-inset}\n",
    "```{shinylive-python}\n",
    "#| standalone: true\n",
    "#| viewerHeight: 700\n",
    "\n",
    "from shiny import App, ui, render, reactive\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Fixed population\n",
    "rng = np.random.default_rng(123)\n",
    "N_POP = 2000\n",
    "x_pop = rng.normal(size=N_POP)\n",
    "true_beta = 0.5\n",
    "y_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\n",
    "population = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\n",
    "\n",
    "app_ui = ui.page_fluid(\n",
    "    # Title\n",
    "    ui.div(\n",
    "        ui.h2(\"Sampling Variability in Regression Inference\"),\n",
    "        style=\"text-align:center; margin-bottom:20px; margin-top:20px; font-weight:bold;\"\n",
    "    ),\n",
    "    # Top row with inputs\n",
    "    ui.layout_columns(\n",
    "        ui.input_slider(\"n\", \"Sample size\", min=5, max=200, value=50, step=5, width=\"300px\"),\n",
    "        ui.input_numeric(\"perm\", \"Number of permutations\", value=500, min=100, max=2000, step=100, width=\"200px\"),\n",
    "        ui.input_action_button(\"resample\", \"Draw New Sample\", width=\"200px\"),\n",
    "    ),\n",
    "    # Second row with two side-by-side plots\n",
    "    ui.navset_tab(\n",
    "        ui.nav_panel(\"Visualization\",\n",
    "            ui.div(\n",
    "                ui.output_text(\"coef_text\"),\n",
    "                style=\"text-align:center; font-size:18px; margin-top:10px; margin-bottom:10px;\"\n",
    "            ),\n",
    "            ui.layout_columns(\n",
    "                ui.output_plot(\"scatter_plot\"),\n",
    "                ui.output_plot(\"perm_plot\"),\n",
    "            )\n",
    "        ),\n",
    "        ui.nav_panel(\"Permutation Table\",\n",
    "            ui.div(\n",
    "                \"First 50 rows of original sample and first 10 permutations:\",\n",
    "                style=\"text-align:center; font-size:16px; margin-top:10px; margin-bottom:10px;\"\n",
    "            ),\n",
    "            ui.output_table(\"perm_table\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "def server(input, output, session):\n",
    "    # ✅ Store the current sample as state\n",
    "    current_sample = reactive.value(pd.DataFrame())\n",
    "\n",
    "    # Update sample when button is clicked or sample size changes\n",
    "    @reactive.effect\n",
    "    @reactive.event(input.resample, input.n)\n",
    "    def _():\n",
    "        idx = rng.choice(len(population), size=input.n(), replace=False)\n",
    "        current_sample.set(population.iloc[idx])\n",
    "\n",
    "    # ✅ Compute regression results based on stored sample\n",
    "    @reactive.calc\n",
    "    def regression_results():\n",
    "        data = current_sample()\n",
    "        if data.empty:\n",
    "            return {\"beta_hat\": np.nan, \"p_val\": np.nan, \"perm_coefs\": np.array([]), \"perm_table\": pd.DataFrame()}\n",
    "\n",
    "        model = smf.ols(\"y ~ x\", data=data).fit()\n",
    "        beta_hat = model.params[\"x\"]\n",
    "        p_val = model.pvalues[\"x\"]\n",
    "\n",
    "        perm_coefs = []\n",
    "        perm_tables = []\n",
    "        for i in range(input.perm()):\n",
    "            y_perm = rng.permutation(data[\"y\"])\n",
    "            perm_model = smf.ols(\"y ~ x\", data=data.assign(y=y_perm)).fit()\n",
    "            perm_coefs.append(perm_model.params[\"x\"])\n",
    "\n",
    "            if i < 10:  # store first 10 permutations for table\n",
    "                df_perm = pd.DataFrame({\"x\": data[\"x\"].values, f\"y_perm_{i+1}\": y_perm})\n",
    "                perm_tables.append(df_perm[[f\"y_perm_{i+1}\"]])\n",
    "\n",
    "        # Merge the first few permutations into one table\n",
    "        if perm_tables:\n",
    "            perm_table_df = pd.concat([data[[\"x\", \"y\"]].reset_index(drop=True)] + perm_tables, axis=1)\n",
    "        else:\n",
    "            perm_table_df = pd.DataFrame()\n",
    "        \n",
    "        # compute permutation p-value\n",
    "        perm_coefs = np.array(perm_coefs)\n",
    "        # two-tailed p-value\n",
    "        p_val_perm = np.mean(np.abs(perm_coefs) >= np.abs(beta_hat))\n",
    "\n",
    "        return {\n",
    "            \"beta_hat\": beta_hat,\n",
    "            \"p_val\": p_val,\n",
    "            \"p_val_perm\": p_val_perm,\n",
    "            \"perm_coefs\": np.array(perm_coefs),\n",
    "            \"perm_table\": perm_table_df\n",
    "        }\n",
    "\n",
    "    @output\n",
    "    @render.text\n",
    "    def coef_text():\n",
    "        res = regression_results()\n",
    "        if np.isnan(res[\"beta_hat\"]):\n",
    "            return \"Click 'Draw New Sample' to begin.\"\n",
    "        return f\"True slope: {true_beta:.3f}, Estimated slope: {res['beta_hat']:.3f}, Parametric p-value: {res['p_val']:.4f}, Permutation p-value: {res['p_val_perm']:.4f}\"\n",
    "\n",
    "    @output\n",
    "    @render.plot\n",
    "    def scatter_plot():\n",
    "        data = current_sample()\n",
    "        res = regression_results()\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.scatterplot(x=\"x\", y=\"y\", data=population, alpha=0.1, color=\"gray\")\n",
    "\n",
    "        if not data.empty:\n",
    "            sns.scatterplot(x=\"x\", y=\"y\", data=data, color=\"blue\", label=\"Sampled Points\")\n",
    "            x_vals = np.linspace(population[\"x\"].min(), population[\"x\"].max(), 100)\n",
    "            y_vals = res[\"beta_hat\"] * x_vals + data[\"y\"].mean() - res[\"beta_hat\"] * data[\"x\"].mean()\n",
    "            plt.plot(x_vals, y_vals, color=\"red\", lw=2, label=\"Estimated Regression Line\")\n",
    "            # Add true population regression line\n",
    "            plt.plot(x_vals, true_beta * x_vals, alpha=0.3, color=\"gray\", linestyle=\"--\", label=\"True Regression Line\")\n",
    "            plt.legend()\n",
    "\n",
    "\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.xlim(population[\"x\"].min(), population[\"x\"].max())\n",
    "        plt.ylim(population[\"y\"].min(), population[\"y\"].max())\n",
    "        plt.title(\"Sampled Points and Fitted Regression Line\")\n",
    "\n",
    "    @output\n",
    "    @render.plot\n",
    "    def perm_plot():\n",
    "        res = regression_results()\n",
    "        if len(res[\"perm_coefs\"]) == 0:\n",
    "            plt.figure()\n",
    "            plt.text(0.5, 0.5, \"Click 'Draw New Sample' to start.\", ha=\"center\", va=\"center\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        sns.histplot(res[\"perm_coefs\"], bins=30, kde=False, color=\"gray\")\n",
    "        plt.axvline(res[\"beta_hat\"], color=\"red\", lw=2, label=\"Observed β̂\")\n",
    "        plt.axvline(true_beta, color=\"blue\", lw=2, linestyle=\"--\", label=\"True β\")\n",
    "        plt.xlabel(r\"Estimated slope ($\\hat{\\beta}$)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlim(min(-1, res[\"perm_coefs\"].min()), max(1, res[\"perm_coefs\"].max()))\n",
    "        plt.title(r\"Null Distribution of $\\hat{\\beta}$ (Permutation Test)\")\n",
    "        plt.legend()\n",
    "    @output\n",
    "    @render.table\n",
    "    def perm_table():\n",
    "        res = regression_results()\n",
    "        df = res[\"perm_table\"]\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        return df.head(50)  # Only show first 50 rows\n",
    "\n",
    "app = App(app_ui, server)\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49444bc1",
   "metadata": {},
   "source": [
    ":::{.callout-note title=\"Fitting the regression model\" collapse=\"true\"}\n",
    "As in the previous lecture, we use the `statsmodels` library to fit the regression model. \n",
    "\n",
    "Under the hood, the app is doing the following:\n",
    "\n",
    "```python\n",
    "# Fixed population\n",
    "rng = np.random.default_rng(123)\n",
    "N_POP = 2000\n",
    "x_pop = rng.normal(size=N_POP)\n",
    "true_beta = 0.5\n",
    "y_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\n",
    "population = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\n",
    "```\n",
    "This creates a fixed population of 2000 points with a known slope of 0.5. The app then allows you to draw samples from this population and fit a regression model to the sampled data.\n",
    "\n",
    "More specifically, we subsample the population to create a new sample of size `n`:\n",
    "\n",
    "```python\n",
    "idx = rng.choice(len(population), size=n, replace=False)\n",
    "data = population.iloc[idx]\n",
    "```\n",
    "Then we fit the regression model using `statsmodels`:\n",
    "```python\n",
    "model = smf.ols('y ~ x', data=data).fit()\n",
    "    beta_hat = model.params['x']\n",
    "```\n",
    ":::\n",
    "\n",
    "\n",
    "### Hypothesis Testing in Regression\n",
    "\n",
    "In regression analysis, we often want to test hypotheses about the relationship between the predictor variable $X$ and the response variable $Y$. The most common hypothesis to test is whether the slope of the regression line ($\\beta_1$) is different from zero, which would indicate that there is a relationship between $X$ and $Y$.\n",
    "\n",
    "So we can set up the following hypotheses:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0:& ~\\beta_1 = 0 \\quad \\text{(no relationship)} \\\\\n",
    "H_1:& ~\\beta_1 \\neq 0 \\quad \\text{(there is a relationship)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We're back in our comfort zone now. How can we simulate the distribution of the estimated slope $\\hat{\\beta}_1$ under the null hypothesis? We'll use a **permutation test** -- take a shot at implementing it in the code cell below.\n",
    "\n",
    "\n",
    "::::{.panel-tabset}\n",
    "\n",
    "## Exercise\n",
    "\n",
    "```{pyodide}\n",
    "#| exercise: hypothesis-testing-regression\n",
    "#| caption: Write a permutation test to simulate the distribution of the estimated slope under the null hypothesis that there is no relationship between X and Y.\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "x = rng.normal(size=100)\n",
    "y = 0.5 * x + 0.5 * rng.normal(size=100)\n",
    "data = pd.DataFrame({'x': x, 'y': y})\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def permutation_test_regression(data, n_permutations=1000):\n",
    "    # first, fit the regression model (on the original data)\n",
    "    model = smf.ols('y ~ x', data=data).fit()\n",
    "    beta_hat = model.params['x']\n",
    "    \n",
    "    # TODO: implement the permutation test. \n",
    "    # You should be able to return the observed slope, \n",
    "    # the array of permuted slopes, and the p-value \n",
    "    # associated with the test.\n",
    "\n",
    "    return {\n",
    "        \"beta_hat\": beta_hat,\n",
    "        \"perm_beta_hats\": perm_beta_hats,\n",
    "        \"p_val\": p_val,\n",
    "    } \n",
    "```\n",
    "\n",
    "## Hint\n",
    "::: {.hint exercise=\"hypothesis-testing-regression\"}\n",
    "To implement the permutation test, you will need to:\n",
    "\n",
    "1. Fit the regression model to the original data to get the observed slope $\\hat{\\beta}_1$.\n",
    "2. For each permutation:\n",
    "   - Permute the response variable $Y$ to break the relationship with $X$.\n",
    "   - Fit the regression model to the permuted data.\n",
    "   - Store the estimated slope $\\hat{\\beta}_1$ from the permuted data.\n",
    "3. Compute the p-value as the proportion of permuted slopes that are greater than or equal to the observed slope in absolute value (two-tailed test).\n",
    "\n",
    ":::\n",
    "\n",
    "## Solution\n",
    "::: {.solution exercise=\"hypothesis-testing-regression\"}\n",
    "\n",
    "```python\n",
    "def permutation_test_regression(data, n_permutations=1000):\n",
    "    # first, fit the regression model (on the original data)\n",
    "    model = sm.ols('y ~ x', data=data).fit()\n",
    "    beta_hat = model.params['x']\n",
    "    \n",
    "    perm_beta_hats = [] # list to store permuted slopes\n",
    "    for _ in range(n_permutations):\n",
    "        # permute the response variable y\n",
    "        # this breaks the relationship between x and y\n",
    "        y_perm = rng.permutation(data['y'])\n",
    "        # fit the regression model to the permuted data\n",
    "        perm_model = sm.OLS(y_perm, X).fit()\n",
    "        # store the estimated slope\n",
    "        perm_beta_hats.append(perm_model.params['x'])\n",
    "    # the slopes from the permuted data are\n",
    "    # your null distribution\n",
    "    perm_beta_hats = np.array(perm_beta_hats)\n",
    "    # compute p-value\n",
    "    p_val = np.mean(np.abs(perm_beta_hats) >= np.abs(beta_hat))\n",
    "    \n",
    "    return {\n",
    "        \"beta_hat\": beta_hat,\n",
    "        \"perm_beta_hats\": perm_beta_hats,\n",
    "        \"p_val\": p_val,\n",
    "    }\n",
    "```\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c871ba8",
   "metadata": {},
   "source": [
    "This is precisely the permutation test that generates the null distribution displayed in the app. \n",
    "\n",
    "Notice that the permuation test $p$-value is nearly identical to the parametric $p$-value computed by `statsmodels` using the assumption that the errors are normally distributed. \n",
    "\n",
    ":::{.callout-note title=\"Parametric inference in regression\" collapse=\"true\"}\n",
    "The app uses `statsmodels` to compute the regression coefficients and their associated p-values. To get these $p$-values (without doing simulations / randomization), you have to make assumptions about the underlying probability distributions. \n",
    "\n",
    "In the linear regression model, the typical assumption is that (as we discussed in the previous lecture) the errors $\\epsilon$ are normally distributed. This makes the $Y$ values themselves normal random variables (i.e. $Y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X, \\sigma^2)$). This assumption allows us to use the properties of the normal distribution to figure out analytical expressions for the uncertainty in the estimated coefficients (a.k.a. the standard errors) and the $p$-values for hypothesis tests.\n",
    "\n",
    "The details of the parametric inference are beyond the scope of this course but you are encouraged to take more advanced courses in statistics or econometrics to learn more about it! \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3f9ef",
   "metadata": {},
   "source": [
    "### Confidence Intervals for Regression Coefficients\n",
    "\n",
    "$p$-values are nice, but they don't tell us the whole story. We might want to get a sense of the range of plausible values for the regression coefficients, because the effect size is important too.\n",
    "\n",
    ":::{.callout-warning title=\"Effect Size\" collapse=\"true\"}\n",
    "**Effect size** refers to the magnitude of a quantity you are interested in estimating -- often a difference in averages or a regression coefficient. \n",
    "\n",
    "For a simple example, let's go all the way back to coin flips. We saw examples where we tried to estimate the probability of seeing a certain set of outcomes if the coin was fair. We saw that with lots of coin flips, we were able to confidently detect a biased coin. But we saw an example where the coin was **very** biased (only 25% heads). What if the coin was just a little biased (say 51% heads)? This is a much smaller effect size, and it would be harder to detect with the same number of coin flips -- but with enough flips we could still detect it. Is a 1% difference in probability of heads worth fighting over? What about a 0.1% difference? \n",
    "\n",
    "This comes up a lot in scientific research. Say you are testing a new drug and you find that it reduces symptoms by 5% compared to a placebo. Is that a meaningful effect? What if the drug only reduces symptoms by 0.5%? Producing the drug might be very expensive, so a small effect size might not be worth the cost (or possibly the risk of side effects).\n",
    "\n",
    "::: \n",
    "\n",
    "In addition to hypothesis testing, we can also use bootstrapping to construct confidence intervals for the regression coefficients. \n",
    "\n",
    "Here's some code that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.699\n",
      "Model:                            OLS   Adj. R-squared:                  0.696\n",
      "Method:                 Least Squares   F-statistic:                     227.5\n",
      "Date:                Sun, 27 Jul 2025   Prob (F-statistic):           2.71e-27\n",
      "Time:                        18:50:46   Log-Likelihood:                -132.94\n",
      "No. Observations:                 100   AIC:                             269.9\n",
      "Df Residuals:                      98   BIC:                             275.1\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.0933      0.093     22.569      0.000       1.909       2.277\n",
      "x              1.5567      0.103     15.084      0.000       1.352       1.761\n",
      "==============================================================================\n",
      "Omnibus:                        0.741   Durbin-Watson:                   1.945\n",
      "Prob(Omnibus):                  0.690   Jarque-Bera (JB):                0.313\n",
      "Skew:                          -0.044   Prob(JB):                        0.855\n",
      "Kurtosis:                       3.259   Cond. No.                         1.15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "########################################\n",
      "Bootstrap CI for slope: [1.358, 1.760]\n"
     ]
    }
   ],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bootstrap_regression_ci(data, x_col, y_col, n_bootstraps=1000, alpha=0.05):\n",
    "    n = len(data)\n",
    "    coefs = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample = data.sample(n, replace=True)  # <1>\n",
    "        model = sm.ols(f\"{y_col} ~ {x_col}\", data=sample).fit()  # <2>\n",
    "        coefs.append(model.params[x_col])  # <3>\n",
    "    coefs = np.array(coefs)\n",
    "    lower_bound = np.percentile(coefs, 100 * alpha / 2)  # <4>\n",
    "    upper_bound = np.percentile(coefs, 100 * (1 - alpha / 2))\n",
    "    return coefs, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "# Generate synthetic data\n",
    "x = rng.normal(size=100)\n",
    "y = 1.5 * x + 2 + rng.normal(size=100)\n",
    "data = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "\n",
    "model = sm.ols(\"y ~ x\", data=data).fit()  # <5>\n",
    "print(model.summary())  # <6>\n",
    "\n",
    "bootstrap_coefs, lower_bound, upper_bound = bootstrap_regression_ci(\n",
    "    data, \"x\", \"y\", n_bootstraps=10000\n",
    ")\n",
    "print(\"#\" * 40)\n",
    "print(f\"Bootstrap CI for slope: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babfd09",
   "metadata": {},
   "source": [
    "1. Bootstrap the data to create multiple samples (same size as the original sample, sampled with replacement).\n",
    "2. Fit the regression model to each bootstrap sample.\n",
    "3. Store the estimated coefficients from each bootstrap sample.\n",
    "4. Compute the confidence intervals based on the distribution of the bootstrap estimates.\n",
    "5. Fit the regression model to the original data to get the point estimates.\n",
    "6. Print statsmodels summary table with the confidence intervals.\n",
    "\n",
    "The summary table has lots of extra information you can ignore for now, but the key part is the `coef` column which contains the point estimates of the regression coefficients, and the `[0.025, 0.975]` columns which contain the lower and upper bounds of the 95% confidence intervals.\n",
    "\n",
    "We also printed the bootstrapped confidence intervals, and you can see that they are very similar to the ones computed by `statsmodels` using the normality assumption.\n",
    "\n",
    "Still confused? Try the following app to visualize what's going on:\n",
    "\n",
    "```{shinylive-python}\n",
    "#| standalone: true\n",
    "#| viewerHeight: 500\n",
    "\n",
    "from shiny import App, ui, render, reactive\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Original population\n",
    "rng = np.random.default_rng(123)\n",
    "N_POP = 500\n",
    "x_pop = rng.normal(size=N_POP)\n",
    "true_beta = 0.5\n",
    "y_pop = true_beta * x_pop + rng.normal(scale=1, size=N_POP)\n",
    "population = pd.DataFrame({\"x\": x_pop, \"y\": y_pop})\n",
    "\n",
    "# Fit regression on full population\n",
    "pop_model = smf.ols(\"y ~ x\", data=population).fit()\n",
    "pop_slope = pop_model.params[\"x\"]\n",
    "\n",
    "app_ui = ui.page_fluid(\n",
    "    ui.layout_columns(\n",
    "        ui.input_slider(\"n\", \"Sample Size\", min=10, max=500, value=100),\n",
    "        ui.input_action_button(\"resample\", \"Resample 1x\"),\n",
    "        ui.input_action_button(\"resample10\", \"Resample 10x\"),\n",
    "        ui.input_action_button(\"resample100\", \"Resample 100x\"),\n",
    "    ),\n",
    "    ui.output_plot(\"scatter_plot\"),\n",
    "    ui.output_plot(\"hist_plot\"),\n",
    ")\n",
    "\n",
    "def server(input, output, session):\n",
    "    current_sample = reactive.value(pd.DataFrame())\n",
    "    bootstrap_coefs = reactive.value([])\n",
    "\n",
    "    def take_sample():\n",
    "        sample = population.sample(n=input.n(), replace=True)\n",
    "        model = smf.ols(\"y ~ x\", data=sample).fit()\n",
    "        bootstrap_coefs.set(bootstrap_coefs() + [model.params[\"x\"]])\n",
    "        current_sample.set(sample)\n",
    "\n",
    "    @reactive.effect\n",
    "    @reactive.event(input.resample)\n",
    "    def resample_once():\n",
    "        take_sample()\n",
    "\n",
    "    @reactive.effect\n",
    "    @reactive.event(input.resample10)\n",
    "    def resample_ten():\n",
    "        for _ in range(10):\n",
    "            take_sample()\n",
    "\n",
    "    @reactive.effect\n",
    "    @reactive.event(input.resample100)\n",
    "    def resample_hundred():\n",
    "        for _ in range(100):\n",
    "            take_sample()\n",
    "\n",
    "    @output\n",
    "    @render.plot\n",
    "    def scatter_plot():\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        # Plot population faintly\n",
    "        sns.scatterplot(x=\"x\", y=\"y\", data=population, alpha=0.1, color=\"gray\", label=\"Population\")\n",
    "\n",
    "        # Population regression line\n",
    "        x_vals = np.linspace(population[\"x\"].min(), population[\"x\"].max(), 100)\n",
    "        y_vals = pop_model.params[\"Intercept\"] + pop_model.params[\"x\"] * x_vals\n",
    "        plt.plot(x_vals, y_vals, color=\"black\", lw=2, linestyle=\"--\", label=\"Population Line\")\n",
    "\n",
    "        # If we have a current sample, overlay it\n",
    "        sample = current_sample()\n",
    "        if not sample.empty:\n",
    "            sns.scatterplot(x=\"x\", y=\"y\", data=sample, color=\"blue\", alpha=0.6, label=\"Bootstrap Sample\")\n",
    "            model = smf.ols(\"y ~ x\", data=sample).fit()\n",
    "            y_sample = model.params[\"Intercept\"] + model.params[\"x\"] * x_vals\n",
    "            plt.plot(x_vals, y_sample, color=\"red\", lw=2, label=\"Sample Line\")\n",
    "\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(\"Population (faint) and Current Bootstrap Sample\")\n",
    "        plt.legend()\n",
    "\n",
    "    @output\n",
    "    @render.plot\n",
    "    def hist_plot():\n",
    "        coefs = bootstrap_coefs()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        if coefs:\n",
    "            sns.histplot(coefs, bins=20, kde=False, color=\"lightblue\")\n",
    "\n",
    "            # Vertical lines at quantiles\n",
    "            q_low, q_high = np.quantile(coefs, [0.025, 0.975])\n",
    "            plt.axvline(q_low, color=\"purple\", linestyle=\"--\", lw=2, label=\"2.5% Quantile\")\n",
    "            plt.axvline(q_high, color=\"purple\", linestyle=\"--\", lw=2, label=\"97.5% Quantile\")\n",
    "\n",
    "        # Vertical line at population slope\n",
    "        plt.axvline(pop_slope, color=\"black\", lw=2, label=\"Observed (Population) β̂\")\n",
    "\n",
    "        plt.xlabel(\"Bootstrap Slope Estimates\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Bootstrap Distribution of β̂\")\n",
    "        plt.legend()\n",
    "\n",
    "app = App(app_ui, server)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding-uncertainty-iVxgN8uU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
