---
title: Lecture 00
date: now
format:
  html:
    code-fold: true
    code-tools: true
    code-summary: Code
    code-block-name: Code
execute:
  warning: false
jupyter: python3
---


```{python}
#| hidden: true
#| echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# some personal style settings to make the plots look nice
# and save some space in the notebook
plt.style.use("../style.mplstyle")
```

## Welcome! 
So begins "Understanding Uncertainty", a course in statistical thinking and data science. 

This is lecture 0. See the [syllabus](../syllabus.qmd) for an overview of the course.

Our basic goals for the course are:

1) to build a strong intuition about data, where it comes from, and what questions it can answer. 

2) to learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!

## Why statistics? 
Statistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: (1) description, (2) inference, and (3) prediction. 

## Description
Descriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand. 

Let's load in some data and take a look at it.

The [dataset](https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata) contains Airbnb listings in New York City, including prices, locations, and other features. 

```{python}
# from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata
# import the data on Airbnb listings in the New York City
airbnb = pd.read_csv("../data/airbnb.csv")
# data cleaning
airbnb = airbnb.rename(columns={"neighbourhood group": "borough"})
airbnb = airbnb.dropna(subset=["borough", "price", "long", "lat"])
airbnb["borough"] = airbnb["borough"].str.lower()
airbnb["borough"] = airbnb["borough"].str.replace("manhatan", "manhattan")
airbnb["borough"] = airbnb["borough"].str.replace("brookln", "brooklyn")
# print column names
print(airbnb.columns.values)
```

```{python}
# print the first 5 rows
airbnb[:5]
```

Now there's a lot you can do, but let's start with a simple histogram of the price of listings.

```{python}
# format the price column
airbnb['price'] = airbnb['price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)
# plot a histogram of the price column
plt.figure(figsize=(10, 5))
plt.hist(airbnb['price'], bins=50)
plt.title('Price Distribution of Airbnb Listings in NYC')
plt.xlabel('Price')
plt.ylabel('Frequency')
```

Computing statistics like the mean, standard deviation, and quartiles is easy.

```{python}
airbnb["price"].describe()
```

We can even use specialized libraries to make use of the geographic information in the data. For example, we can use the `geopandas` library to plot the locations of listings on a map of New York City. 

```{python}
import geopandas as gpd
from geodatasets import get_path
# load the shapefile of NYC neighborhoods
nyc_neighborhoods = gpd.read_file(get_path('nybb'))
nyc_neighborhoods = nyc_neighborhoods.to_crs(epsg=4326)  # convert to WGS84
# plot the neighborhoods with airbnb listings
nyc_neighborhoods.plot(figsize=(8, 8), color='white', edgecolor='black')
# plot the airbnb listings on top of the neighborhoods
# use the 'long' and 'lat' columns to create a GeoDataFrame
airbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb['long'], airbnb['lat']), crs='EPSG:4326')
# set the coordinate reference system to WGS84
airbnb_gdf.plot(ax=plt.gca(), column="price", markersize=3, alpha=0.05, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})
plt.title('Airbnb Listings in NYC')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
```

There is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics **only describe the data**. 

Why is this limiting? After all, we like data -- it tells us things about the world and it's objective and quantifiable. 

The problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.

Let's look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small "sample" or subset of the data?

```{python}
# separately plot 3 samples of airbnb listings
fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)
ax = ax.flatten()
for i in range(4):
    # sample 1000 listings
    sample = airbnb.sample(100, random_state=i)
    # plot the neighborhoods with airbnb listings
    nyc_neighborhoods.plot(ax=ax[i], color='white', edgecolor='black')
    # plot the airbnb listings on top of the neighborhoods
    # use the 'long' and 'lat' columns to create a GeoDataFrame
    airbnb_gdf_sample = gpd.GeoDataFrame(sample, geometry=gpd.points_from_xy(sample['long'], sample['lat']), crs='EPSG:4326')
    # set the coordinate reference system to WGS84
    airbnb_gdf_sample.plot(ax=ax[i], column="price", markersize=3, alpha=0.8, legend=True, cmap='viridis', legend_kwds={'shrink': 0.5, 'label': 'Price ($)'})
    ax[i].set_title(f'Airbnb Listings in NYC (Sample {i+1})\n Average Price: ${sample["price"].mean():.2f}')
    ax[i].set_xlabel('Longitude')
    ax[i].set_ylabel('Latitude')
plt.tight_layout()
plt.show()
```

Notice how the samples differ from one another. They have different geography and different prices. This means you can't just look at the descriptive statistics of a single sample and draw conclusions about the entire population.

::: {.callout-note title="Sample vs. Population" collapse="true"}
A **population** is the entire set of data that you are interested in. A **sample** is a subset of a population. For example, if you are interested in the average price of all Airbnb listings in New York City, then the population is all of those listings. A sample would be a smaller subset of those listings, which may or may not be representative of the entire population.

Note that this definition is flexible. For example, if you are interested in the average price of all short-term rentals in New York City, then the population is all rentals. Even an exhaustive list of Airbnb listings would just be a sample from that population.

Often, the population is actually more abstract or theoretical. For example, if you are interested in the average price of all possible Airbnb listings in New York City, then the population includes all potential listings, not just the ones that currently exist.
:::

Descriptive statistics are useful for understanding the data at hand, but they don't necessarily tell us much about the world outside of the data. For that, we need to do something more. 

## Inference

So what if we want to answer questions about a population *based* on a sample? This is where **inference** comes in. Specifically, we want to use the given sample to **infer** something about the population.

How do we do this if we can't ever see the entire population? The answer is that we need a link which connects the sample to the population -- specifically, we can explicitly treat the sample as the outcome of a **data-generating process** (DGP).

:::{.callout-caution title="There is always a DGP" collapse="true"}
A **data-generating process** (DGP) is a theoretical construct that describes how data is generated in a population. It encompasses all the factors that influence the data, including the underlying mechanisms and relationships between variables.

There *has* to be a DGP, even if we don't know what it is. The DGP is the process that generates the data we observe.

The full, true DGP is usually unknown. However, we can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).
:::

Of course, we don't necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!

This is where the **model** comes in. A model is a simplified mathematical representation of the DGP that allows us to make inferences about the population based on the sample. At the end of the day, a model is sort of a guess -- a guess about where your data come from.  

:::{.callout-tip title="All models are wrong" collapse="true"}
There is a very famous and oft-cited quote by George Box, a statistician, that goes:

> "All models are wrong, but some are useful."

This means that all mathematical models are simplifications of reality, and they will never perfectly capture the true DGP. However, some models can still be useful for making predictions or drawing inferences about the population. We will talk more about this and see examples throughout the course.
:::

### A simple model


We'll talk about probability distributions in more detail later in the course, but for now let's just say that a probability distribution is a mathematical function that describes the likelihood of different outcomes in a random process.

The most basic example is a coin flip. If we flip a fair coin, there are two possible outcomes: heads or tails. The probability of each outcome is 0.5, so we can represent this with a simple probability distribution.

\begin{equation}
P(X) = \begin{cases}
0.5 & \text{if } X = \text{heads} \\
0.5 & \text{if } X = \text{tails} \\
0 & \text{otherwise}
\end{cases}
\end{equation}


## Quiz question goes here {.quiz-question}

- Option 1
- [Option 2 which is correct]{.correct}
- Option 3
- Option 4

```{python}
# average price per borough
borough_price = airbnb.groupby('borough')['price'].count().reset_index()
# plot the average price per borough
plt.figure(figsize=(10, 5))
plt.bar(borough_price['borough'], borough_price['price'])
plt.title('Number of Airbnb Listings per Borough')
plt.xlabel('Borough')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
airbnb_gdf.plot(column="neighbourhood group")
plt.title('Airbnb Listings in NYC by borough')
```

## Prediction

Prediction is the process of using a model to make predictions about unseen (or future) data. 

Take $X \sim \text{Uniform}(0, 1)$

```{python}
x = np.random.uniform(0, 1, 1000)

plt.hist(x, bins=20, edgecolor="black")
plt.show()
```


