{"title":"Lecture 06: Confidence Intervals and Bootstrapping","markdown":{"yaml":{"title":"Lecture 06: Confidence Intervals and Bootstrapping","date":"now","format":{"live-html":{"toc":true,"toc-location":"right","code-fold":true,"code-tools":true,"code-summary":"Code","code-block-name":"Code"}},"pyodide":{"packages":["matplotlib","numpy"]},"execute":{"warning":false}},"headingText":"Confidence Intervals","containsRefs":false,"markdown":"\n\n\nWe have talked about quantifying uncertainty with $p$-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking \"how likely is it that the average is $\\mu=0$?\" we might ask \"what is a plausible range of values for the average?\"\n\nThis range is called a **confidence interval**. Like in hypothesis testing, we can choose a risk level $\\alpha$ (e.g., 5%). \nThe confidence interval, like a hypothesis test, is computed based on a sample -- so sometimes it is incorrect! \n\nMathematically, we the confidence interval for a parameter $\\theta$ is defined as the range of values bounded by $[L, U]$ that are likely to contain the true value of the parameter with probability $1-\\alpha$:\n$$\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n$$\n\nThere is an important distinction to make here: $\\theta$ is not the thing that changes! In fact, we can't compute $\\theta$ - it is a theoretical property of the population, which we can't access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n### Confidence Intervals when the distribution is known\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\n1. The sample mean $\\bar{X}$ is an unbiased estimator of the population mean $\\mu$. Meaning, we can assume that the sample mean is typically close to the population mean.\n2. The variance of the sample mean is $\\sigma^2/n$, where $\\sigma^2$ can be estimated from the sample.\n3. The distribution of the sample mean is symmetric.\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within $k$ standard deviations of the mean? \n\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range $[\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]$ as:\n$$\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n$$\nwhere we made the substitution $u = (x - \\mu) \\sqrt{n}/\\sigma$. \n\nWe can set this integral to be equal to the probability $1 - \\alpha$ to find the value of $z$ that corresponds to a given confidence level. \n\n$$\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n$$\n\nSolving this integral for a chosen value of $\\alpha$ gives us the value of $z$ that corresponds to that confidence level. We denote it as $z_{\\alpha/2}$, because the upper limit corresponds to the $(1 - \\frac{\\alpha}{2})$ quantile of the distribution.\n\n::: {.callout-note title=\"Standard normal distribution quantiles\" collapse=\"true\"}\nThe **standard normal** distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So $(1 - \\alpha)$ is the probability that a sample from the standard normal distribution falls within $[-z, z]$.\n\nThis is the $(1 - \\frac{\\alpha}{2})$ because the standard normal distribution is symmetric around 0. So the probability $\\alpha$ of the sample falling outside of $[-z, z]$ is split equally between the two tails of the distribution. So there is a probability of $\\frac{\\alpha}{2}$ of the sample falling below $-z$ and a probability of $\\frac{\\alpha}{2}$ of the sample falling above $z$.\n:::\n\nIf you evaluate this integral, you find that the probability of the sample mean being within $z$ standard errors ^[i.e. the standard deviation of the sampling distribution] of the mean is, for various values of $z$:\n\n| $z$ | Probability |\n|-----|-------------|\n| 1   | 0.6827      |\n| 2   | 0.9545      |\n| 3   | 0.9973      |\n| 4   | > 0.9999    |\n\nWe use this logic to define a confidence interval for the population mean $\\mu$ as:\n$$\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n$$\n\n\n\n## Uncertainty without models\n\nSo far, we've emphasized the importance of developing **models** of data-generating processes in order to quantify randomness and uncertainty. If you know something about the underlying process that generates your data, you can pick an appropriate statistical model and use it to sample from the distribution, simulate new data, and evaluate hypotheses. The CLT is a powerful tool because even when we don't know what a good model of the data-generating process is, we can often use the normal distribution as an approximation for the distribution of sample means.\n\nHowever, there are still many situations where we don't know enough about the DGP to confidently specify a model. In these cases, we can still use statistical techniques to evaluate hypotheses about the data. The idea is that if we don't have any knowledge about the DGP, we rely on the only information we have: the data itself. \n\n### Resampling methods\n\nHow did we get a dataset in the first place? Recall that every dataset is a sample from some underlying distribution.\n\nWhat is the main difference between a sample and a population? A sample is a subset of the population, and it is usually much smaller than the population. The sample might not cover all the \"edge cases\" or rare events that are present in the population, or certain outcomes might be overrepresented (you *could* flip a fair coin 10 times and get 10 heads, even if it is unlikely). But as we have seen, the bigger the sample is the more it tends to represent the population well.\n\nIf the sample starts to look like the population... then maybe we can just use the sample itself as a proxy for the population? This is the idea behind **resampling methods**. When the sample is pretty large, we can **sub-sample** from the original dataset as though it were the population, and use the sub-samples to estimate properties of the population.\n\nLet's try this with the example above. Treating each of the histograms as a proxy for the population, we will sub-sample (with replacement) from the sample 1000 times to create new samples, and see what they look like.\n\nAs it turns out, the distribution of the resampled data is very similar to the original sample. So if the original sample is similar to the true population distribution, the resampled distribution will retain that similarity. \n\n## Bootstrapping\nBootstrapping is a specific type of resampling method that allows us to estimate the sampling distribution of any test statistic (e.g., mean, median, variance) by repeatedly resampling with replacement from the observed data. This bootstrapped distribution can then be used to compute confidence intervals or perform hypothesis tests without making *any* assumptions about the underlying distribution of the data! \n\nBootstrapping works as follows:\n\n1. Given a dataset of size $n$, draw a sample of size $n$ from the original dataset with replacement.\n2. Compute the test statistic (e.g., mean, median, variance) on the bootstrapped sample.\n3. Repeat steps 1 and 2 a large number of times (e.g., 1000 times) to create a distribution of the test statistic.\n4. Use the distribution of the test statistic to compute confidence intervals or perform hypothesis tests.\n\n::: {.callout-note title=\"Why sample with replacement?\" collapse=true}\nThe idea behind the bootstrap is to take *independent* samples from the original dataset. It is supposed to simulate i.i.d. sampling from the population. Sampling with replacement allows us to create new samples such that taking a sample does not affect the next sample, and every bootstrapped sample comes from the same proxy distribution.\n\nThink about what would happen if we sampled without replacement: since the sample is the same size as the original dataset, we would end up with the same sample every time! There are only $n$ datapoints in the sample, so if we sample without replacement, we will always get the same sample back. \n:::\n\nLet's generate a sample from a normal distribution with known parameters $\\mu=3.2$ and $\\sigma=1.5$. \nThen, we will use bootstrapping to estimate the mean and variance of the population from the sample.\n\nHere is the real power of bootstrapping, though: because we have the whole distribution of the resampled data, we can compute confidence intervals any statistic by looking at at different percentiles of the resampled distribution. For example, we can compute a 95% confidence interval for the mean by taking the 2.5th and 97.5th percentiles of the resampled means.\n\n### Understanding the meaning of a confidence interval\n\nThe definition of a confidence interval is a bit confusing, but bootstrapping can actually help us demonstrate what it means. \n\nThe *uncertainty* in the confidence interval does **NOT** come from uncertainty in the value of the parameter we are estimating. In our example, we designed the distribution and we know that the true mean is 3.2, so there is no uncertainty about the value of the parameter. The uncertainty comes from the fact that we are estimating the parameter based on a sample, and the sample is not necessarily representative of the population.\n\nTo illustrate this, let's repeatedly sample from our distribution and run the bootstrapping procedure on each sample. In the plot below, each interval represents a 95% confidence interval for the mean of a different sample. The highlighted intervals are the ones that failed to capture the true mean of 3.2.\n\nNotice that about 5% of the intervals do not capture the true mean, which is exactly what we expect from a 95% confidence interval. \n\n## Example: variability in scoring average\n\nLet's return to our analysis of NBA players' scoring ability. \n\nWe can use bootstrapping to estimate confidence intervals for each player's scoring average. \n\nNotice that Shai's 95% CI does not overlap with Giannis' scoring average, but it does overlap with Giannis' 95% CI!\n\nThis actually highlights a problem with our previous analysis: we only considered the variability in SGA's scoring average, and asked how likely it is that his scoring average was equivalent to a specific value (Giannis' scoring average). But we didn't consider the variability in Giannis' scoring average! \n\nThink about it like this: perhaps SGA's scoring average was randomly higher than expected, and Giannis' scoring average was randomly lower than expected. This possibly makes it more likely that the two averages are closer together than it would appear.\n\nAnother way of stating this is that our null hypothesis was too weak (we chose a baseline equal to Giannis' scoring average, but Giannis could actually be better or worse than that). \n\nLet's fix this mistake! We can use bootstrapping to generate scoring average *distributions* for both players, and then compare the two distributions to see how much they overlap.\n\nThe question is: if we resample with replacement from both players' game logs (i.e., their individual game scores), how often do we find that Giannis scores more on average than SGA?\n\nRecall that in the previous analysis, our $p$-value was 0.004! So our corrected analysis estimates that the probability that SGA's scoring title is a \"fluke\" is actually an order of magnitude higher.\n\nAnother way to look at this is to return to the lens of hypothesis testing. \nWhat hypothesis test captures the variability of both players' scoring? \nHow is our null hypothesis different from the one we used before?\n\nEarlier we just tested if SGA's true scoring rate was higher than a constant value (Giannis' observed scoring average). Now, we are testing if SGA's true scoring rate is higher than Giannis' true scoring rate, which is a random variable that can take on different values across different samples.\n\n- $H_0: ~\\mu_{\\text{SGA}} \\leq \\mu_{\\text{Giannis}}$\n- $H_1: ~\\mu_{\\text{SGA}} > \\mu_{\\text{Giannis}}$\n\nThis null hypothesis basically posits that in the best case, SGA's scoring average is equal to Giannis' scoring average (if not worse). This would mean that their scoring output is indistinguishable -- if the variance of their scoring is on the same scale, then their game logs are actually being sampled from the same distribution! We can simulate this in a bootstrapping procedure by **combining** the two players' game logs and resampling from the combined distribution. \n\nThe question is: if we resample with replacement from the combined game logs (i.e. the players have the exact same scoring distribution), how often do we see such a large advantage for SGA?\n\nSimulation and resampling methods give us a powerful way to quantify uncertainty in a way that does not rely so heavily on assumptions about the underlying distribution of the data. \n\nIn the next lecture, we'll look at another simulation-based method for hypothesis testing: permutation tests. \n","srcMarkdownNoYaml":"\n\n## Confidence Intervals\n\nWe have talked about quantifying uncertainty with $p$-values (the probability of observing the data under a some hypothesis). But oftentimes we want to quantify uncertainty in a different way: by estimating the range of values that some parameter is likely to take. So, rather than asking \"how likely is it that the average is $\\mu=0$?\" we might ask \"what is a plausible range of values for the average?\"\n\nThis range is called a **confidence interval**. Like in hypothesis testing, we can choose a risk level $\\alpha$ (e.g., 5%). \nThe confidence interval, like a hypothesis test, is computed based on a sample -- so sometimes it is incorrect! \n\nMathematically, we the confidence interval for a parameter $\\theta$ is defined as the range of values bounded by $[L, U]$ that are likely to contain the true value of the parameter with probability $1-\\alpha$:\n$$\n\\mathbb{P}(\\theta \\in [L, U]) = 1 - \\alpha\n$$\n\nThere is an important distinction to make here: $\\theta$ is not the thing that changes! In fact, we can't compute $\\theta$ - it is a theoretical property of the population, which we can't access in its entirety. What changes is the sample we draw from the population, and the confidence interval is computed based on that sample. So, if we draw a different sample, we will get a different confidence interval.\n\n### Confidence Intervals when the distribution is known\nThink back to the Central Limit Theorem (CLT). The CLT tells us that the distribution of the sample mean is roughly normal. From this, we know 3 important things:\n\n1. The sample mean $\\bar{X}$ is an unbiased estimator of the population mean $\\mu$. Meaning, we can assume that the sample mean is typically close to the population mean.\n2. The variance of the sample mean is $\\sigma^2/n$, where $\\sigma^2$ can be estimated from the sample.\n3. The distribution of the sample mean is symmetric.\n\n\nSee that for a normal distribution, much of the probability mass is concentrated around the mean within a few standard deviations. How much probability mass is concentrated within $k$ standard deviations of the mean? \n\nRecall that when the distribution is known, we can compute the probability of the sample mean being in a certain range by integrating the probability density function (PDF). So for a normal distribution, we can compute the probability of the sample mean being in the range $[\\mu - z\\sigma/\\sqrt{n}, \\mu + z\\sigma/\\sqrt{n}]$ as:\n$$\n\\begin{align*}\n\\mathbb{P}(\\mu - z\\sigma/\\sqrt{n} \\leq \\bar{X} \\leq \\mu + z\\sigma/\\sqrt{n}) &= \\int_{\\mu - z\\sigma/\\sqrt{n}}^{\\mu + z\\sigma/\\sqrt{n}} \\frac{1}{\\sqrt{2\\pi\\sigma^2/n}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2/n}} \\, dx \\\\\n&= \\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du\n\\end{align*}\n$$\nwhere we made the substitution $u = (x - \\mu) \\sqrt{n}/\\sigma$. \n\nWe can set this integral to be equal to the probability $1 - \\alpha$ to find the value of $z$ that corresponds to a given confidence level. \n\n$$\n\\int_{-z}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, du = 1 - \\alpha\n$$\n\nSolving this integral for a chosen value of $\\alpha$ gives us the value of $z$ that corresponds to that confidence level. We denote it as $z_{\\alpha/2}$, because the upper limit corresponds to the $(1 - \\frac{\\alpha}{2})$ quantile of the distribution.\n\n::: {.callout-note title=\"Standard normal distribution quantiles\" collapse=\"true\"}\nThe **standard normal** distribution is a normal distribution with mean 0 and standard deviation 1. See how after the substitution, the integrand is the PDF of the standard normal distribution. So $(1 - \\alpha)$ is the probability that a sample from the standard normal distribution falls within $[-z, z]$.\n\nThis is the $(1 - \\frac{\\alpha}{2})$ because the standard normal distribution is symmetric around 0. So the probability $\\alpha$ of the sample falling outside of $[-z, z]$ is split equally between the two tails of the distribution. So there is a probability of $\\frac{\\alpha}{2}$ of the sample falling below $-z$ and a probability of $\\frac{\\alpha}{2}$ of the sample falling above $z$.\n:::\n\nIf you evaluate this integral, you find that the probability of the sample mean being within $z$ standard errors ^[i.e. the standard deviation of the sampling distribution] of the mean is, for various values of $z$:\n\n| $z$ | Probability |\n|-----|-------------|\n| 1   | 0.6827      |\n| 2   | 0.9545      |\n| 3   | 0.9973      |\n| 4   | > 0.9999    |\n\nWe use this logic to define a confidence interval for the population mean $\\mu$ as:\n$$\n\\begin{align*}\n\\left[\\bar{X} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right]\n\\end{align*}\n$$\n\n\n\n## Uncertainty without models\n\nSo far, we've emphasized the importance of developing **models** of data-generating processes in order to quantify randomness and uncertainty. If you know something about the underlying process that generates your data, you can pick an appropriate statistical model and use it to sample from the distribution, simulate new data, and evaluate hypotheses. The CLT is a powerful tool because even when we don't know what a good model of the data-generating process is, we can often use the normal distribution as an approximation for the distribution of sample means.\n\nHowever, there are still many situations where we don't know enough about the DGP to confidently specify a model. In these cases, we can still use statistical techniques to evaluate hypotheses about the data. The idea is that if we don't have any knowledge about the DGP, we rely on the only information we have: the data itself. \n\n### Resampling methods\n\nHow did we get a dataset in the first place? Recall that every dataset is a sample from some underlying distribution.\n\nWhat is the main difference between a sample and a population? A sample is a subset of the population, and it is usually much smaller than the population. The sample might not cover all the \"edge cases\" or rare events that are present in the population, or certain outcomes might be overrepresented (you *could* flip a fair coin 10 times and get 10 heads, even if it is unlikely). But as we have seen, the bigger the sample is the more it tends to represent the population well.\n\nIf the sample starts to look like the population... then maybe we can just use the sample itself as a proxy for the population? This is the idea behind **resampling methods**. When the sample is pretty large, we can **sub-sample** from the original dataset as though it were the population, and use the sub-samples to estimate properties of the population.\n\nLet's try this with the example above. Treating each of the histograms as a proxy for the population, we will sub-sample (with replacement) from the sample 1000 times to create new samples, and see what they look like.\n\nAs it turns out, the distribution of the resampled data is very similar to the original sample. So if the original sample is similar to the true population distribution, the resampled distribution will retain that similarity. \n\n## Bootstrapping\nBootstrapping is a specific type of resampling method that allows us to estimate the sampling distribution of any test statistic (e.g., mean, median, variance) by repeatedly resampling with replacement from the observed data. This bootstrapped distribution can then be used to compute confidence intervals or perform hypothesis tests without making *any* assumptions about the underlying distribution of the data! \n\nBootstrapping works as follows:\n\n1. Given a dataset of size $n$, draw a sample of size $n$ from the original dataset with replacement.\n2. Compute the test statistic (e.g., mean, median, variance) on the bootstrapped sample.\n3. Repeat steps 1 and 2 a large number of times (e.g., 1000 times) to create a distribution of the test statistic.\n4. Use the distribution of the test statistic to compute confidence intervals or perform hypothesis tests.\n\n::: {.callout-note title=\"Why sample with replacement?\" collapse=true}\nThe idea behind the bootstrap is to take *independent* samples from the original dataset. It is supposed to simulate i.i.d. sampling from the population. Sampling with replacement allows us to create new samples such that taking a sample does not affect the next sample, and every bootstrapped sample comes from the same proxy distribution.\n\nThink about what would happen if we sampled without replacement: since the sample is the same size as the original dataset, we would end up with the same sample every time! There are only $n$ datapoints in the sample, so if we sample without replacement, we will always get the same sample back. \n:::\n\nLet's generate a sample from a normal distribution with known parameters $\\mu=3.2$ and $\\sigma=1.5$. \nThen, we will use bootstrapping to estimate the mean and variance of the population from the sample.\n\nHere is the real power of bootstrapping, though: because we have the whole distribution of the resampled data, we can compute confidence intervals any statistic by looking at at different percentiles of the resampled distribution. For example, we can compute a 95% confidence interval for the mean by taking the 2.5th and 97.5th percentiles of the resampled means.\n\n### Understanding the meaning of a confidence interval\n\nThe definition of a confidence interval is a bit confusing, but bootstrapping can actually help us demonstrate what it means. \n\nThe *uncertainty* in the confidence interval does **NOT** come from uncertainty in the value of the parameter we are estimating. In our example, we designed the distribution and we know that the true mean is 3.2, so there is no uncertainty about the value of the parameter. The uncertainty comes from the fact that we are estimating the parameter based on a sample, and the sample is not necessarily representative of the population.\n\nTo illustrate this, let's repeatedly sample from our distribution and run the bootstrapping procedure on each sample. In the plot below, each interval represents a 95% confidence interval for the mean of a different sample. The highlighted intervals are the ones that failed to capture the true mean of 3.2.\n\nNotice that about 5% of the intervals do not capture the true mean, which is exactly what we expect from a 95% confidence interval. \n\n## Example: variability in scoring average\n\nLet's return to our analysis of NBA players' scoring ability. \n\nWe can use bootstrapping to estimate confidence intervals for each player's scoring average. \n\nNotice that Shai's 95% CI does not overlap with Giannis' scoring average, but it does overlap with Giannis' 95% CI!\n\nThis actually highlights a problem with our previous analysis: we only considered the variability in SGA's scoring average, and asked how likely it is that his scoring average was equivalent to a specific value (Giannis' scoring average). But we didn't consider the variability in Giannis' scoring average! \n\nThink about it like this: perhaps SGA's scoring average was randomly higher than expected, and Giannis' scoring average was randomly lower than expected. This possibly makes it more likely that the two averages are closer together than it would appear.\n\nAnother way of stating this is that our null hypothesis was too weak (we chose a baseline equal to Giannis' scoring average, but Giannis could actually be better or worse than that). \n\nLet's fix this mistake! We can use bootstrapping to generate scoring average *distributions* for both players, and then compare the two distributions to see how much they overlap.\n\nThe question is: if we resample with replacement from both players' game logs (i.e., their individual game scores), how often do we find that Giannis scores more on average than SGA?\n\nRecall that in the previous analysis, our $p$-value was 0.004! So our corrected analysis estimates that the probability that SGA's scoring title is a \"fluke\" is actually an order of magnitude higher.\n\nAnother way to look at this is to return to the lens of hypothesis testing. \nWhat hypothesis test captures the variability of both players' scoring? \nHow is our null hypothesis different from the one we used before?\n\nEarlier we just tested if SGA's true scoring rate was higher than a constant value (Giannis' observed scoring average). Now, we are testing if SGA's true scoring rate is higher than Giannis' true scoring rate, which is a random variable that can take on different values across different samples.\n\n- $H_0: ~\\mu_{\\text{SGA}} \\leq \\mu_{\\text{Giannis}}$\n- $H_1: ~\\mu_{\\text{SGA}} > \\mu_{\\text{Giannis}}$\n\nThis null hypothesis basically posits that in the best case, SGA's scoring average is equal to Giannis' scoring average (if not worse). This would mean that their scoring output is indistinguishable -- if the variance of their scoring is on the same scale, then their game logs are actually being sampled from the same distribution! We can simulate this in a bootstrapping procedure by **combining** the two players' game logs and resampling from the combined distribution. \n\nThe question is: if we resample with replacement from the combined game logs (i.e. the players have the exact same scoring distribution), how often do we see such a large advantage for SGA?\n\nSimulation and resampling methods give us a powerful way to quantify uncertainty in a way that does not rely so heavily on assumptions about the underlying distribution of the data. \n\nIn the next lecture, we'll look at another simulation-based method for hypothesis testing: permutation tests. \n"},"formats":{"live-html":{"identifier":{"display-name":"HTML","target-format":"live-html","base-format":"html","extension-name":"live"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["/Users/jrudoler/Documents/teaching/understanding-uncertainty/_extensions/r-wasm/live/live.lua","shinylive"],"toc":true,"output-file":"lecture-06.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","ojs-engine":true,"revealjs-plugins":[],"theme":{"light":"flatly","dark":"darkly"},"title":"Lecture 06: Confidence Intervals and Bootstrapping","date":"now","pyodide":{"packages":["matplotlib","numpy"]},"toc-location":"right","code-summary":"Code","code-block-name":"Code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["live-html"]}