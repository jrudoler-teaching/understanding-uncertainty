{"title":"Lecture 02: Probability and Random Variables","markdown":{"yaml":{"title":"Lecture 02: Probability and Random Variables","description":"Introduction to Probability and Random Variables","author":"Joseph Rudoler","date":"now","format":{"live-html":{"toc":true,"toc-location":"right","code-fold":true,"code-tools":true,"code-summary":"Code","code-block-name":"Code"}},"pyodide":{"packages":["matplotlib","numpy"]},"execute":{"warning":false}},"headingText":"Probability","containsRefs":false,"markdown":"\n\n::: {.hidden}\n$$\n \\renewcommand{\\P}{\\mathbb{P}}\n \\newcommand{\\E}{\\mathbb{E}}\n \\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n \\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n$$\n:::\n\nMost of you are probably familiar with the basic intuition of **probability**: essentially it measures how likely an event is to occur. \n\nIn mathematical terms, the probability $\\P$ of an event $A$ is defined as:\n\n\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\n\nBy definition this quantity cannot be negative ($\\P(A) = 0$ means $A$ never occurs), and it must be less than or equal to 1 ($\\P(A) = 1$ means $A$ always occurs).\n\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads ($H$) and tails ($T$). If we let $A$ be the event that the coin lands on heads, then we can compute the probability of $A$ as follows:\n\n\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\n\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\n\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say \"there's a 100% chance that *something* will happen\". \n\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events $H$ and $T$ cover 100% of the possible outcomes. So we can write:\n\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\n\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event $A$, the event either happens or it doesn't. So we can compute the probability of the event not occurring as:\n\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\n\n\n### Probability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: $HH$, $HT$, $TH$, and $TT$. \n\nIf we let $B$ be the event that at least one coin lands on heads, we can compute the probability of $B$ as follows:\n\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\n\n### Addition and multiplication rules (and / or)\n\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event $C = \\{HH\\}$)?\n\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that $\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}$.\n\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as $B$ above, so we can use the same calculation: $\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}$.\n\n::: {.callout-note title=\"Note on notation\" collapse=\"true\"}\nIn the above, we used $H_1$ and $H_2$ to denote heads on the first and second flips, respectively. The notation $H_1 ~\\text{and}~ H_2$ means both flips are heads, while $H_1 ~\\text{or}~ H_2$ means at least one flip is heads.\n\nIn probability theory, we often use the symbols $\\cap$ and $\\cup$ to denote \"and\" and \"or\" respectively. So we could also write $\\P(H_1 \\cap H_2)$ for the probability of both flips being heads, and $\\P(H_1 \\cup H_2)$ for the probability of at least one flip being heads. Technically, this is set notation where $\\cap$ means intersection (the event where both $H_1$ and $H_2$ occur), while $\\cup$ means union (the event where either $H_1$ or $H_2$ occurs).\n:::\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events $A$ and $B$, the following rules hold:\n\n- **Addition rule**: For any two events $A$ and $B$, the probability of either $A$ or $B$ occurring is given by:\n  $$\n  \\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n  $$\n  This last term, $\\P(A \\cap B)$, is necessary to avoid double counting the outcomes where both $A$ and $B$ occur.\n\n  Note that if $A$ and $B$ are mutually exclusive (i.e., they cannot both occur at the same time), then $\\P(A \\cap B) = 0$, and the formula simplifies to:\n  $$  \\P(A \\cup B) = \\P(A) + \\P(B)$$\n\n:::{.callout-tip title=\"Visualizing sets of events\" collapse=\"true\"}\nThe following image illustrates the addition rule for two events $A$ and $B$ using a Venn diagram. \n![probability-set](../images/probability-sets-union.jpeg)\n:::\n\n- **Multiplication rule**: For any two events $A$ and $B$, the probability of both $A$ and $B$ occurring is given by:\n$$\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)$$\nwhere $\\P(B | A)$ is the conditional probability of $B$ given that $A$ has occurred. This means you first consider the outcomes where $A$ occurs, and then look at the probability of $B$ within that subset.\n\n\n::: {.callout-tip title=\"Conditional probability\" collapse=\"false\"}\nThe notation $\\P(B | A)$ is read as \"the probability of $B$ given $A$\". It represents the probability of event $B$ occurring under the condition that event $A$ has already occurred. \n\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event $A$ is \"it is hot outside\", and the event $B$ is \"I buy ice cream\". The conditional probability $\\P(B | A)$ would be higher than $\\P(B)$ on a typical day.\n\nLet's think about this in the context of our coin flips. If we know that the first flip is heads ($H_1$), then only two outcomes are possible ($HH$ and $HT$) instead of four ($HH$, $HT$, $TH$, $TT$). \n\nSo the conditional probability $\\P(H_2 | H_1)$, which is the probability of the second flip being heads given that the first flip was heads, is:\n\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\n:::\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\n\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, \"What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)\"\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n### Independence\nTwo events $A$ and $B$ are said to be **independent** if the occurrence of one does not affect the probability of the other. \n\nHow does this relate to the multiplication rule? If $A$ and $B$ are independent, then the conditional probability $\\P(B | A)$ is simply $\\P(B)$. That is, knowing that $A$ has occurred does not change the probability of $B$ occurring.\n\nThis means that for independent events, the multiplication rule simplifies to:  \n$$\\P(A \\cap B) = \\P(A) \\cdot \\P(B)$$\n\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply $\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$.\n\n### Complicated counting\n\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes. \n\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let's think about the case where the first flip is heads. The probability of getting a head on the first flip is $\\frac{1}{2}$, and the probability of getting tails on the 4 other flips is also $\\frac{1}{2}$ each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips:\n$$\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}$$\n\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting \"exactly one head\". For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\n\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions:\n$$\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16$$\n\nThere are two common types of outcomes we want to count: **permutations** and **combinations**. \n\nA **combination** is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are **choosing** an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\n$$\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n$$\n\nIt is clear here that there are 5 possible \"slots\" where we can place a head.\n\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still\napplies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\n\nLet's take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices.\n$$\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n$$\n\nNow let's consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\n$$\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n$$\n\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads.\nThis gives us a total of $5 \\cdot 4 = 20$ ways to choose 2 flips to be heads out of 5 flips. \n\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order -- where flip 1 was heads first).\n\nThis illustrates the key distinction between combinations and permutations.\nA **permutation** is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. \nIf you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting. \n\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign \"slots\" in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length $n$, the number of permutations is:\n$$\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n$$\n\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\n\nNamely, the number of combinations of $k$ items from a set of $n$ items is given by the formula:\n$$\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n$$\n\nThis formula counts *all* of the possible permutations of the sequence, and then divides by the number of ways to arrange the $k$ items that are selected (which is $k!$) and the number of ways to arrange the remaining $n-k$ items (which is $(n-k)!$).\n\nIn our example, we have $n = 5$ (the number of flips) and $k = 2$ (the number of heads). So the number of combinations of 2 heads from 5 flips is:\n$$\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n$$\n\n\n## Probability functions\n\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck. \n\nHowever, it is often more convenient to work with **probability functions**. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function $f$ as follows:\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nwhere $x$ is the outcome of the coin flip (0 for heads, 1 for tails).\n\n::: {.callout-tip title=\"Functions map inputs to outputs\" collapse=\"true\"}\nFunctions are just a \"map\" that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range $[0, 1]$.\n:::\n\nThis might seem a bit redundant because we're just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values. \n\nFor example, if we have a die with six sides, we can define a probability function $f$ as follows:\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$ \n\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with $k$ sides, we can define a probability function $f$ as follows:\n\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThis is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| caption: Implement a discrete uniform probability function. Assume inputs are integers.\n#| echo: true\n\n# Define your function below\ndef f(x, k):\n    # your code here\n    pass\n\n# Example usage:\n# print(f(1, 5))  # Expected output: 0.2\n```\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| check: true\n\n# Define test cases\ntest_cases = [\n    (1, 5, 0.2),\n    (5, 5, 0.2),\n    (6, 5, 0),\n    (0, 5, 0),\n    (3, 3, 1/3),\n    (4, 3, 0)\n]\n\n# Initialize feedback\nfeedback = {\"correct\": True, \"message\": \"Great job!\"}\n\n# Check if function 'f' is defined\nif 'f' not in globals():\n    feedback = {\"correct\": False, \"message\": \"Function 'f' is not defined.\"}\nelse:\n    for x, k, expected in test_cases:\n        try:\n            result = f(x, k)\n            if abs(result - expected) > 1e-6:\n                feedback = {\n                    \"correct\": False,\n                    \"message\": f\"Test failed: f({x}, {k}) returned {result}, expected {expected}.\"\n                }\n                break\n        except Exception as e:\n            feedback = {\n                \"correct\": False,\n                \"message\": f\"Error when calling f({x}, {k}): {e}\"\n            }\n            break\n\nfeedback\n```\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| solution: true\n\ndef f(x, k):\n    if 1 <= x <= k:\n        return 1 / k\n    else:\n        return 0\n```\n\n## Random Variables\n\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like $X$ or $Y$. The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like $x$ or $y$. \n\nWe use **probability functions** to describe the probabilities associated with random variables. Specifically, a probability function $f$ for a random variable $X$ gives the probability that $X$ takes on a specific value $x$. \n\nFor example, let $X$ be a random variable that represents the outcome of flipping a fair coin. The probability function for $X$ would be:\n$$\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\n\n::: {.callout-note title=\"Bernoulli random variable\" collapse=\"true\"}\nThe above is an example of a **Bernoulli random variable**, which takes on the value 1 with probability $p$ and the value 0 with probability $1 - p$. In our case, $p = \\frac{1}{2}$ for a fair coin.\n:::\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let $Y$ be a random variable that represents the height of a person in centimeters. Let's assume that every person's height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for $Y$ would be:\n$$\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\n\n::: {.callout-note title=\"Uniform random variable\" collapse=\"true\"}\nThe above is an example of a **uniform random variable**, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is $\\frac{1}{50}$.\n:::\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\n\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.\n\n## Probability distributions and histograms\n\nWe call the probability function for a random variable a **probability distribution**, which describes how the probabilities are distributed across the possible values of the random variable.\n\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a **probability mass function (PMF)**, which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a **probability density function (PDF)**, which gives the density of probability at each point.\n\nLet's say we have a random variable $X$, but we don't know the exact probability function. Instead, we have a set of observed data points $\\{x_1, x_2, \\ldots, x_n\\}$ that we believe are individual realizations of $X$. In other words, we have a sample of data that we think is representative of the underlying random variable.\n\nHow can we visualize this data to understand the distribution of $X$? The simplest solution is to just plot how many times each value occurs in the data. We can use a **bar chart** to visualize the counts of each value. \n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let's check it out:\n\n\nNotice that when the $y$-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\n\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a **histogram** to visualize the distribution of the data.\n\nThe **histogram** is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or \"bins\") along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\n\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins. \n\n\n\n::: {.callout-tip title=\"Area under a probability distribution\"}\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. \nThis is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1. \n\nFor discrete:\n$$ \\sum_{x} f(x) = 1 $$\nFor continuous:\n$$ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 $$\n\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable $X$ falls between $a$ and $b$, we can compute the area under the PDF from $a$ to $b$:\n$$ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx $$\n\n:::\n\n## Expectation \n\nWe are often interested in the **average** value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\n\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play. \n\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you're not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero. \n\n:::{.callout-warning title=\"Gambling warning: the house always wins\" collapse=\"true\"}\nActually, at real casinos, the games are designed so that \"the house always wins\" in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance -- they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\n\nIn the short term this is hardly noticeable -- you're actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n:::\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\n\nThe **expectation** (or expected value) of a random variable $X$ is gives the average value of $X$ over many instances. It is denoted as $\\mathbb{E}[X]$ or $\\mu_X$. The expectation is calculated as follows:\n$$\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n$$\nwhere $f(x)$ is the probability function of $X$. For continuous random variables, the sum is replaced with an integral:\n$$\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n$$\n\nThe way to think about this is that the expectation is a weighted average of all possible values of $X$, where the weights are the probabilities of each value.\n\nSo in our roulette example, you can either lose \\$1 (with 90\\% probability) or win \\$9 (with 10\\% probability). The expectation would be:\n$$\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n$$\n\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win \\$10 on one game and lose \\$1 on the other 9 games. The average outcome is:\n$$\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n$$\n\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as $\\bar{X}$, and referred to as the sample mean. \n$$\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n\nThis approximation becomes more accurate as the number of samples $n$ increases. We will talk about this more in a future lecture.\n\n## Variance and standard deviation\n\nThe average is a useful summary of a random variable's central tendency, but it does not tell us anything about how spread out the values are. \n\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game -- the **average** amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\n\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can't afford to lose all of it on a single game.\n\nWe need a statistic that captures the typical *distance* between the values of the random variable and the average value. \n\n:::{.callout-note title=\"Why distance from the average?\" collapse=\"true\"}\nLet's imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what -- the only question is how much you win. Let's take an example where the payouts still differ by $10: you get $5 if you \"lose\" and $15 if you \"win\".\n\nIn this case, the expected value is:\n$$\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n$$\nSo you can expect to win $6 per game on average. \n\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\n\nThe answer is simple: we subtract the average from each value of $X$:\n$$X' = X - \\mathbb{E}[X]$$\nThis gives us a new random variable $X'$ that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\n$$\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0$$\n\nor more generally:\n$$\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0$$\n:::\n\nSo let's compute exactly that - the distance from the average. The formula for distance between two vectors $x$ and $y$ is:\n$$\nd^2 = \\sum_{i} (x_i - y_i)^2\n$$\nwhere $x_i$ and $y_i$ are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange ($a^2 + b^2 = c^2$).\n\nIn our case, we want to compute the distance between the values of the random variable $X$ and the average value $\\mathbb{E}[X]$.\n$$\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n$$\n\nNow we're getting somewhere! However, this is adding up all of the squared distances -- that means that the more values we have, the larger the distance will be. This is not quite right -- instead we want to compute the average distance from the mean in order to get a sense of how spread out the values *typically* are. \n\nSo we need to divide by the number of values:\n$$\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n$$\n\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. \nIf we replace the average with the expectation, we get the formula for the **variance** of a random variable $X$:\n$$\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n$$\n\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\n\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if $X$ represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the **standard deviation**:\n\n$$\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n$$\n\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset:\n$$\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n$$\n\n:::{.callout-note title=\"Sample variance vs. population variance\" collapse=\"true\"}\nTechnically, the formula above is an imperfect estimate of the population standard deviation.\nIt's in general a little bit too small, because the sample mean $\\bar{X}$ does not perfectly represent the population mean $\\mathbb{E}[X]$. We can correct for this by dividing by $n-1$ instead of $n$:\n$$\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n$$\nThis is called the **sample standard deviation**. \n\nWhy is the initial estimate too small? In a small dataset, the sample mean \"overfits\" the data, meaning it is closer to the individual data points than the true population mean. Let's think about this in terms of coin flips. If we flip a coin once, the sample mean is either $\\hat{X}=0$ or 1, depending on whether we got heads or tails. But the true population mean is $\\mathbb{E}[X]=\\frac{1}{2}$. If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either $(1-1)^2$ or $(0-0)^2$).\n\nBy contrast, the true (population) standard deviation is $\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$, which is larger than the estimate using the sample mean.\n\nThis **bias** in computing the standard deviation gets smaller as the sample size $n$ increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the $n-1$ correction to get a more accurate estimate of the population standard deviation.\n:::\n\n\n## Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. \nIt is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n<!-- Table of common probability distributions: -->\n| Distribution | Type | Probability Function | Parameters | Mean | Variance |\n|--------------|------|----------------------|------------|------|----------|\n| Bernoulli    | Discrete | $f(x) = p^x (1-p)^{1-x}$ | $p \\in [0, 1]$ | $p$ | $p(1-p)$ |\n| Binomial     | Discrete | $f(x) = \\binom{n}{x} p^x (1-p)^{n-x}$ | $n \\in \\mathbb{N}, p \\in [0, 1]$ | $np$ | $np(1-p)$ |\n| Poisson      | Discrete | $f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}$ | $\\lambda > 0$ | $\\lambda$ | $\\lambda$ |\n| Uniform      | Continuous | $f(x) = \\frac{1}{b-a}$ | $a < b$ | $\\frac{a+b}{2}$ | $\\frac{(b-a)^2}{12}$ |\n| Normal       | Continuous | $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ | $\\mu \\in \\mathbb{R}, \\sigma > 0$ | $\\mu$ | $\\sigma^2$  |\n| Exponential  | Continuous | $f(x) = \\lambda e^{-\\lambda x}$ | $\\lambda > 0$ | $\\frac{1}{\\lambda}$ | $\\frac{1}{\\lambda^2}$ |\n\nThe plots below show the probability functions for each of these distributions. \n\n\n\n## Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. \nProbability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\n\nIn particular, we covered:\n\n- The basic definition of probability and how to compute it for simple events.\n- The addition and multiplication rules for calculating probabilities of multiple events.\n- The concept of independence and how it affects probabilities.\n- Random variables and their probability distributions\n- The expectation (or expected value) of a random variable\n- Variance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\n\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!\n\n\n","srcMarkdownNoYaml":"\n\n::: {.hidden}\n$$\n \\renewcommand{\\P}{\\mathbb{P}}\n \\newcommand{\\E}{\\mathbb{E}}\n \\newcommand{\\Var}{\\mathbb{V}\\text{ar}}\n \\newcommand{\\Cov}{\\mathbb{C}\\text{ov}}\n$$\n:::\n\n## Probability\nMost of you are probably familiar with the basic intuition of **probability**: essentially it measures how likely an event is to occur. \n\nIn mathematical terms, the probability $\\P$ of an event $A$ is defined as:\n\n\\begin{align*}\n\\P(A) &= \\frac{\\text{ \\# of outcomes where } A \\text{ occurs}}{\\text{ total \\# of outcomes}} \\\\\n\\end{align*}\n\nBy definition this quantity cannot be negative ($\\P(A) = 0$ means $A$ never occurs), and it must be less than or equal to 1 ($\\P(A) = 1$ means $A$ always occurs).\n\nThe classical example of probability is flipping a coin. When you flip a fair coin, there are two possible outcomes: heads ($H$) and tails ($T$). If we let $A$ be the event that the coin lands on heads, then we can compute the probability of $A$ as follows:\n\n\\begin{align*}\n\\P(\\text{H}) &= \\frac{\\text{ \\# of heads}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{1}{2} \\\\\n\\end{align*}\n\nThis matches our intuition that a fair coin has a 50% chance of landing on heads.\n\nAn important property of probabilities is that the sum of the probabilities of all possible outcomes must equal 1. This is like say \"there's a 100% chance that *something* will happen\". \n\nIn our coin flip example, we have two possible outcomes: heads and tails. If the coin flip is not heads, it must be tails. In other words, the events $H$ and $T$ cover 100% of the possible outcomes. So we can write:\n\\begin{align*}\n\\P(H) + \\P(T) &= 1 \\\\\n\\frac{1}{2} + \\frac{1}{2} &= 1\n\\end{align*}\n\nWhen we know the events we are interested in make up all of the possible outcomes, we can use this property to compute probabilities. For example, for any event $A$, the event either happens or it doesn't. So we can compute the probability of the event not occurring as:\n\\begin{align*}\n\\P(\\text{not}~ A) &= 1 - \\P(A)\n\\end{align*}\n\n\n### Probability of multiple events\nBut what if we flip the coin twice? Now there are four possible outcomes: $HH$, $HT$, $TH$, and $TT$. \n\nIf we let $B$ be the event that at least one coin lands on heads, we can compute the probability of $B$ as follows:\n\\begin{align*}\n\\P(B) &= \\frac{\\text{ \\# of outcomes with at least one head}}{\\text{ total \\# of outcomes}} \\\\\n      &= \\frac{|\\{HH, HT, TH\\}|}{|\\{HH, HT, TH, TT\\}|} \\\\\n      &= \\frac{3}{4} \\\\\n\\end{align*}\n\n### Addition and multiplication rules (and / or)\n\nWhat is the probability of getting heads on the first flip AND the second flip (i.e., the event $C = \\{HH\\}$)?\n\nWell, there is only one outcome where both flips are heads, and there are still four total outcomes. So using our initial approach we know that $\\P(C) = \\P (H_1 ~\\text{and}~ H_2) = \\frac{1}{4}$.\n\nWhat about the probability of getting heads on the first flip OR the second flip? This is actually the same event as $B$ above, so we can use the same calculation: $\\P(B) = \\P(H_1 ~\\text{or}~ H_2) = \\frac{3}{4}$.\n\n::: {.callout-note title=\"Note on notation\" collapse=\"true\"}\nIn the above, we used $H_1$ and $H_2$ to denote heads on the first and second flips, respectively. The notation $H_1 ~\\text{and}~ H_2$ means both flips are heads, while $H_1 ~\\text{or}~ H_2$ means at least one flip is heads.\n\nIn probability theory, we often use the symbols $\\cap$ and $\\cup$ to denote \"and\" and \"or\" respectively. So we could also write $\\P(H_1 \\cap H_2)$ for the probability of both flips being heads, and $\\P(H_1 \\cup H_2)$ for the probability of at least one flip being heads. Technically, this is set notation where $\\cap$ means intersection (the event where both $H_1$ and $H_2$ occur), while $\\cup$ means union (the event where either $H_1$ or $H_2$ occurs).\n:::\n\nThere are some important rules for calculating probabilities of multiple events. In particular, if you hve two events $A$ and $B$, the following rules hold:\n\n- **Addition rule**: For any two events $A$ and $B$, the probability of either $A$ or $B$ occurring is given by:\n  $$\n  \\P(A \\cup B) = \\P(A) + \\P(B) - \\P(A \\cap B)\n  $$\n  This last term, $\\P(A \\cap B)$, is necessary to avoid double counting the outcomes where both $A$ and $B$ occur.\n\n  Note that if $A$ and $B$ are mutually exclusive (i.e., they cannot both occur at the same time), then $\\P(A \\cap B) = 0$, and the formula simplifies to:\n  $$  \\P(A \\cup B) = \\P(A) + \\P(B)$$\n\n:::{.callout-tip title=\"Visualizing sets of events\" collapse=\"true\"}\nThe following image illustrates the addition rule for two events $A$ and $B$ using a Venn diagram. \n![probability-set](../images/probability-sets-union.jpeg)\n:::\n\n- **Multiplication rule**: For any two events $A$ and $B$, the probability of both $A$ and $B$ occurring is given by:\n$$\\P(A \\cap B) = \\P(A) \\cdot \\P(B | A)$$\nwhere $\\P(B | A)$ is the conditional probability of $B$ given that $A$ has occurred. This means you first consider the outcomes where $A$ occurs, and then look at the probability of $B$ within that subset.\n\n\n::: {.callout-tip title=\"Conditional probability\" collapse=\"false\"}\nThe notation $\\P(B | A)$ is read as \"the probability of $B$ given $A$\". It represents the probability of event $B$ occurring under the condition that event $A$ has already occurred. \n\nWe make these adjustments in our heads all of the time. For example, you might expect that it is more likely I will buy ice cream if it is hot outside. In this case, the event $A$ is \"it is hot outside\", and the event $B$ is \"I buy ice cream\". The conditional probability $\\P(B | A)$ would be higher than $\\P(B)$ on a typical day.\n\nLet's think about this in the context of our coin flips. If we know that the first flip is heads ($H_1$), then only two outcomes are possible ($HH$ and $HT$) instead of four ($HH$, $HT$, $TH$, $TT$). \n\nSo the conditional probability $\\P(H_2 | H_1)$, which is the probability of the second flip being heads given that the first flip was heads, is:\n\\begin{align*}\n\\P(H_2 | H_1) &= \\frac{\\text{ \\# of outcomes where } H_2 \\text{ occurs and } H_1 \\text{ has occurred}}{\\text{ total \\# of outcomes where } H_1 \\text{ has occurred}} \\\\\n&= \\frac{|\\{HH\\}|}{|\\{HH, HT\\}|} \\\\\n&= \\frac{1}{2} \\\\\n\\end{align*}\n:::\n\nThe multiplication rule helps us calculate the probability of multiple events happening, as long as we know how one event affects the other (i.e., the conditional probability). An example will help clarify make this concrete.\n\nConsider a deck of cards (52 cards total, 13 of each suit). I might ask you, \"What is the probability of drawing a club on the first draw and a club on the second draw? (Assuming you do not replace the first card.)\"\n\n\n\n\n\nYou will see more complicated examples of probability in the assignment for this lecture, but the basic idea is the same: you count the number of outcomes where the event occurs, and divide by the total number of outcomes.\n\n### Independence\nTwo events $A$ and $B$ are said to be **independent** if the occurrence of one does not affect the probability of the other. \n\nHow does this relate to the multiplication rule? If $A$ and $B$ are independent, then the conditional probability $\\P(B | A)$ is simply $\\P(B)$. That is, knowing that $A$ has occurred does not change the probability of $B$ occurring.\n\nThis means that for independent events, the multiplication rule simplifies to:  \n$$\\P(A \\cap B) = \\P(A) \\cdot \\P(B)$$\n\nOur coin flip example illustrates this nicely. If we flip a fair coin twice, the outcome of the first flip does not affect the outcome of the second flip. Therefore, the two events (the first flip being heads and the second flip being heads) are independent. So the probability of both flips being heads is simply $\\P(H_1 \\cap H_2) = \\P(H_1) \\cdot \\P(H_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$.\n\n### Complicated counting\n\nCounting gets confusing and cumbersome quickly, especially when we have many events or outcomes. \n\nSay that I want to know the probability of getting exactly one head when flipping a coin 5 times. Let's think about the case where the first flip is heads. The probability of getting a head on the first flip is $\\frac{1}{2}$, and the probability of getting tails on the 4 other flips is also $\\frac{1}{2}$ each. Because the flips are independent, we can multiply these probabilities together to get the probability of this specific sequence of flips:\n$$\\P(H_1 \\cap T_2 \\cap T_3 \\ldots \\cap T_{5}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{2}\\right)^4 = \\frac{1}{2^{5}}$$\n\nAre we done? As it stands, this is the probability of getting heads on the first flip and tails on all other flips. But there are many other sequences that would also meet the conditions of getting \"exactly one head\". For example, we could have heads on the second flip and tails on all other flips, or heads on the third flip and tails on all other flips, and so on.\n\nIn fact, there are exactly 5 different sequences that would meet the conditions of getting exactly one head in 5 flips. So we need to multiply our previous result by the number of sequences that meet the conditions:\n$$\\P(\\text{exactly one head in 5 flips}) = 5 \\cdot \\frac{1}{2^{5}} = \\frac{5}{32} \\approx .16$$\n\nThere are two common types of outcomes we want to count: **permutations** and **combinations**. \n\nA **combination** is a selection of items or events without regard to the order in which they occur. For example, the number of ways 1 out of 5 flips could be heads. An important intuitive way to think about combinations is that we are **choosing** an item from a set. In our example, we are choosing 1 flip to be heads out of 5 flips.\n\n$$\n\\begin{align*}\n\\text{Flip 1 is heads} &= \\{1, 0, 0, 0, 0\\} \\\\\n\\text{Flip 2 is heads} &= \\{0, 1, 0, 0, 0\\} \\\\\n\\text{Flip 3 is heads} &= \\{0, 0, 1, 0, 0\\} \\\\\n\\vdots \\\\\n\\text{Flip 5 is heads} &= \\{0, 0, 0, 0, 1\\}\n\\end{align*}\n$$\n\nIt is clear here that there are 5 possible \"slots\" where we can place a head.\n\nWhat if we want to know the number of ways to choose 2 flips to be heads out of 5 flips? Naturally the logic above still\napplies, and the 5 flips we counted above are still all valid placements for one of the two heads. Now we just need to consider the second head.\n\nLet's take the first row from above, where the first flip is heads. Given that the first flip is heads, how many ways can we choose a second flip to also be heads? The second flip can be any of the remaining 4 flips, so there are 4 possible choices.\n$$\n\\begin{align*}\n\\text{Flip 1 and 2 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 1 and 3 are heads} &= \\{1, 0, 1, 0, 0\\} \\\\\n\\text{Flip 1 and 4 are heads} &= \\{1, 0, 0, 1, 0\\} \\\\\n\\text{Flip 1 and 5 are heads} &= \\{1, 0, 0, 0, 1\\}\n\\end{align*}\n$$\n\nNow let's consider the second row, where the second flip is heads. Given that the second flip is heads, how many ways can we choose a first flip to also be heads? The first flip can be any of the remaining 4 flips, so there are again 4 possible choices.\n\n$$\n\\begin{align*}\n\\text{Flip 2 and 1 are heads} &= \\{1, 1, 0, 0, 0\\} \\\\\n\\text{Flip 2 and 3 are heads} &= \\{0, 1, 1, 0, 0\\} \\\\\n\\text{Flip 2 and 4 are heads} &= \\{0, 1, 0, 1, 0\\} \\\\\n\\text{Flip 2 and 5 are heads} &= \\{0, 1, 0, 0, 1\\}\n\\end{align*}\n$$\n\nYou would continue this process for the third, fourth, and fifth flips. So for each of the 5 flips, you can choose any of the remaining 4 flips to be heads.\nThis gives us a total of $5 \\cdot 4 = 20$ ways to choose 2 flips to be heads out of 5 flips. \n\nBut wait! We already counted the combination of flips 2 and 1 earlier (just in a different order -- where flip 1 was heads first).\n\nThis illustrates the key distinction between combinations and permutations.\nA **permutation** is an arrangement of items or events in a specific order. So every possible combination of heads can be arranged in different ways, leading to different sequences of flips. \nIf you are only interested in counting combinations, listing out all of the possible arrangements like we did above leads to double counting. \n\nCounting all of the possible permutations of a sequence is straightforward. Using the logic above, you just assign \"slots\" in a sequence to each of the items you are arranging. Each time you allocate a slot, you have one fewer item to place in the remaining slots. So for a sequence of length $n$, the number of permutations is:\n$$\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdots 2 \\cdot 1\n$$\n\nNow, if we want to count combinations instead of permutations, we start with the number of permutations and then discount to account for the fact that the order does not matter.\n\nNamely, the number of combinations of $k$ items from a set of $n$ items is given by the formula:\n$$\n\\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!}\n$$\n\nThis formula counts *all* of the possible permutations of the sequence, and then divides by the number of ways to arrange the $k$ items that are selected (which is $k!$) and the number of ways to arrange the remaining $n-k$ items (which is $(n-k)!$).\n\nIn our example, we have $n = 5$ (the number of flips) and $k = 2$ (the number of heads). So the number of combinations of 2 heads from 5 flips is:\n$$\n\\binom{5}{2} = \\frac{5!}{2! \\cdot (5-2)!} = \\frac{5!}{2! \\cdot 3!} = \\frac{5 \\cdot 4 \\cdot 3!}{2 \\cdot 1 \\cdot 3!} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n$$\n\n\n## Probability functions\n\nThinking about probability in terms of counting outcomes is useful, and it is always a good idea to keep that intuition in mind if you ever get stuck. \n\nHowever, it is often more convenient to work with **probability functions**. A probability function assigns a probability to each possible outcome. In order to define a probability function, we need to be able to assign numerical values to each outcome. For example, if we have a fair coin, we can define a probability function $f$ as follows:\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nwhere $x$ is the outcome of the coin flip (0 for heads, 1 for tails).\n\n::: {.callout-tip title=\"Functions map inputs to outputs\" collapse=\"true\"}\nFunctions are just a \"map\" that tells you what output to expect for each input. A probability function is a special type of function that maps inputs to probabilities in the range $[0, 1]$.\n:::\n\nThis might seem a bit redundant because we're just presenting the same information in a new format. However, one reason that probability functions are important is that they allow us to concisely describe the probability of outcomes that have many possible values. \n\nFor example, if we have a die with six sides, we can define a probability function $f$ as follows:\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{6} & \\text{if } x = 1, 2, 3, 4, 5, 6 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$ \n\nBut we can also use the same function to describe the probability of rolling a die with any number of sides. For example, if we have a die with $k$ sides, we can define a probability function $f$ as follows:\n\n$$\nf(x) = \\begin{cases}\n    \\frac{1}{k} & \\text{if } x = 1, 2, \\ldots, k \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThis is much more concise than writing out the probability for each possible outcome, and it allows us to easily generalize to any number of sides.\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| caption: Implement a discrete uniform probability function. Assume inputs are integers.\n#| echo: true\n\n# Define your function below\ndef f(x, k):\n    # your code here\n    pass\n\n# Example usage:\n# print(f(1, 5))  # Expected output: 0.2\n```\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| check: true\n\n# Define test cases\ntest_cases = [\n    (1, 5, 0.2),\n    (5, 5, 0.2),\n    (6, 5, 0),\n    (0, 5, 0),\n    (3, 3, 1/3),\n    (4, 3, 0)\n]\n\n# Initialize feedback\nfeedback = {\"correct\": True, \"message\": \"Great job!\"}\n\n# Check if function 'f' is defined\nif 'f' not in globals():\n    feedback = {\"correct\": False, \"message\": \"Function 'f' is not defined.\"}\nelse:\n    for x, k, expected in test_cases:\n        try:\n            result = f(x, k)\n            if abs(result - expected) > 1e-6:\n                feedback = {\n                    \"correct\": False,\n                    \"message\": f\"Test failed: f({x}, {k}) returned {result}, expected {expected}.\"\n                }\n                break\n        except Exception as e:\n            feedback = {\n                \"correct\": False,\n                \"message\": f\"Error when calling f({x}, {k}): {e}\"\n            }\n            break\n\nfeedback\n```\n\n```{pyodide}\n#| exercise: uniform-pdf\n#| solution: true\n\ndef f(x, k):\n    if 1 <= x <= k:\n        return 1 / k\n    else:\n        return 0\n```\n\n## Random Variables\n\nA random variable is a quantity that can take on different values based on the outcome of a random event. It might be a discrete variable (like the outcome of a coin flip) or a continuous variable (like the height of a person). Basically it is an quantity that has randomness associated with it. We denote random variables with capital letters, like $X$ or $Y$. The specific values that a random variable can take on in a particular instance are usually denoted with lowercase letters, like $x$ or $y$. \n\nWe use **probability functions** to describe the probabilities associated with random variables. Specifically, a probability function $f$ for a random variable $X$ gives the probability that $X$ takes on a specific value $x$. \n\nFor example, let $X$ be a random variable that represents the outcome of flipping a fair coin. The probability function for $X$ would be:\n$$\nf(x) = \\P (X = x) = \\begin{cases}\n    \\frac{1}{2} & \\text{if } x = 0 \\text{ (heads)} \\\\\n    \\frac{1}{2} & \\text{if } x = 1 \\text{ (tails)} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\n\n::: {.callout-note title=\"Bernoulli random variable\" collapse=\"true\"}\nThe above is an example of a **Bernoulli random variable**, which takes on the value 1 with probability $p$ and the value 0 with probability $1 - p$. In our case, $p = \\frac{1}{2}$ for a fair coin.\n:::\n\nAs mentioned above, we can also think about random variables with continuous values. For example, let $Y$ be a random variable that represents the height of a person in centimeters. Let's assume that every person's height is equally likely to be between 150 cm and 200 cm (this is not true of course). The probability function for $Y$ would be:\n$$\nf(y) = \\P (Y = y) = \\begin{cases}\n    \\frac{1}{50} & \\text{if } 150 \\leq y \\leq 200 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\n\n::: {.callout-note title=\"Uniform random variable\" collapse=\"true\"}\nThe above is an example of a **uniform random variable**, which takes on values in a continuous range with equal probability. In our case, the range is from 150 cm to 200 cm, and the probability density function is $\\frac{1}{50}$.\n:::\n\n\nIn statistics, we treat our data as a random variable (or a collection of random variables). What this means is that we assume that the data we observe is just one possible outcome of a random process.\n\nThis is a powerful assumption because it allows us to use probability theory to make inferences about the underlying process that generated the data. This is going to be a key idea in the next lecture and throughout the course.\n\n## Probability distributions and histograms\n\nWe call the probability function for a random variable a **probability distribution**, which describes how the probabilities are distributed across the possible values of the random variable.\n\nDistributions can be discrete or continuous, depending on the type of random variable. For discrete random variables, the probability distribution is often represented as a **probability mass function (PMF)**, which gives the probability of each possible value. For continuous random variables, the probability distribution is represented as a **probability density function (PDF)**, which gives the density of probability at each point.\n\nLet's say we have a random variable $X$, but we don't know the exact probability function. Instead, we have a set of observed data points $\\{x_1, x_2, \\ldots, x_n\\}$ that we believe are individual realizations of $X$. In other words, we have a sample of data that we think is representative of the underlying random variable.\n\nHow can we visualize this data to understand the distribution of $X$? The simplest solution is to just plot how many times each value occurs in the data. We can use a **bar chart** to visualize the counts of each value. \n\n\n\nConsider a bunch of dice rolls. If we roll a die 100 times, we would expect to see each number appear roughly a similar number of times. If we plot the frequencies of each roll, we should see a discrete uniform distribution, where each number from 1 to 6 has approximately the same height. Let's check it out:\n\n\nNotice that when the $y$-axis represents probabilities, the heights of the bars sum to 1. This is because the total probability of all possible outcomes must equal 1!\n\nWhat about continuous random variables? In this case, we cannot just count the number of occurrences of each value, because there are infinitely many possible values. Instead, we can use a **histogram** to visualize the distribution of the data.\n\nThe **histogram** is a graphical representation that summarizes the distribution of a dataset. It divides the data into discrete, equally-sized intervals (or \"bins\") along the x-axis and counts how many data points fall into each bin. The height of each bar represents the either the total count of data points in that bin or the proportion of data points in that bin. If the height of the bar is the proportion, then the area of the bar represents the probability of the random variable falling within that bin.\n\nThe prices of Airbnb listings from back in the first lecture are a good example of a continuous random variable. The resolution (cents) is so small that basically every price is unique. So we cannot just count the number of occurrences of each price. Instead, we can create a histogram to visualize the distribution of prices across bins. \n\n\n\n::: {.callout-tip title=\"Area under a probability distribution\"}\nAt the beginning of this lecture, we said that the probability of all possible outcomes must sum up to 1. \nThis is true for both discrete and continuous random variables. For discrete random variables, the sum of the probabilities of all possible outcomes equals 1. For continuous random variables, the area under the probability density function (PDF) must equal 1. \n\nFor discrete:\n$$ \\sum_{x} f(x) = 1 $$\nFor continuous:\n$$ \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 $$\n\nWe can use the same idea to compute the probability of a continuous random variable falling within a certain range. For example, if we want to know the probability that a continuous random variable $X$ falls between $a$ and $b$, we can compute the area under the PDF from $a$ to $b$:\n$$ \\P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx $$\n\n:::\n\n## Expectation \n\nWe are often interested in the **average** value of a random variable. For example, if we play roulette, we might want to know the average amount of money we can expect to win or lose per game.\n\nWhy do we need an average? Since a random variable can take on many different values, a single sample does not give you a lot of information. You might win hundreds of dollars on one game, but this does not mean you will win that much every time you play. \n\nInstead, think about what would happen if we repeated the random process many times and took the average. Values that occur more frequently will tend to have a larger impact on the average, while values that occur less frequently will have a smaller impact. For example, at a casino roulette table perhaps you place a bet that has a 10% chance of winning. You might bet $1 and win $10 ($9 net profit) on one game, but if you lose $1 on the next 9 games you're not making money in the long run. Even though $9 profit sounds great, the fact that it happens so infrequently (and you lose $1 90% of the time) means that your average profit is actually zero. \n\n:::{.callout-warning title=\"Gambling warning: the house always wins\" collapse=\"true\"}\nActually, at real casinos, the games are designed so that \"the house always wins\" in the long run. So they would not let you bet $10 to win $100 ($90 profit) with a 10% chance -- they would give you worse odds, like a 9% chance of winning $100 for a $10 bet.\n\nIn the short term this is hardly noticeable -- you're actually quite likely to win a few times! But in the long term, the house edge means that you will lose money if you keep playing. This is why casinos are profitable businesses.\n:::\n\nWe can formalize this idea that the average gives more weight to values that occur more frequently.\n\nThe **expectation** (or expected value) of a random variable $X$ is gives the average value of $X$ over many instances. It is denoted as $\\mathbb{E}[X]$ or $\\mu_X$. The expectation is calculated as follows:\n$$\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x)\n$$\nwhere $f(x)$ is the probability function of $X$. For continuous random variables, the sum is replaced with an integral:\n$$\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n$$\n\nThe way to think about this is that the expectation is a weighted average of all possible values of $X$, where the weights are the probabilities of each value.\n\nSo in our roulette example, you can either lose \\$1 (with 90\\% probability) or win \\$9 (with 10\\% probability). The expectation would be:\n$$\n\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x \\in \\{-1, 10\\}} x \\cdot f(x) \\\\\n&= (-1) \\cdot 0.9 + (10-1) \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n$$\n\nThis is also the same as what you get if you just take the average of the outcomes. Say we play roulette 10 times, and we win \\$10 on one game and lose \\$1 on the other 9 games. The average outcome is:\n$$\n\\begin{align*}\n\\frac{1}{10} \\left( -1 \\cdot 9 + 9 \\cdot 1 \\right) &= -1 \\cdot 0.9 + 9 \\cdot 0.1 \\\\\n&= -0.9 + 0.9 \\\\\n&= 0\n\\end{align*}\n$$\n\nSo for a finite dataset, or set of outcomes, we can estimate the expected value by taking the average of the outcomes. This is often written as $\\bar{X}$, and referred to as the sample mean. \n$$\n\\mathbb{E}[X] \\approx \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n\nThis approximation becomes more accurate as the number of samples $n$ increases. We will talk about this more in a future lecture.\n\n## Variance and standard deviation\n\nThe average is a useful summary of a random variable's central tendency, but it does not tell us anything about how spread out the values are. \n\nConsider the roulette example again. If we play roulette many times, it does not matter how much we bet on each game -- the **average** amount we can expect to win or lose is always zero. You can bet $1 or $10,000 on each game, but the average outcome is still zero.\n\nOf course, the outcome of each game is not zero. Sometimes you win, sometimes you lose, and the amount you win or lose changes drastically depending on how much you bet.\n\n\n\nHow can we quantify this spread? Meaning, we want to capture that even though the average outcome is zero, winning $90 and losing $10 is very different from winning $9 and losing $1. Maybe you want to pay for dinner with your winnings, so a $90 payout is much more useful than a $9 payout. Or maybe you only have $10 in your pocket, so you can't afford to lose all of it on a single game.\n\nWe need a statistic that captures the typical *distance* between the values of the random variable and the average value. \n\n:::{.callout-note title=\"Why distance from the average?\" collapse=\"true\"}\nLet's imagine for a moment that there was a casino (a very poorly run casino) that let you place bets that win no matter what -- the only question is how much you win. Let's take an example where the payouts still differ by $10: you get $5 if you \"lose\" and $15 if you \"win\".\n\nIn this case, the expected value is:\n$$\n\\mathbb{E}[X] = \\sum_{x} x \\cdot f(x) = 5 \\cdot 0.9 + 15 \\cdot 0.1 = 4.5 + 1.5 = 6\n$$\nSo you can expect to win $6 per game on average. \n\nThe amount that the winnings vary, though is exactly the same as the original roulette game. How can we replicate this notion mathematically?\n\nThe answer is simple: we subtract the average from each value of $X$:\n$$X' = X - \\mathbb{E}[X]$$\nThis gives us a new random variable $X'$ that represents the distance from the average. Notice that this new random variable has an average of zero, just like the original roulette game.\n\n$$\\mathbb{E}[X'] = \\sum_{x} (x - \\mathbb{E}[X]) \\cdot f(x) = ((5-6) \\cdot 0.9 + (15-6) \\cdot 0.1 = (-1) \\cdot 0.9 + (9) \\cdot 0.1 = -0.9 + 0.9 = 0$$\n\nor more generally:\n$$\\mathbb{E}[X'] = \\mathbb{E}[X - \\mathbb{E}[X]] = \\mathbb{E}[X] - \\mathbb{E}[X] = 0$$\n:::\n\nSo let's compute exactly that - the distance from the average. The formula for distance between two vectors $x$ and $y$ is:\n$$\nd^2 = \\sum_{i} (x_i - y_i)^2\n$$\nwhere $x_i$ and $y_i$ are the elements of the two vectors. This is like the Pythagorean Theorem for computing the length of athe hypotenuse of a triange ($a^2 + b^2 = c^2$).\n\nIn our case, we want to compute the distance between the values of the random variable $X$ and the average value $\\mathbb{E}[X]$.\n$$\nd^2 = \\sqrt{\\sum_x (x - \\mathbb{E}[X])^2}\n$$\n\nNow we're getting somewhere! However, this is adding up all of the squared distances -- that means that the more values we have, the larger the distance will be. This is not quite right -- instead we want to compute the average distance from the mean in order to get a sense of how spread out the values *typically* are. \n\nSo we need to divide by the number of values:\n$$\nd^2_\\text{avg} = \\frac{1}{n} \\sum_x (x - \\mathbb{E}[X])^2\n$$\n\nSomething should feel familiar about this expression. Recall that the average is related to the expectation. \nIf we replace the average with the expectation, we get the formula for the **variance** of a random variable $X$:\n$$\n\\text{Var}(X) = \\sigma^2(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot f(x)\n$$\n\nThe variance tells us how spread out the values of a random variable are around the average. A larger variance means that the values are more spread out, while a smaller variance means that the values are closer to the average.\n\nThe variance is a useful statistic, but it is not in the same units as the original random variable. For example, if $X$ represents the amount of money you win or lose in dollars, then the variance is in dollars squared. This can make it difficult to interpret. So we often take the square root of the variance to get the **standard deviation**:\n\n$$\n\\text{SD}(X) = \\sigma(X) = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}\n$$\n\nLike with expected value, we can replace the expectation with the sample mean to get an estimate of the standard deviation (or variance) from a finite dataset:\n$$\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n$$\n\n:::{.callout-note title=\"Sample variance vs. population variance\" collapse=\"true\"}\nTechnically, the formula above is an imperfect estimate of the population standard deviation.\nIt's in general a little bit too small, because the sample mean $\\bar{X}$ does not perfectly represent the population mean $\\mathbb{E}[X]$. We can correct for this by dividing by $n-1$ instead of $n$:\n$$\n\\hat{\\sigma}(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2}\n$$\nThis is called the **sample standard deviation**. \n\nWhy is the initial estimate too small? In a small dataset, the sample mean \"overfits\" the data, meaning it is closer to the individual data points than the true population mean. Let's think about this in terms of coin flips. If we flip a coin once, the sample mean is either $\\hat{X}=0$ or 1, depending on whether we got heads or tails. But the true population mean is $\\mathbb{E}[X]=\\frac{1}{2}$. If we compute the standard deviation using the original formula, the distance from the sample mean is exactly 0! So the standard deviation is also 0 (either $(1-1)^2$ or $(0-0)^2$).\n\nBy contrast, the true (population) standard deviation is $\\sigma(X) = \\sqrt{\\mathbb{E}[(X - \\mathbb{E}[X])^2]} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$, which is larger than the estimate using the sample mean.\n\nThis **bias** in computing the standard deviation gets smaller as the sample size $n$ increases, so for large datasets the difference is negligible. In smaller datasets, though, it is important to use the $n-1$ correction to get a more accurate estimate of the population standard deviation.\n:::\n\n\n## Common probability distributions\nCertain probability distributions are very common, and their corresponding probability functions are well-known. \nIt is not necessary to memorize these distributions, but it is useful to be familiar with them and their properties.\n\n<!-- Table of common probability distributions: -->\n| Distribution | Type | Probability Function | Parameters | Mean | Variance |\n|--------------|------|----------------------|------------|------|----------|\n| Bernoulli    | Discrete | $f(x) = p^x (1-p)^{1-x}$ | $p \\in [0, 1]$ | $p$ | $p(1-p)$ |\n| Binomial     | Discrete | $f(x) = \\binom{n}{x} p^x (1-p)^{n-x}$ | $n \\in \\mathbb{N}, p \\in [0, 1]$ | $np$ | $np(1-p)$ |\n| Poisson      | Discrete | $f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}$ | $\\lambda > 0$ | $\\lambda$ | $\\lambda$ |\n| Uniform      | Continuous | $f(x) = \\frac{1}{b-a}$ | $a < b$ | $\\frac{a+b}{2}$ | $\\frac{(b-a)^2}{12}$ |\n| Normal       | Continuous | $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ | $\\mu \\in \\mathbb{R}, \\sigma > 0$ | $\\mu$ | $\\sigma^2$  |\n| Exponential  | Continuous | $f(x) = \\lambda e^{-\\lambda x}$ | $\\lambda > 0$ | $\\frac{1}{\\lambda}$ | $\\frac{1}{\\lambda^2}$ |\n\nThe plots below show the probability functions for each of these distributions. \n\n\n\n## Summary\nThis lecture introduced many important concepts from probability theory that will be useful throughout the course. \nProbability gives us a mathematical language and toolkit for reasoning about uncertainty and randomness in data, by thinking about possible outcomes and their likelihoods.\n\nIn particular, we covered:\n\n- The basic definition of probability and how to compute it for simple events.\n- The addition and multiplication rules for calculating probabilities of multiple events.\n- The concept of independence and how it affects probabilities.\n- Random variables and their probability distributions\n- The expectation (or expected value) of a random variable\n- Variance and standard deviation\n\nGoing forward, these concepts will be foundational for statistical modeling and designing good simulations and statistical tests.\n\nAssignment 2 will give you a chance to work through some of these concepts in more detail, so be sure to check it out!\n\n\n"},"formats":{"live-html":{"identifier":{"display-name":"HTML","target-format":"live-html","base-format":"html","extension-name":"live"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["/Users/jrudoler/Documents/teaching/understanding-uncertainty/_extensions/r-wasm/live/live.lua"],"toc":true,"output-file":"lecture-02.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","ojs-engine":true,"revealjs-plugins":[],"theme":{"light":"flatly","dark":"darkly"},"title":"Lecture 02: Probability and Random Variables","description":"Introduction to Probability and Random Variables","author":"Joseph Rudoler","date":"now","pyodide":{"packages":["matplotlib","numpy"]},"toc-location":"right","code-summary":"Code","code-block-name":"Code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["live-html"]}