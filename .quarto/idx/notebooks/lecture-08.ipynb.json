{"title":"Lecture 08: Linear Regression","markdown":{"yaml":{"title":"Lecture 08: Linear Regression","date":"now","format":{"live-html":{"toc":true,"toc-location":"right","code-fold":true,"code-tools":true,"code-summary":"Code","code-block-name":"Code"}},"pyodide":{"packages":["matplotlib","numpy"]},"execute":{"warning":false}},"headingText":"Predictions from patterns","containsRefs":false,"markdown":"\n\nNow that we've talked about inference in depth, it is time to talk about **prediction**. In this lecture, we will build up to the basics of linear regression, which is a powerful tool for making predictions based on data.\n\n\nHow do we make predictions? We typically look for patterns in the data and use those patterns to make informed guesses about future outcomes. \n\n:::{.callout-note title=\"Why not memorize?\"}\nWhy not just memorize the data? Think about a dataset like this:\n\n| Name | Apples / Day |\n|------|--------------|\n| Alice | 3 |\n| Bob   | 5 |\n| Charlie | 2 |\n\nIf you were asked to predict how many apples a new person would eat per day, you could memorize the data and say \"Alice eats 3 apples, Bob eats 5 apples, Charlie eats 2 apples.\" You would be 100% correct on this dataset! But what if you were asked to predict how many apples a new person would eat? You would have no idea, because your memorized list only contains the data you have seen already. You have no way to make a prediction about a new person like Dave:\n| Name | Apples / Day |\n|------|--------------|\n| Dave | ? |\n\nSo, we need a better strategy!\n\nMore broadly, this is called **generalization**: the ability to make predictions about new data based on patterns learned from existing data. \n:::\n\nConsider the following dataset of the heights (in inches) of a sample of adults. The dataset also records the height of their parents, so we'll refer to the adults as \"children\" and their parents as \"parents\" in this context. ^[The Galton dataset is is a classic dataset in statistics, named after Sir Francis Galton, who studied the relationship between the heights of parents and their children. The study is the origin of the term \"linear regression\" and is murkily involved in the eugenics movement of the early 1900s. Data from [here](https://doi.org/10.7910/DVN/T0HSJ1).]\n\nLet's say I asked you to predict the height of another child not in the dataset. Assume they're from the same population as the children in the dataset. You have no other information about the child. How would you do it?\n\nThe most straightforward thing to do is ask yourself: \"What is the typical height of a child in this dataset?\" You could then use that typical height as your prediction. This is the simplest kind of pattern recognition: finding the average value in the dataset and using it as a prediction. \n\nThis isn't bad as a first guess, and you actually can't do any better than this if you have no other information about the child. \n\nBut what if you had more information? For example, say you had other information about the child (besides their own height), such as their sex (male / female) or the heights of their parents. If you think that these factors might be related to the child's height, they should inform your prediction.\n\nLet's start by just plotting the heights of the children in the dataset against the heights of their parents. This will help us visualize the relationship between the two variables.\n\nNotice how the naive prediction we made earlier (the average height of the children), shown as a blue dashed line, totally ignores the heights of the parents. It's constant across the entire plot, and does not capture any of the variation in the data.\n\nYou may, based on your own experience and understanding of the world, expect that children with taller parents tend to be taller themselves. It sure looks that way in the plot above -- the taller children tend to have taller parents, and the shorter children tend to have shorter parents. It's not always the case, by any means, but it seems to be a general trend.\n\nBut **how much** taller? We want to quantify this relationship, so that we can get a formula for predicting a child's height based on their parents' heights. Mathematically, we want to define a function $f$ that takes the heights of the parents as input and returns the predicted height of the child as output. \n\n#### Preparing the data\n\nDealing with two separate parents is cumbersome. Can we combine the heights of the parents into a single number that represents \"the height of the parents\"? One way to do this is to take the average of the two heights. \n\nThis is problematic at the moment. Based on the plot it seems that the father's height has a different relationship with the child's height than the mother's height does. The trends are similar, but the plots have different shapes. This is probably because men and women tend to have different average heights (as you can see in the plot above). Also because men are taller on average, they contribute more to the average height of the parents than women do.\n\nThere is a useful trick we can use to simplify this. We can put the heights of men and women in the dataset on the same scale by **standardizing** them. \n\n$$\n\\begin{align*}\n\\text{standardized female height} = \\frac{\\text{height} - \\text{average of female heights}}{\\text{standard deviation of female heights}} \\\\\n\\text{standardized male height} = \\frac{\\text{height} - \\text{average of male heights}}{\\text{standard deviation of male heights}} \\\\\n\\end{align*}\n$$\n\nThis procedure transforms all of the heights into a common scale, where the average is zero and the standard deviation is one. This process is called **standardization** or **z-scoring**. \n\nLet's do this and plot the standardized heights of the parents against the heights of the children.\n\nNow we have put the heights of the parents on the same scale, so we can combine them into a single number that represents \"the height of the parents\". We can do this by taking the average of the standardized heights of the parents.\n\nNice! It seems like this retains the relationship between the heights of the parents and the heights of the children, but now we have a single number that represents the height of the parents.\n\nLet's go back to the units we care about predicting, though. It's ok to use standardized heights for the parents, but we want to predict the height of the child in inches.\n\n### Quantifying relationships\nBack to our question: **how much** taller are the children of tall parents? \n\nThe simplest way to quantify this relationship is to use a linear function, which is a function of the form $f(x) = mx + b$, where $m$ is the slope of the line and $b$ is the y-intercept. A linear function has a constant rate of change, which means that for every unit increase in the input, the output increases by a fixed amount. Fitting a linear function to data is called **linear regression**.\n\nIn our case, we choose to *model* the relationship between the heights of the parents and the heights of the children as a linear function. \n\n$$\\text{predicted height of child} = \\text{slope} \\times \\text{midparent height} + \\text{intercept}$$\n\nNote that our first model, the average height of the children, is a special case of this linear function where the slope is zero and the intercept is the average height of the children.\n\nWhat *should* the slope be? Basically, we want it to be whatever value makes the predicted heights of the children as close to the actual heights of the children as possible.\n\nWe can use a statistical technique called **ordinary least squares** (OLS) to find the best-fitting line for our data. OLS minimizes the (squared) distance between the predicted heights of the children and the actual heights of the children.\n\n::: {.callout-note title=\"Why squared?\" collapse=true}\nOLS minimizes $(\\text{predicted height} - \\text{actual height})^2$. Why squared? There are two main reasons:\n1. Squaring the differences ensures that we don't have negative values canceling out positive values\n2. When you square a number, it becomes larger as the number gets larger. This means that larger errors are penalized more heavily than smaller errors. So you'd rather be off by 1 inch 10 times (squared error of 10) than be off by 10 inches once (squared error of 100). This helps us find a line that is close to all of the points and avoids being too far off on any one point.\n\n*Advanced note:* If you like calculus, the other benefit of squared errors is that they are differentiable (think about a parabola $y = x^2$ - the slope is always changing, but it never has a sharp corner). This is a big difference compared to absolute errors, which have a sharp corner at zero (the slope of $|x|$ is not defined at zero, where it changes from -1 to 1).\n:::\n\nLet's fit a linear regression model to the data and see what the best-fitting line looks like.\n\n::: {.callout-note title=\"Specifying regression models\" collapse=true}\nStatistical software packages have built-in functions for fitting many common models to data. There are a variety of packages that do pretty much the same thing, and they all have their own syntax for specifying the model you want to fit.\n\n\n\nSome packages accept a formula as input, which specifies the relationship between the variables. The \"formula\" is a string that describes the model you want to fit, and has its own syntax. In this case, we want to predict the height of the child based on the midparent height, so we use the formula `height ~ midparent`. The `~` symbol separates the outcome variable (the variable we want to predict) from the input variable (the variable we are using to make the prediction). The `+` symbol can be used to add more input variables to the model, but in this case we only have one input variable. The intercept is included by default, so we don't need to specify it explicitly.\n:::\n\nIn the second plot, we show the prediction errors for a few datapoints. The red lines show the difference between the predicted height of the child and the actual height of the child. OLS finds the best fit that minimizes the sum of the squared lengths of these red lines.\n\nSo what's the slope that answers our question? We can extract the parameters of the model to find out. The slope is the parameter associated with the midparent height variable.\n\nSo what exactly can we say? \n\nThe slope of a line $y = mx + b$ is the change in $y$ for a one-unit increase in $x$. So, in our case, the slope is the change in the predicted height of the child for a one-unit increase in the midparent height.\n\nSo, for every one unit increase in the midparent height, the predicted height of the child increases by about 1.7 inches. \n\nThis probably seems confusing because the midparent heights are standardized and averaged. It means that if the parents' heights are on average one standard deviation above the mean height, the predicted height of the child is 1.7 inches taller than the average height of the children in the dataset.\n\nSo how did we do in terms of prediction? \n\nLet's take a look at the distribution of the errors. \n\nWe often evaluate our predictions by studying the mean and standard deviation of the prediction errors. \nWe want the mean to be close to zero, which means that our predictions are on average correct. You don't want to be consistently over- or under-predicting. The standard deviation tells us how much the prediction errors vary.\n\nSpecifically, the standard deviation is called the \"Root Mean Squared Error\" (RMSE), and it tells us how far off our predictions are from the actual values on average:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted height}_i - \\text{actual height}_i)^2}$$\nSo, in our case the RMSE is about 3.4. This means that on average, our predictions are off by about 3.4 inches. \n\nLet's compare this to the RMSE of the naive prediction we made earlier (the average height of the children). \n\nNotice anything else interesting about the residual distribution? It's centered around zero, and most of the residuals close to that mean -- it's approximately normal!\n\nWe skipped over this important detail before, but that's actually a key assumption of the linear regression model. The residuals (or you can think of them as \"noise\" -- variation in the data that we can't explain with our model) should be normally distributed. \n\nThis highlights the connection between linear regression and the kind of probabilistic statistical models we have been discussing in this course all along. The regression model treats the variable we are predicting (the height of the child) as a random variable itself. It has an expected value that depends in a linear way on the input variable, but it also has some inherent variability.\n\nSo the linear regression model predictions can be written as:\n$$y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\sigma^2$ is the variance of the residuals.\n\nThe consequence of this variability is that, just like our previous models, our data-driven conclusions are subject to uncertainty from sampling. We might just **happen** to get a sample where the children of tall parents are shorter than average, and get a slope that is negative. \n\nThis is why we need to be careful about interpreting the results of our regression model. We'll return to the hypothesis testing framework in the next lecture, and show how it can be applied to regression models.\n","srcMarkdownNoYaml":"\n\nNow that we've talked about inference in depth, it is time to talk about **prediction**. In this lecture, we will build up to the basics of linear regression, which is a powerful tool for making predictions based on data.\n\n## Predictions from patterns\n\nHow do we make predictions? We typically look for patterns in the data and use those patterns to make informed guesses about future outcomes. \n\n:::{.callout-note title=\"Why not memorize?\"}\nWhy not just memorize the data? Think about a dataset like this:\n\n| Name | Apples / Day |\n|------|--------------|\n| Alice | 3 |\n| Bob   | 5 |\n| Charlie | 2 |\n\nIf you were asked to predict how many apples a new person would eat per day, you could memorize the data and say \"Alice eats 3 apples, Bob eats 5 apples, Charlie eats 2 apples.\" You would be 100% correct on this dataset! But what if you were asked to predict how many apples a new person would eat? You would have no idea, because your memorized list only contains the data you have seen already. You have no way to make a prediction about a new person like Dave:\n| Name | Apples / Day |\n|------|--------------|\n| Dave | ? |\n\nSo, we need a better strategy!\n\nMore broadly, this is called **generalization**: the ability to make predictions about new data based on patterns learned from existing data. \n:::\n\nConsider the following dataset of the heights (in inches) of a sample of adults. The dataset also records the height of their parents, so we'll refer to the adults as \"children\" and their parents as \"parents\" in this context. ^[The Galton dataset is is a classic dataset in statistics, named after Sir Francis Galton, who studied the relationship between the heights of parents and their children. The study is the origin of the term \"linear regression\" and is murkily involved in the eugenics movement of the early 1900s. Data from [here](https://doi.org/10.7910/DVN/T0HSJ1).]\n\nLet's say I asked you to predict the height of another child not in the dataset. Assume they're from the same population as the children in the dataset. You have no other information about the child. How would you do it?\n\nThe most straightforward thing to do is ask yourself: \"What is the typical height of a child in this dataset?\" You could then use that typical height as your prediction. This is the simplest kind of pattern recognition: finding the average value in the dataset and using it as a prediction. \n\nThis isn't bad as a first guess, and you actually can't do any better than this if you have no other information about the child. \n\nBut what if you had more information? For example, say you had other information about the child (besides their own height), such as their sex (male / female) or the heights of their parents. If you think that these factors might be related to the child's height, they should inform your prediction.\n\nLet's start by just plotting the heights of the children in the dataset against the heights of their parents. This will help us visualize the relationship between the two variables.\n\nNotice how the naive prediction we made earlier (the average height of the children), shown as a blue dashed line, totally ignores the heights of the parents. It's constant across the entire plot, and does not capture any of the variation in the data.\n\nYou may, based on your own experience and understanding of the world, expect that children with taller parents tend to be taller themselves. It sure looks that way in the plot above -- the taller children tend to have taller parents, and the shorter children tend to have shorter parents. It's not always the case, by any means, but it seems to be a general trend.\n\nBut **how much** taller? We want to quantify this relationship, so that we can get a formula for predicting a child's height based on their parents' heights. Mathematically, we want to define a function $f$ that takes the heights of the parents as input and returns the predicted height of the child as output. \n\n#### Preparing the data\n\nDealing with two separate parents is cumbersome. Can we combine the heights of the parents into a single number that represents \"the height of the parents\"? One way to do this is to take the average of the two heights. \n\nThis is problematic at the moment. Based on the plot it seems that the father's height has a different relationship with the child's height than the mother's height does. The trends are similar, but the plots have different shapes. This is probably because men and women tend to have different average heights (as you can see in the plot above). Also because men are taller on average, they contribute more to the average height of the parents than women do.\n\nThere is a useful trick we can use to simplify this. We can put the heights of men and women in the dataset on the same scale by **standardizing** them. \n\n$$\n\\begin{align*}\n\\text{standardized female height} = \\frac{\\text{height} - \\text{average of female heights}}{\\text{standard deviation of female heights}} \\\\\n\\text{standardized male height} = \\frac{\\text{height} - \\text{average of male heights}}{\\text{standard deviation of male heights}} \\\\\n\\end{align*}\n$$\n\nThis procedure transforms all of the heights into a common scale, where the average is zero and the standard deviation is one. This process is called **standardization** or **z-scoring**. \n\nLet's do this and plot the standardized heights of the parents against the heights of the children.\n\nNow we have put the heights of the parents on the same scale, so we can combine them into a single number that represents \"the height of the parents\". We can do this by taking the average of the standardized heights of the parents.\n\nNice! It seems like this retains the relationship between the heights of the parents and the heights of the children, but now we have a single number that represents the height of the parents.\n\nLet's go back to the units we care about predicting, though. It's ok to use standardized heights for the parents, but we want to predict the height of the child in inches.\n\n### Quantifying relationships\nBack to our question: **how much** taller are the children of tall parents? \n\nThe simplest way to quantify this relationship is to use a linear function, which is a function of the form $f(x) = mx + b$, where $m$ is the slope of the line and $b$ is the y-intercept. A linear function has a constant rate of change, which means that for every unit increase in the input, the output increases by a fixed amount. Fitting a linear function to data is called **linear regression**.\n\nIn our case, we choose to *model* the relationship between the heights of the parents and the heights of the children as a linear function. \n\n$$\\text{predicted height of child} = \\text{slope} \\times \\text{midparent height} + \\text{intercept}$$\n\nNote that our first model, the average height of the children, is a special case of this linear function where the slope is zero and the intercept is the average height of the children.\n\nWhat *should* the slope be? Basically, we want it to be whatever value makes the predicted heights of the children as close to the actual heights of the children as possible.\n\nWe can use a statistical technique called **ordinary least squares** (OLS) to find the best-fitting line for our data. OLS minimizes the (squared) distance between the predicted heights of the children and the actual heights of the children.\n\n::: {.callout-note title=\"Why squared?\" collapse=true}\nOLS minimizes $(\\text{predicted height} - \\text{actual height})^2$. Why squared? There are two main reasons:\n1. Squaring the differences ensures that we don't have negative values canceling out positive values\n2. When you square a number, it becomes larger as the number gets larger. This means that larger errors are penalized more heavily than smaller errors. So you'd rather be off by 1 inch 10 times (squared error of 10) than be off by 10 inches once (squared error of 100). This helps us find a line that is close to all of the points and avoids being too far off on any one point.\n\n*Advanced note:* If you like calculus, the other benefit of squared errors is that they are differentiable (think about a parabola $y = x^2$ - the slope is always changing, but it never has a sharp corner). This is a big difference compared to absolute errors, which have a sharp corner at zero (the slope of $|x|$ is not defined at zero, where it changes from -1 to 1).\n:::\n\nLet's fit a linear regression model to the data and see what the best-fitting line looks like.\n\n::: {.callout-note title=\"Specifying regression models\" collapse=true}\nStatistical software packages have built-in functions for fitting many common models to data. There are a variety of packages that do pretty much the same thing, and they all have their own syntax for specifying the model you want to fit.\n\n\n\nSome packages accept a formula as input, which specifies the relationship between the variables. The \"formula\" is a string that describes the model you want to fit, and has its own syntax. In this case, we want to predict the height of the child based on the midparent height, so we use the formula `height ~ midparent`. The `~` symbol separates the outcome variable (the variable we want to predict) from the input variable (the variable we are using to make the prediction). The `+` symbol can be used to add more input variables to the model, but in this case we only have one input variable. The intercept is included by default, so we don't need to specify it explicitly.\n:::\n\nIn the second plot, we show the prediction errors for a few datapoints. The red lines show the difference between the predicted height of the child and the actual height of the child. OLS finds the best fit that minimizes the sum of the squared lengths of these red lines.\n\nSo what's the slope that answers our question? We can extract the parameters of the model to find out. The slope is the parameter associated with the midparent height variable.\n\nSo what exactly can we say? \n\nThe slope of a line $y = mx + b$ is the change in $y$ for a one-unit increase in $x$. So, in our case, the slope is the change in the predicted height of the child for a one-unit increase in the midparent height.\n\nSo, for every one unit increase in the midparent height, the predicted height of the child increases by about 1.7 inches. \n\nThis probably seems confusing because the midparent heights are standardized and averaged. It means that if the parents' heights are on average one standard deviation above the mean height, the predicted height of the child is 1.7 inches taller than the average height of the children in the dataset.\n\nSo how did we do in terms of prediction? \n\nLet's take a look at the distribution of the errors. \n\nWe often evaluate our predictions by studying the mean and standard deviation of the prediction errors. \nWe want the mean to be close to zero, which means that our predictions are on average correct. You don't want to be consistently over- or under-predicting. The standard deviation tells us how much the prediction errors vary.\n\nSpecifically, the standard deviation is called the \"Root Mean Squared Error\" (RMSE), and it tells us how far off our predictions are from the actual values on average:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted height}_i - \\text{actual height}_i)^2}$$\nSo, in our case the RMSE is about 3.4. This means that on average, our predictions are off by about 3.4 inches. \n\nLet's compare this to the RMSE of the naive prediction we made earlier (the average height of the children). \n\nNotice anything else interesting about the residual distribution? It's centered around zero, and most of the residuals close to that mean -- it's approximately normal!\n\nWe skipped over this important detail before, but that's actually a key assumption of the linear regression model. The residuals (or you can think of them as \"noise\" -- variation in the data that we can't explain with our model) should be normally distributed. \n\nThis highlights the connection between linear regression and the kind of probabilistic statistical models we have been discussing in this course all along. The regression model treats the variable we are predicting (the height of the child) as a random variable itself. It has an expected value that depends in a linear way on the input variable, but it also has some inherent variability.\n\nSo the linear regression model predictions can be written as:\n$$y \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\sigma^2$ is the variance of the residuals.\n\nThe consequence of this variability is that, just like our previous models, our data-driven conclusions are subject to uncertainty from sampling. We might just **happen** to get a sample where the children of tall parents are shorter than average, and get a slope that is negative. \n\nThis is why we need to be careful about interpreting the results of our regression model. We'll return to the hypothesis testing framework in the next lecture, and show how it can be applied to regression models.\n"},"formats":{"live-html":{"identifier":{"display-name":"HTML","target-format":"live-html","base-format":"html","extension-name":"live"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["/Users/jrudoler/Documents/teaching/understanding-uncertainty/_extensions/r-wasm/live/live.lua"],"toc":true,"output-file":"lecture-08.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","ojs-engine":true,"revealjs-plugins":[],"theme":{"light":"flatly","dark":"darkly"},"title":"Lecture 08: Linear Regression","date":"now","pyodide":{"packages":["matplotlib","numpy"]},"toc-location":"right","code-summary":"Code","code-block-name":"Code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["live-html"]}