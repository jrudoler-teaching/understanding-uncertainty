{"title":"Lecture 04: Data Generating Processes and Statistical Models","markdown":{"yaml":{"title":"Lecture 04: Data Generating Processes and Statistical Models","date":"now","format":{"live-html":{"toc":true,"toc-location":"right","code-fold":true,"code-tools":true,"code-summary":"Code","code-block-name":"Code"}},"reference-location":"margin","citation-location":"margin","pyodide":{"packages":["matplotlib","numpy"]},"execute":{"warning":false}},"headingText":"Data Generating Processes","containsRefs":false,"markdown":"\n\n\nIn the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example). \n\nThis leads us to a useful way of thinking about data: **all data is generated by some underlying process**. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! \"Some process\" is a bit of a catch-all: of course the data doesn't just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about **data generating processes** (DGPs) is the key to analyzing data.\n\n\n## Statistical Models\nA **statistical model** is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\n\nLet's return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a **Bernoulli distribution**, which describes the probability of each outcome. Specifically, \n\n$$P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}$$\n\nwhere \\(X\\) is the outcome of the coin flip, \\(p\\) is the probability of heads, and \\(1 - p\\) is the probability of tails. If we assume a fair coin, then \\(p = 0.5\\).\n\nNow clearly, this model does not capture the complexity of a real-world coin flip, which is of course influenced by many factors such as the weight of the coin, the force of the flip, air resistance, etc. Statistical models are always reductive in this sense. But the important thing is that a Bernoulli distribution really does do a good job of describing the **outcomes** of a coin flip. As long as that is the case, we can use the model to make predictions about the data generated by the process.\n\nTo see this, consider the following process: you roll a fair die 100 times. For each roll, you record whether the number you rolled was even or odd. Since there are three even numbers (2, 4, 6) and three odd numbers (1, 3, 5), and each number has an equal probability of being rolled, the probability of rolling an even number is 0.5 and the probability of rolling an odd number is also 0.5. This is statistically identical to flipping a fair coin! So we can use the same Bernoulli distribution to model the outcomes of this process, even though it completely ignore the details of the die rolling process itself. We lose information about the specific numbers rolled, but as long as we accurately capture the probability of the outcomes we care about, it doesn't matter.\n\n\n\n<!-- Think back to Lecture 00, where we analyzed a dataset of Airbnb listings.  -->\n\nStatistical modeling generally uses this kind of trick. We attempt to find a simple model that captures the essential features of the data generating process. \n\nWe don't always know as much about the data generating process as we do for a coin flip or a die roll. In fact, often we don't know anything about it at all! In these cases, the approach is a little more \"guess and check\". \nWe start with a simple model, and see how well it describes the data. If it doesn't work, we try a more complex model or a different model altogether. \n\n### Challenges with finite samples\nIt can be hard to tell if you have a good model or not. With any finite dataset, it is quite likely that the data will not perfectly match the model. This is because the model describes probabilistic *tendencies* of the data. But in any small sample we will likely see deviations from the idealized outcomes described by the model. \n\nThink about this in the extreme: if you flip a coin only once, you will either get heads or tails. So your observed proportion of heads will be either 0 or 1, which is very different from the expected proportion of 0.5. In fact it is impossible to observe the expected proportion of heads in a single flip!\nThis is true for a single observation of basically any random variable that takes on more than one value -- it is close to impossible to learn anything about the variable's tendencies from just one observation. \n\nEven with a small number of observations, it is quite difficult to tell if a model is a good fit for the data. \n\nLet's say you and your roommate are always arguing about who should take out the trash. You decide to flip a coin to decide who takes it out (you are notorious for always choosing tails, because you think it's good luck). You decide to do a best-of-ten series, so you flip the coin 10 times and record the number of heads. It turns out that you get 3 heads and 7 tails. Your roommate is *furious*: \"That's not fair! You always get tails! I bet you rigged the coin!\"\n\nIs your roommate justified in being suspicious? What is the probability of getting 3 heads in 10 flips of a fair coin? What is the probability of getting 3 heads in 10 flips of a biased coin that has a 25\\% chance of landing on heads?\n\n\nAs this exercise shows, with a small number of observations you have *some* information that can help you distinguish between the two models, but it is not enough to be confident in your conclusion. \n\nWith more data though, suddenly the evidence becomes much stronger. The probability of getting a badly imbalanced distribution of heads and tails becomes tiny as the number of flips increases.\n\nIn general this is great news -- the more data you have, the more confident you can be in your conclusions. It's probably not worth flipping a coin 1000 times to decide who takes out the trash, but if you did, you would be able to tell with much more certainty whether the coin is fair or not.\n\n## Convergence for large sample sizes\nWhen the size of a dataset is large enough, we get some guarantees.\n\n### Law of Large Numbers\nThe Law of Large Numbers (LLN) states that as the sample size increases, the sample mean will converge to the population mean. In other words, if we take enough samples, the average of those samples will be close to the true average of the population.\n\n::: {.callout-note title=\"Law of Large Numbers\" collapse=\"false\"}\nLet $X_1, X_2, \\ldots, X_n$ be independent and identically distributed random variables with expected value $\\mathbb{E}[X]$, then the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ converges to $\\mathbb{E}[X]$ as $n$ approaches infinity:\n$$\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1$$\n\nPrecisely, this states that the probability that the sample mean converges to the population mean is 1. \n^[\nThis is technically the *Strong Law of Large Numbers*, stating that the sample mean converges almost surely to the population mean. There is also a *Weak Law of Large Numbers* which states that the sample mean converges in probability to the population mean, which is a weaker condition.\n]\n:::\n\nYou don't just have to take this for granted -- let's see the LLN in action. \n\nWe will simulate flipping a biased coin $n$ times, and plot the proportion of heads as we increase the number of flips. We will see that the proportion converges to 0.25 as the number of flips increases.\n\nThat's just one experiment (flip $n$ coins a single time). But this is a random process, so we can simulate it many times\nand see how the proportion of heads changes. \n\nAs you can see, the variability across simulations decreases as the number of flips increases. This is what we mean when we say that the sample mean converges to the population mean. It's not just close to the true mean on average, it is also more consistent across different samples/simulations.\n\nWe're going to take advantage of this fact all the time. Basically any time you compare groups, or competing hypotheses, or evaluate predictive models, you end up taking the average of some quantity. The Law of Large Numbers tells us that as the sample size increases, the average will converge to the true value.\n\nAgain, what does this mean for your argument with your roommate? As you gather more and more data (coin flips), the proportion of heads (which is itself a sample mean) will converge to the true proportion of heads (the population mean). With enough flips, it becomes very clear whether the coin is rigged or not.\n\n:::{.callout-note title=\"Unbiased Estimators\"}\nNotice in the simulation above that even when the sample mean was highly variable in small samples, it was always still centered around the true population mean. This is a crucial property for any estimator: it is said to be **unbiased** if the expected value of the estimator is equal to the true value of the parameter being estimated. In other words, on average, the estimator will give you the correct answer.\n\nThe LLN gives you a different guarantee: that as the sample size increases, the *variance* of the estimator decreases, so it becomes more and more likely that the estimator will be close to the true value.\n:::\n\n### Central Limit Theorem\nThe Central Limit Theorem states that the sample mean of $n$ independent and identically distributed random variables will converge to a normal distribution, regardless of the distribution of the individual variables. This means that even if a variable is not normally distributed, if you average a bunch of them together, the sample mean will be approximately normally distributed.\n\n::: {.callout-note title=\"Central Limit Theorem\" collapse=\"false\"}\nLet $X_1, X_2, \\ldots, X_n$ be independent and identically distributed random variables with mean $\\mu$ and variance $\\sigma^2$. Then the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ converges in distribution to a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$ as $n$ approaches infinity:\n$$\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$$\n:::\nConsider rolling a die -- there is an equal chance (1/6) of getting each of the values between 1 and 6. \n\nNote that the expected value of each roll of the die is 3.5 -- we can compute this as $$E\\left[X\\right]=1\\left(\\frac{1}{6}\\right)+2\\left(\\frac{1}{6}\\right)+3\\left(\\frac{1}{6}\\right)+4\\left(\\frac{1}{6}\\right)+5\\left(\\frac{1}{6}\\right)+6\\left(\\frac{1}{6}\\right)=3.5$$\n\nSo the mean of the distribution of dice values should be 3.5, but as you can see below it's definitely not normal -- values are distributed evenly from 1 to 6 rather than clustering closer to the mean. \n\n\nBut what happens if we average the values of the 100 dice rolls in our sample? We can simulate this process many times (roll a dice 100 times, take the average, repeat) to get a distribution of the average. \n\n\nAs you can see the distribution of the sum is approximately normal! It has the mean we would expect (3.5) and clearly values are clustered symmetrically around the mean. \n\nFor the purposes of the class it's not necessary to understand why this happens, just that it happens. To give a bit of intuition, though: with more samples, averages tend to cluster around the true mean of the distribution. \n\nWhy is this useful? Well, the normal distribution is a well studied distribution with many useful properties. It is symmetric, easy to work with mathematically, and is useful for approximating the distribution of many real-world processes. The normal distribution is decently good description of most data that clusters around its mean. \n\nThe CLT tells us that even if we don't know the distribution of the individual variables, if we average enough of them together, we magically know the distribution of the sample mean.\n\nRemember earlier in the lecture, we talked about how statistical models are useful as long as they are a faithful description of the outcome probabilities for a DGP? Well, the CLT gives us a guarantee that for a huge class of DGPs, we can use the normal distribution to model the sample mean.\n\n\n#### Standard errors\n\nThe CLT tells us that the sample mean will converge to a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$ as $n$ approaches infinity. This means that the standard deviation of the sample mean (called the **standard error**) is equal to $\\frac{\\sigma}{\\sqrt{n}}$. \n\nWhat does it mean for the standard error to decrease as the sample size increases? It means that as we collect more data, our estimate of the mean becomes more precise. It's basically the same point as the Law of Large Numbers!\n\nIn the above example we multiplied the deviations of the sample mean ($\\bar{X}-\\mathbb{E}[X]$) by $\\sqrt{n}$ to put the distributions on the same scale, which makes it easier to visualize their shapes (so we can see that they become increasingly \"normal\"). But without that scaling, the distributions get increasingly narrow as $n$ increases, which is exactly what the standard error describes.\n","srcMarkdownNoYaml":"\n\n## Data Generating Processes\n\nIn the previous lecture we saw examples of running simulations to generate data, whether by simply drawing samples from a probability distribution (i.e. flipping coins / rolling dice) or by simulating a complex process explicitly (the busking musician example). \n\nThis leads us to a useful way of thinking about data: **all data is generated by some underlying process**. The process can be simple or complex, deterministic or stochastic, observed or unobserved, but it is always there. If that sounds obvious, it is because it is! \"Some process\" is a bit of a catch-all: of course the data doesn't just appear out of nowhere. However, it is also important to keep in mind because, as we will see, thinking about **data generating processes** (DGPs) is the key to analyzing data.\n\n\n## Statistical Models\nA **statistical model** is a formal mathematical representation of a data generating process. Specifically, it describes the probability distribution of the data. Based on the model, we can make precise statements about the data generated by the process. For example, we can say how likely it is to observe a certain value or set of values. We can tell what the average (or expected) value is, what the most likely value is, and so on.\n\nLet's return to coin flips once again. The data generating process is the flipping of a coin, which has two possible outcomes: heads or tails. The statistical model for this process is a **Bernoulli distribution**, which describes the probability of each outcome. Specifically, \n\n$$P(X) = \\begin{cases}\np & \\text{if } X = 1 ~\\text{heads} \\\\\n1 - p & \\text{if } X = 0 ~\\text{tails}\n\\end{cases}$$\n\nwhere \\(X\\) is the outcome of the coin flip, \\(p\\) is the probability of heads, and \\(1 - p\\) is the probability of tails. If we assume a fair coin, then \\(p = 0.5\\).\n\nNow clearly, this model does not capture the complexity of a real-world coin flip, which is of course influenced by many factors such as the weight of the coin, the force of the flip, air resistance, etc. Statistical models are always reductive in this sense. But the important thing is that a Bernoulli distribution really does do a good job of describing the **outcomes** of a coin flip. As long as that is the case, we can use the model to make predictions about the data generated by the process.\n\nTo see this, consider the following process: you roll a fair die 100 times. For each roll, you record whether the number you rolled was even or odd. Since there are three even numbers (2, 4, 6) and three odd numbers (1, 3, 5), and each number has an equal probability of being rolled, the probability of rolling an even number is 0.5 and the probability of rolling an odd number is also 0.5. This is statistically identical to flipping a fair coin! So we can use the same Bernoulli distribution to model the outcomes of this process, even though it completely ignore the details of the die rolling process itself. We lose information about the specific numbers rolled, but as long as we accurately capture the probability of the outcomes we care about, it doesn't matter.\n\n\n\n<!-- Think back to Lecture 00, where we analyzed a dataset of Airbnb listings.  -->\n\nStatistical modeling generally uses this kind of trick. We attempt to find a simple model that captures the essential features of the data generating process. \n\nWe don't always know as much about the data generating process as we do for a coin flip or a die roll. In fact, often we don't know anything about it at all! In these cases, the approach is a little more \"guess and check\". \nWe start with a simple model, and see how well it describes the data. If it doesn't work, we try a more complex model or a different model altogether. \n\n### Challenges with finite samples\nIt can be hard to tell if you have a good model or not. With any finite dataset, it is quite likely that the data will not perfectly match the model. This is because the model describes probabilistic *tendencies* of the data. But in any small sample we will likely see deviations from the idealized outcomes described by the model. \n\nThink about this in the extreme: if you flip a coin only once, you will either get heads or tails. So your observed proportion of heads will be either 0 or 1, which is very different from the expected proportion of 0.5. In fact it is impossible to observe the expected proportion of heads in a single flip!\nThis is true for a single observation of basically any random variable that takes on more than one value -- it is close to impossible to learn anything about the variable's tendencies from just one observation. \n\nEven with a small number of observations, it is quite difficult to tell if a model is a good fit for the data. \n\nLet's say you and your roommate are always arguing about who should take out the trash. You decide to flip a coin to decide who takes it out (you are notorious for always choosing tails, because you think it's good luck). You decide to do a best-of-ten series, so you flip the coin 10 times and record the number of heads. It turns out that you get 3 heads and 7 tails. Your roommate is *furious*: \"That's not fair! You always get tails! I bet you rigged the coin!\"\n\nIs your roommate justified in being suspicious? What is the probability of getting 3 heads in 10 flips of a fair coin? What is the probability of getting 3 heads in 10 flips of a biased coin that has a 25\\% chance of landing on heads?\n\n\nAs this exercise shows, with a small number of observations you have *some* information that can help you distinguish between the two models, but it is not enough to be confident in your conclusion. \n\nWith more data though, suddenly the evidence becomes much stronger. The probability of getting a badly imbalanced distribution of heads and tails becomes tiny as the number of flips increases.\n\nIn general this is great news -- the more data you have, the more confident you can be in your conclusions. It's probably not worth flipping a coin 1000 times to decide who takes out the trash, but if you did, you would be able to tell with much more certainty whether the coin is fair or not.\n\n## Convergence for large sample sizes\nWhen the size of a dataset is large enough, we get some guarantees.\n\n### Law of Large Numbers\nThe Law of Large Numbers (LLN) states that as the sample size increases, the sample mean will converge to the population mean. In other words, if we take enough samples, the average of those samples will be close to the true average of the population.\n\n::: {.callout-note title=\"Law of Large Numbers\" collapse=\"false\"}\nLet $X_1, X_2, \\ldots, X_n$ be independent and identically distributed random variables with expected value $\\mathbb{E}[X]$, then the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ converges to $\\mathbb{E}[X]$ as $n$ approaches infinity:\n$$\\mathbb{P} \\left[\\lim_{n \\to \\infty} \\bar{X}_n = \\mathbb{E}[X]\\right] = 1$$\n\nPrecisely, this states that the probability that the sample mean converges to the population mean is 1. \n^[\nThis is technically the *Strong Law of Large Numbers*, stating that the sample mean converges almost surely to the population mean. There is also a *Weak Law of Large Numbers* which states that the sample mean converges in probability to the population mean, which is a weaker condition.\n]\n:::\n\nYou don't just have to take this for granted -- let's see the LLN in action. \n\nWe will simulate flipping a biased coin $n$ times, and plot the proportion of heads as we increase the number of flips. We will see that the proportion converges to 0.25 as the number of flips increases.\n\nThat's just one experiment (flip $n$ coins a single time). But this is a random process, so we can simulate it many times\nand see how the proportion of heads changes. \n\nAs you can see, the variability across simulations decreases as the number of flips increases. This is what we mean when we say that the sample mean converges to the population mean. It's not just close to the true mean on average, it is also more consistent across different samples/simulations.\n\nWe're going to take advantage of this fact all the time. Basically any time you compare groups, or competing hypotheses, or evaluate predictive models, you end up taking the average of some quantity. The Law of Large Numbers tells us that as the sample size increases, the average will converge to the true value.\n\nAgain, what does this mean for your argument with your roommate? As you gather more and more data (coin flips), the proportion of heads (which is itself a sample mean) will converge to the true proportion of heads (the population mean). With enough flips, it becomes very clear whether the coin is rigged or not.\n\n:::{.callout-note title=\"Unbiased Estimators\"}\nNotice in the simulation above that even when the sample mean was highly variable in small samples, it was always still centered around the true population mean. This is a crucial property for any estimator: it is said to be **unbiased** if the expected value of the estimator is equal to the true value of the parameter being estimated. In other words, on average, the estimator will give you the correct answer.\n\nThe LLN gives you a different guarantee: that as the sample size increases, the *variance* of the estimator decreases, so it becomes more and more likely that the estimator will be close to the true value.\n:::\n\n### Central Limit Theorem\nThe Central Limit Theorem states that the sample mean of $n$ independent and identically distributed random variables will converge to a normal distribution, regardless of the distribution of the individual variables. This means that even if a variable is not normally distributed, if you average a bunch of them together, the sample mean will be approximately normally distributed.\n\n::: {.callout-note title=\"Central Limit Theorem\" collapse=\"false\"}\nLet $X_1, X_2, \\ldots, X_n$ be independent and identically distributed random variables with mean $\\mu$ and variance $\\sigma^2$. Then the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ converges in distribution to a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$ as $n$ approaches infinity:\n$$\\bar{X}_n \\xrightarrow{d} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$$\n:::\nConsider rolling a die -- there is an equal chance (1/6) of getting each of the values between 1 and 6. \n\nNote that the expected value of each roll of the die is 3.5 -- we can compute this as $$E\\left[X\\right]=1\\left(\\frac{1}{6}\\right)+2\\left(\\frac{1}{6}\\right)+3\\left(\\frac{1}{6}\\right)+4\\left(\\frac{1}{6}\\right)+5\\left(\\frac{1}{6}\\right)+6\\left(\\frac{1}{6}\\right)=3.5$$\n\nSo the mean of the distribution of dice values should be 3.5, but as you can see below it's definitely not normal -- values are distributed evenly from 1 to 6 rather than clustering closer to the mean. \n\n\nBut what happens if we average the values of the 100 dice rolls in our sample? We can simulate this process many times (roll a dice 100 times, take the average, repeat) to get a distribution of the average. \n\n\nAs you can see the distribution of the sum is approximately normal! It has the mean we would expect (3.5) and clearly values are clustered symmetrically around the mean. \n\nFor the purposes of the class it's not necessary to understand why this happens, just that it happens. To give a bit of intuition, though: with more samples, averages tend to cluster around the true mean of the distribution. \n\nWhy is this useful? Well, the normal distribution is a well studied distribution with many useful properties. It is symmetric, easy to work with mathematically, and is useful for approximating the distribution of many real-world processes. The normal distribution is decently good description of most data that clusters around its mean. \n\nThe CLT tells us that even if we don't know the distribution of the individual variables, if we average enough of them together, we magically know the distribution of the sample mean.\n\nRemember earlier in the lecture, we talked about how statistical models are useful as long as they are a faithful description of the outcome probabilities for a DGP? Well, the CLT gives us a guarantee that for a huge class of DGPs, we can use the normal distribution to model the sample mean.\n\n\n#### Standard errors\n\nThe CLT tells us that the sample mean will converge to a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$ as $n$ approaches infinity. This means that the standard deviation of the sample mean (called the **standard error**) is equal to $\\frac{\\sigma}{\\sqrt{n}}$. \n\nWhat does it mean for the standard error to decrease as the sample size increases? It means that as we collect more data, our estimate of the mean becomes more precise. It's basically the same point as the Law of Large Numbers!\n\nIn the above example we multiplied the deviations of the sample mean ($\\bar{X}-\\mathbb{E}[X]$) by $\\sqrt{n}$ to put the distributions on the same scale, which makes it easier to visualize their shapes (so we can see that they become increasingly \"normal\"). But without that scaling, the distributions get increasingly narrow as $n$ increases, which is exactly what the standard error describes.\n"},"formats":{"live-html":{"identifier":{"display-name":"HTML","target-format":"live-html","base-format":"html","extension-name":"live"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["/Users/jrudoler/Documents/teaching/understanding-uncertainty/_extensions/r-wasm/live/live.lua"],"toc":true,"reference-location":"margin","output-file":"lecture-04.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","ojs-engine":true,"revealjs-plugins":[],"theme":{"light":"flatly","dark":"darkly"},"title":"Lecture 04: Data Generating Processes and Statistical Models","date":"now","citation-location":"margin","pyodide":{"packages":["matplotlib","numpy"]},"toc-location":"right","code-summary":"Code","code-block-name":"Code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["live-html"]}