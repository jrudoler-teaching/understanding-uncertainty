{"title":"Lecture 05: Hypothesis Testing","markdown":{"yaml":{"title":"Lecture 05: Hypothesis Testing","date":"now","format":{"live-html":{"toc":true,"toc-location":"right","code-fold":true,"code-tools":true,"code-summary":"Code","code-block-name":"Code"}},"reference-location":"margin","citation-location":"margin","code-annotations":"hover","pyodide":{"packages":["matplotlib","numpy"]},"execute":{"warning":false}},"headingText":"The burden of proof","containsRefs":false,"markdown":"\n\nIn the introductory lecture, we talked about the role of inference: drawing more general conclusions (about a population) from specific observations (a sample). Now that we have covered the most important building blocks, we can start to actually talk about how to make statistical inferences.\n\nHow can we use statistics to \"prove\" something? In diverse domains ranging from politics to sports, people often cite data as evidence. They may cherry-pick data that appears to support their argument, often constructing charts that highlight the point they want to make. You might hear data-grounded statements like \"the unemployment rate is lower than it was 4 years ago\" or \"player X has the highest scoring average in the league\". These statistical facts can sound iron-clad, but they ignore the complexity of the underlying data and the uncertainty therein. They typically fail to acknowledge aspects of the data that do not support their argument, and they are rarely transparent about how the data was collected or analyzed. \n\nFor example, a politician might claim that their policies have led to a reduction in unemployment. Their support for this claim might be primarily based on a single statistic: \"the unemployment rate is lower than it was 4 years ago\". However, this statement does not provide any context or evidence that the politician's policies are actually responsible for the change. \n\nLet's look at unemployment data from the US Bureau of Labor Statistics (BLS) ^[from [here](https://data.bls.gov/dataQuery/find?fq=survey:%5Bln%5D&s=popularity:D), accessed July 2025]\n\nWhat do you think? Are you convinced by the data presented? There's a pretty clear trend showing that unemployment decreased, as the politician claimed. It has basically leveled off in the last few years, but it is still lower than it was 4 years ago. \n\nTo see one of the many things that the politician is not telling you, let's look at the unemployment rate over a longer time period.\n\nHmmm. Suddenly the data looks a lot less convincing. Unemployment **spiked** in 2020 due to the COVID-19 pandemic, and by 2021 it was not yet back to pre-pandemic levels. This complicates the politician's claim. The decrease in unemployment over the last 4 years is not necessarily due to their policies, but rather a recovery from an unprecedented event.\n\nIf you changed the time period you were looking at, you could easily come to a different conclusion. For example, if you looked at the last 3 years or the last 6 years, you would see that unemployment actually increased. \n\nThe key point is that *one should always be skeptical about data presented as evidence*. Data can be manipulated, cherry-picked, or misinterpreted to support a particular narrative. It's especially hard to evaluate the validity of a claim when the data is not presented in a transparent way. Because the world is so complicated, there are almost always alternative explanations for a given observation. In this case, the politician cited statistic can potentially be explained by external factors (like the pandemic) rather than their policies. \n\nTo make matters worse, as we saw in the previous lecture, the data itself is not perfect. There is additional variability in the data generating process that we have not accounted for. You will notice that the unemployment rate is not a smooth line, but rather has some fluctuations. This is at least partly due to the fact that the data is collected from a sample of the population, and there is uncertainty in the estimates. ^[They survey around 50,000 housing units each month. See the BLS's [documentation](https://www.bls.gov/opub/hom/cps/design.htm#sample-size) for more information on the sample size and design of the Current Population Survey, which is used to estimate the unemployment rate.] How do we know that a change in the unemployment rate is not just due to random fluctuations in the data?\n\nThe **burden of proof** is on the person making the claim. If you want to convince someone that your claim is true, you need to provide evidence that supports it. \n\n:::{.callout-note title=\"Beyond a reasonable doubt?\" collapse=true}\nIn a legal context, the phrase \"beyond a reasonable doubt\" refers to the standard of proof required to convict a defendant in a criminal trial (in the U.S.). It means that the evidence presented must be so convincing that there is no reasonable doubt left in the mind of a juror about the defendant's guilt. Of course you can never be 100% certain -- perhaps the defendant was being mind-controlled by aliens! -- but the evidence must be strong enough to convince a reasonable person that the defendant is guilty.\nAny alternative explanations must be ruled out as implausible. This is a high standard of proof, reflecting the serious consequences of a criminal conviction. If the prosecution fails to meet this standard, the defendant is presumed innocent and acquitted of the charges.\n\nIn the context of statistical claims, we apply a similar standard of evidence. \"Beyond a reasonable doubt\" in our case means that \nthe alternative explanations for the data are so unlikely (i.e. the probability of them being true is very small) that a reasonable person would simply ignore them. This is really a subjective judgment that is mostly about risk tolerance. \n\nWhat level of uncertainty are you willing to accept? Is it okay to be wrong 10% of the time? 5%? 1%? 0.1%? \n:::\n\n### Formalizing hypotheses\n\nOne way to increase the transparency of empirical claims is to formalize them as hypotheses. A hypothesis is a specific, testable statement about observed variables. This helps us clarify exactly what the claim is, and design tests to evaluate it. \n\nIn the U.S. legal system, a defendant is presumed innocent until proven guilty. In statistical hypothesis testing, we apply a similar principle:\n\nNull hypothesis ($H_0$) \n: The null hypothesis that represents the status quo or a baseline assumption. \nWe call it a \"null\" hypothesis because it is often a statement of no effect or no difference. \n\nAlternative hypothesis (denoted by $H_1$) \n: The alternative hypothesis represents the claim we want to test (in the legal analogy, these are the charges brought against the defendant). It is the so-called \"alternative\" because it is by definition the logical complement of the null hypothesis (meaning $H_1$ is true whenever $H_0$ is false). If the data provides enough evidence against the null hypothesis, we can <span style=\"color:red\">**reject**</span> it in favor of the alternative hypothesis.\n\nThe quantity of interest in hypothesis testing is the **$p$-value**. \n\n$p$-value \n: the probability of a random process producing a result at least as extreme as the observed data, assuming that the null hypothesis is true.^[This is often phrased as \"under the assumption that the null hypothesis is true\" or more succinctly \"under the null\".]\n\nIn other words, *p* is just the probability that the null is true and our observed data is a result of randomness / sampling variability. The logic of hypothesis testing is that if the $p$-value is very small, it suggests that the data is unlikely to have occurred under the null hypothesis. In this case, we would <span style=\"color:red\">reject</span> the null hypothesis in favor of the alternative hypothesis.  \n\nTo get an intuition for $p$-values, we can visualize a simple example. Let's say that we have a random variable $X$ that follows a normal distribution with unknown mean $\\mu$ and standard deviation $\\sigma = 1$. We want to test the hypothesis that $\\mu = 0$ (the null hypothesis) against the alternative hypothesis that $\\mu \\neq 0$ (the alternative hypothesis). We take a sample of $X$ and observe that its value is $X_1 = 2.1$. What is the $p$-value?\n\nTo get the $p$-value, we need the probability distribution of $X$ under the null hypothesis. Since we assume that $X$ follows a normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 1$, we know that the probability density function (PDF) of $X$ is given by:\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n$$\n\nWe can plot this function to visualize the null distribution. Then the question is: what is the probability of sampling a value at least as extreme as $X_1 = 2.1$?\n\nThe answer, it turns out, is exactly equal to the area under the curve of the null distribution where $|X| \\geq 2.1$. These regions are shaded in the figure below.\n\nRecall that the area under the curve of a PDF represents the probability of the random variable falling within a certain range. So the $p$-value in many cases can be computed as the area under the PDF of the null distribution where the observed data is at least as extreme as the observed value. \n\n::: {.callout-note title=\"Statistical significance\" collapse=true}\nThe $p$-value is often compared to a pre-specified threshold for \"significance\" (denoted by $\\alpha$). This number is essentially your risk tolerance: it represents the maximum probability of incorrectly rejecting the null hypothesis.\n\nIf $p < \\alpha$, we say that the result is statistically significant.\n\nIf $p \\geq \\alpha$, we say that the result is not statistically significant.\n\nOften in practice, $\\alpha$ is set to 0.05, meaning that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. In our legal analogy, this would mean that we are willing to convict the defendant if there is only a 5% chance that they are actually innocent. \n\nThe threshold for significance is pretty arbitrary, and it is often set based on convention rather than a careful consideration of the specific context. It is typically better to report the $p$-value itself, rather than just saying whether the result is significant or not. This way, you can see how strong the evidence is against the null hypothesis, rather than just a binary decision.\n:::\n\nA few important notes:\n\n- $H_0$ and $H_1$ should be mutually exclusive: if one is true, the other must be false. Otherwise, you cannot make a clear decision about which hypothesis is supported by the data.\n- $H_0$ and $H_1$ should be exhaustive: one of them must be true (there are no other possibilities). Again, this helps us avoid a situation where *neither* hypothesis is supported by the data.\n\nFurthermore, hypothesis testing does not provide absolute certainty. Instead, it provides a framework for making decisions based on the evidence at hand. \n\n- If we fail to <span style=\"color:red\">reject</span> the null hypothesis, it does not necessarily mean that it is true or that we believe it to be true. It simply means that the data does not provide enough evidence to reject it, and accordingly we *act as if* it is true.\n- If we <span style=\"color:red\">reject</span> the null hypothesis, we cannot say that the alternative hypothesis is true with absolute certainty. We can only say that the data provides strong evidence against the null hypothesis, and therefore we <span style=\"color:blue\">accept</span> the alternative hypothesis as a likely explanation for the data.\n\nFor this reason, we encourage **always reporting the $p$-value** when discussing hypothesis tests. The $p$-value provides a sense of how much uncertainty you have in your conclusion. \n\n::: {.callout-warning title=\"Choosing a null hypothesis\" collapse=true}\nChoosing an appropriate null hypothesis is a crucial step in hypothesis testing. The best way to think about this is to consider the null hypothesis as the baseline you're comparing your data against. \n\nFor a rough claim like \"Joey is good at soccer\", a possible null hypothesis could be \"Joey's soccer skills are no better than the average person.\" This would be a pretty easy baseline to beat,  since most people do not play soccer much. If Joey plays soccer regularly, he is likely to be better than the average person.\n\nInstead, you might want to compare Joey's skills to a more relevant group, like other players in his league. In that case, the null hypothesis could be \"Joey's soccer skills are no better than the average player in his league.\" This is a much more challenging baseline to beat, since the average player in the league is likely to be more skilled than the average person.\n\nThe null hypothesis you choose has a big impact on how we should interpret the results of the hypothesis test. \n:::\n\n### A worked example: the rigged coin\nLet's return to the argument with your roommate from the last lecture. Your roommate has made an unproven claim that you are flipping a rigged coin in your favor. \n#### Define the null and alternative hypotheses\nWe can formalize this as a hypothesis test:\n\n- **$H_0$** (null): The coin is fair, at worst (i.e. the probability of heads is greater than or equal to 0.5).\n- **$H_1$** (alternative): The coin is rigged (i.e. the probability of heads is **less than** 0.5).\n\n\nNote that $H_1$ and $H_0$ are mutually exclusive. If the coin is fair, it cannot be rigged, and vice versa. Moreover, the two hypotheses are exhaustive: either the coin is fair or it is rigged, it cannot be neither. \n\nOur default assumption (the null) is that the coin is fair (typical coins in circulation are not rigged), and we will only <span style=\"color:red\">reject</span> this assumption if the data provides strong evidence that tails are favored over heads.\n\n::: {.callout-note title=\"One-sided vs two-sided tests\" collapse=true}\nNote that we are only interested in whether the coin is rigged in favor of tails, not whether it is rigged in favor of heads. This is because the claim your roommate wants to prove is that you are flipping a rigged coin in your favor, which implies that the probability of heads is less than 0.5. If this probability is greater than or equal to 0.5, then the coin is rigged in his favor -- surely he wouldn't complain about that! \n\nThis is an example of a **one-sided test**. In a one-sided test, we only consider one direction of the alternative hypothesis (in this case, that the coin is rigged in favor of tails).\n\nIn contrast, a **two-sided test** would consider both directions of the alternative hypothesis (i.e. that the coin is rigged in favor of heads or tails). In this case, we would have to reject the null hypothesis if the probability of heads is either less than 0.5 or greater than 0.5. We saw this in the example with the normal distribution above, where we were testing whether the mean was equal to 0 or not.\n:::\n\n#### Simulate the null distribution\nSo how do we actually compute the $p$-value? This is where the **sampling and simulation** come into play. We will simulate the process of flipping a fair coin many times, and then see how often we get a result at least as extreme as the one we observed (getting 3 heads out 10). This will give us a **sampling distribution** of the proportion of heads we would expect to see if the coin is fair.\n\nSampling distribution\n: The distribution of a statistic (like the number of heads in 10 coin flips) computed from many repeated samples of the same size from the same population\n\nWe will use the sampling distribution as a proxy for the null distribution for our hypothesis test.\n\nNow we can see that the probability of getting 3 heads out of 10 flips of a fair coin is approximately 0.17! That's not small at all, so your roommate does not have much evidence to support their claim.\n\n\n## A worked example: The NBA's most valuable player (MVP)\n\"[Shai Gilgeous Alexander](https://en.wikipedia.org/wiki/Shai_Gilgeous-Alexander) (SGA) is the best player in the NBA\" is a very broad claim that is hard to test. What makes a player \"the best\"? Does stating that they scored the most points make them the best? That his team won the most games?\n\nLet's try to instead design a hypothesis that is more specific and testable. We can choose a specific metric to evaluate players, such as points per game (PPG). You can care about other metrics, like assists or rebounds, but let's just focus on PPG for now.\n\nTo make the claim more specific and testable, we'll start off with some assumptions:\n- Every player's scoring follows a distribution with some mean $\\mu$ and standard deviation $\\sigma$. Becuase PPG is an average over the points scored in each game, the CLT tells us that the distribution of average PPG is approximately normal (even if \"points scored\" is not). Specifically, PPG is a normal distribution with mean $\\mu$ and standard deviation $\\sigma / \\sqrt{n}$, where $n$ is the number of games played.\n- The player with the highest $\\mu$ is the best scorer in the league (i.e. assume that teammates, coaches, conferences, etc. are all negligible).\n\nTo make the claim that SGA is a better scorer than his competitors, we need to show how unlikely it is that he has the highest scoring average if his skill level is not actually the highest in the league.\n\n$H_0$: $\\mu_{\\text{SGA}} \\leq c$\n\n$H_1$: $\\mu_{\\text{SGA}} > c$\n\nAll of a sudden, this is way more familiar, specific, and testable. We can now use data to compute the $p$-value for this hypothesis test.\n\nShai scored 32.7 PPG in 76 games the 2024-2025 season, which is the highest average in the league. \nThe next-highest scorer was Giannis Antetokounmpo with 30.4 PPG in 72 games.\n\nWe will use data from the 2024-2025 NBA season, courtesy of [Basketball Reference](https://www.basketball-reference.com/), to evaluate our hypothesis.\n\nNow what is the null distribution? Our null hypothesis is that SGA's scoring ability is not higher than his competitors'. So let's say that under the null, SGA's inherent scoring ability is equal to Giannis's observed PPG of 30.4. That's the mean of our normal distribution given by CLT. For the standard error, we can use the sample standard deviation of Shai's points scored and divide it by the square root of the number of games he played (76). \n\nNow the question is: what is the probability of SGA scoring at least 32.7 PPG if his true scoring ability is actually 30.4 PPG?\n\nWe can just use the normal distribution to compute this probability!\n\n:::{.callout-note title=\"The Cumulative Distribution Function (CDF)\" collapse=true}\nThe CDF of a normal distribution gives us the probability that a random variable is less than or equal to a certain value.\nIn other words, it computes are under a probability density function (PDF) up to a certain point. \n\nTo compute the probability of a random variable being greater than a certain value, we can just subtract the CDF from 1.\n$$\nP(X > x) = 1 - P(X \\leq x) = 1 - \\text{CDF}(x) = 1 - \\int_{-\\infty}^{x} f(t) dt\n$$\n:::\n\nThis analysis tells us that if Shai's true scoring ability is 30.4 PPG, the probability of him scoring at least 32.7 PPG over 76 games is very small. \n\nWe can also try simulating the null distribution through individual game scores to see how likely it is that SGA would score at least 32.7 PPG if his true scoring ability is actually 30.4 PPG. That is, we'll simulate 76 games of scoring, we each game is drawn from a distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai's points scored. We'll repeat this simulation many times to get a distribution of PPG scores under the null hypothesis. \n\nFor this second approach, we didn't use the CLT at all! Instead, we relied on simulation to understand the distribution of Shai's scoring under the null hypothesis. And yet, the results are almost identical to the ones we got using the normal approximation!\n\nCan you think of any limitations of the approach we took? What assumptions did we make that might not be right? Next lecture we'll look at approaches that do not make any assumptions about the distribution of the data, and instead rely on resampling techniques to evaluate hypotheses.\n\n## Errors in hypothesis testing\n\nThere are two types of errors that can occur in hypothesis testing:\n\n1. False positive: reject the null hypothesis when it is actually true. \n2. False negative: fail to reject the null hypothesis when it is actually false.\n\nThese errors are also known as Type I and Type II errors, respectively.\n\nThis is a bit easier undestand if you tabulate the possible outcomes of a hypothesis test:\n\n|  | Reject $H_0$ | Do not reject $H_0$ |\n|----------------------|--------------------------|-------------------------------|\n| **$H_0$ is true** | Type I error (false positive) | Correct decision |\n| **$H_0$ is false** | Correct decision | Type II error (false negative) |\n\nThe convention in statistics is to denote the probability of a Type I error (false positive) as $\\alpha$ and the probability of a Type II error (false negative) as $\\beta$.\n\n## Summary\n\nIn this lecture, we studied hypothesis testing -- a framework for evaluating claims about data and making data-driven decisions under uncertainty. We learned about the need to formalize claims and quantify uncertainty due to sampling variability, how to establish a null hypothesis, the meaning of the $p$-value, and how to compute it based on sampling distributions.\n\nIn the next lecture, we will think about what to do when we don't have any idea about the underlying distribution of the data, which makes it hard to generate sampling distributions. \n\n","srcMarkdownNoYaml":"\n\nIn the introductory lecture, we talked about the role of inference: drawing more general conclusions (about a population) from specific observations (a sample). Now that we have covered the most important building blocks, we can start to actually talk about how to make statistical inferences.\n\n### The burden of proof\nHow can we use statistics to \"prove\" something? In diverse domains ranging from politics to sports, people often cite data as evidence. They may cherry-pick data that appears to support their argument, often constructing charts that highlight the point they want to make. You might hear data-grounded statements like \"the unemployment rate is lower than it was 4 years ago\" or \"player X has the highest scoring average in the league\". These statistical facts can sound iron-clad, but they ignore the complexity of the underlying data and the uncertainty therein. They typically fail to acknowledge aspects of the data that do not support their argument, and they are rarely transparent about how the data was collected or analyzed. \n\nFor example, a politician might claim that their policies have led to a reduction in unemployment. Their support for this claim might be primarily based on a single statistic: \"the unemployment rate is lower than it was 4 years ago\". However, this statement does not provide any context or evidence that the politician's policies are actually responsible for the change. \n\nLet's look at unemployment data from the US Bureau of Labor Statistics (BLS) ^[from [here](https://data.bls.gov/dataQuery/find?fq=survey:%5Bln%5D&s=popularity:D), accessed July 2025]\n\nWhat do you think? Are you convinced by the data presented? There's a pretty clear trend showing that unemployment decreased, as the politician claimed. It has basically leveled off in the last few years, but it is still lower than it was 4 years ago. \n\nTo see one of the many things that the politician is not telling you, let's look at the unemployment rate over a longer time period.\n\nHmmm. Suddenly the data looks a lot less convincing. Unemployment **spiked** in 2020 due to the COVID-19 pandemic, and by 2021 it was not yet back to pre-pandemic levels. This complicates the politician's claim. The decrease in unemployment over the last 4 years is not necessarily due to their policies, but rather a recovery from an unprecedented event.\n\nIf you changed the time period you were looking at, you could easily come to a different conclusion. For example, if you looked at the last 3 years or the last 6 years, you would see that unemployment actually increased. \n\nThe key point is that *one should always be skeptical about data presented as evidence*. Data can be manipulated, cherry-picked, or misinterpreted to support a particular narrative. It's especially hard to evaluate the validity of a claim when the data is not presented in a transparent way. Because the world is so complicated, there are almost always alternative explanations for a given observation. In this case, the politician cited statistic can potentially be explained by external factors (like the pandemic) rather than their policies. \n\nTo make matters worse, as we saw in the previous lecture, the data itself is not perfect. There is additional variability in the data generating process that we have not accounted for. You will notice that the unemployment rate is not a smooth line, but rather has some fluctuations. This is at least partly due to the fact that the data is collected from a sample of the population, and there is uncertainty in the estimates. ^[They survey around 50,000 housing units each month. See the BLS's [documentation](https://www.bls.gov/opub/hom/cps/design.htm#sample-size) for more information on the sample size and design of the Current Population Survey, which is used to estimate the unemployment rate.] How do we know that a change in the unemployment rate is not just due to random fluctuations in the data?\n\nThe **burden of proof** is on the person making the claim. If you want to convince someone that your claim is true, you need to provide evidence that supports it. \n\n:::{.callout-note title=\"Beyond a reasonable doubt?\" collapse=true}\nIn a legal context, the phrase \"beyond a reasonable doubt\" refers to the standard of proof required to convict a defendant in a criminal trial (in the U.S.). It means that the evidence presented must be so convincing that there is no reasonable doubt left in the mind of a juror about the defendant's guilt. Of course you can never be 100% certain -- perhaps the defendant was being mind-controlled by aliens! -- but the evidence must be strong enough to convince a reasonable person that the defendant is guilty.\nAny alternative explanations must be ruled out as implausible. This is a high standard of proof, reflecting the serious consequences of a criminal conviction. If the prosecution fails to meet this standard, the defendant is presumed innocent and acquitted of the charges.\n\nIn the context of statistical claims, we apply a similar standard of evidence. \"Beyond a reasonable doubt\" in our case means that \nthe alternative explanations for the data are so unlikely (i.e. the probability of them being true is very small) that a reasonable person would simply ignore them. This is really a subjective judgment that is mostly about risk tolerance. \n\nWhat level of uncertainty are you willing to accept? Is it okay to be wrong 10% of the time? 5%? 1%? 0.1%? \n:::\n\n### Formalizing hypotheses\n\nOne way to increase the transparency of empirical claims is to formalize them as hypotheses. A hypothesis is a specific, testable statement about observed variables. This helps us clarify exactly what the claim is, and design tests to evaluate it. \n\nIn the U.S. legal system, a defendant is presumed innocent until proven guilty. In statistical hypothesis testing, we apply a similar principle:\n\nNull hypothesis ($H_0$) \n: The null hypothesis that represents the status quo or a baseline assumption. \nWe call it a \"null\" hypothesis because it is often a statement of no effect or no difference. \n\nAlternative hypothesis (denoted by $H_1$) \n: The alternative hypothesis represents the claim we want to test (in the legal analogy, these are the charges brought against the defendant). It is the so-called \"alternative\" because it is by definition the logical complement of the null hypothesis (meaning $H_1$ is true whenever $H_0$ is false). If the data provides enough evidence against the null hypothesis, we can <span style=\"color:red\">**reject**</span> it in favor of the alternative hypothesis.\n\nThe quantity of interest in hypothesis testing is the **$p$-value**. \n\n$p$-value \n: the probability of a random process producing a result at least as extreme as the observed data, assuming that the null hypothesis is true.^[This is often phrased as \"under the assumption that the null hypothesis is true\" or more succinctly \"under the null\".]\n\nIn other words, *p* is just the probability that the null is true and our observed data is a result of randomness / sampling variability. The logic of hypothesis testing is that if the $p$-value is very small, it suggests that the data is unlikely to have occurred under the null hypothesis. In this case, we would <span style=\"color:red\">reject</span> the null hypothesis in favor of the alternative hypothesis.  \n\nTo get an intuition for $p$-values, we can visualize a simple example. Let's say that we have a random variable $X$ that follows a normal distribution with unknown mean $\\mu$ and standard deviation $\\sigma = 1$. We want to test the hypothesis that $\\mu = 0$ (the null hypothesis) against the alternative hypothesis that $\\mu \\neq 0$ (the alternative hypothesis). We take a sample of $X$ and observe that its value is $X_1 = 2.1$. What is the $p$-value?\n\nTo get the $p$-value, we need the probability distribution of $X$ under the null hypothesis. Since we assume that $X$ follows a normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 1$, we know that the probability density function (PDF) of $X$ is given by:\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n$$\n\nWe can plot this function to visualize the null distribution. Then the question is: what is the probability of sampling a value at least as extreme as $X_1 = 2.1$?\n\nThe answer, it turns out, is exactly equal to the area under the curve of the null distribution where $|X| \\geq 2.1$. These regions are shaded in the figure below.\n\nRecall that the area under the curve of a PDF represents the probability of the random variable falling within a certain range. So the $p$-value in many cases can be computed as the area under the PDF of the null distribution where the observed data is at least as extreme as the observed value. \n\n::: {.callout-note title=\"Statistical significance\" collapse=true}\nThe $p$-value is often compared to a pre-specified threshold for \"significance\" (denoted by $\\alpha$). This number is essentially your risk tolerance: it represents the maximum probability of incorrectly rejecting the null hypothesis.\n\nIf $p < \\alpha$, we say that the result is statistically significant.\n\nIf $p \\geq \\alpha$, we say that the result is not statistically significant.\n\nOften in practice, $\\alpha$ is set to 0.05, meaning that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis. In our legal analogy, this would mean that we are willing to convict the defendant if there is only a 5% chance that they are actually innocent. \n\nThe threshold for significance is pretty arbitrary, and it is often set based on convention rather than a careful consideration of the specific context. It is typically better to report the $p$-value itself, rather than just saying whether the result is significant or not. This way, you can see how strong the evidence is against the null hypothesis, rather than just a binary decision.\n:::\n\nA few important notes:\n\n- $H_0$ and $H_1$ should be mutually exclusive: if one is true, the other must be false. Otherwise, you cannot make a clear decision about which hypothesis is supported by the data.\n- $H_0$ and $H_1$ should be exhaustive: one of them must be true (there are no other possibilities). Again, this helps us avoid a situation where *neither* hypothesis is supported by the data.\n\nFurthermore, hypothesis testing does not provide absolute certainty. Instead, it provides a framework for making decisions based on the evidence at hand. \n\n- If we fail to <span style=\"color:red\">reject</span> the null hypothesis, it does not necessarily mean that it is true or that we believe it to be true. It simply means that the data does not provide enough evidence to reject it, and accordingly we *act as if* it is true.\n- If we <span style=\"color:red\">reject</span> the null hypothesis, we cannot say that the alternative hypothesis is true with absolute certainty. We can only say that the data provides strong evidence against the null hypothesis, and therefore we <span style=\"color:blue\">accept</span> the alternative hypothesis as a likely explanation for the data.\n\nFor this reason, we encourage **always reporting the $p$-value** when discussing hypothesis tests. The $p$-value provides a sense of how much uncertainty you have in your conclusion. \n\n::: {.callout-warning title=\"Choosing a null hypothesis\" collapse=true}\nChoosing an appropriate null hypothesis is a crucial step in hypothesis testing. The best way to think about this is to consider the null hypothesis as the baseline you're comparing your data against. \n\nFor a rough claim like \"Joey is good at soccer\", a possible null hypothesis could be \"Joey's soccer skills are no better than the average person.\" This would be a pretty easy baseline to beat,  since most people do not play soccer much. If Joey plays soccer regularly, he is likely to be better than the average person.\n\nInstead, you might want to compare Joey's skills to a more relevant group, like other players in his league. In that case, the null hypothesis could be \"Joey's soccer skills are no better than the average player in his league.\" This is a much more challenging baseline to beat, since the average player in the league is likely to be more skilled than the average person.\n\nThe null hypothesis you choose has a big impact on how we should interpret the results of the hypothesis test. \n:::\n\n### A worked example: the rigged coin\nLet's return to the argument with your roommate from the last lecture. Your roommate has made an unproven claim that you are flipping a rigged coin in your favor. \n#### Define the null and alternative hypotheses\nWe can formalize this as a hypothesis test:\n\n- **$H_0$** (null): The coin is fair, at worst (i.e. the probability of heads is greater than or equal to 0.5).\n- **$H_1$** (alternative): The coin is rigged (i.e. the probability of heads is **less than** 0.5).\n\n\nNote that $H_1$ and $H_0$ are mutually exclusive. If the coin is fair, it cannot be rigged, and vice versa. Moreover, the two hypotheses are exhaustive: either the coin is fair or it is rigged, it cannot be neither. \n\nOur default assumption (the null) is that the coin is fair (typical coins in circulation are not rigged), and we will only <span style=\"color:red\">reject</span> this assumption if the data provides strong evidence that tails are favored over heads.\n\n::: {.callout-note title=\"One-sided vs two-sided tests\" collapse=true}\nNote that we are only interested in whether the coin is rigged in favor of tails, not whether it is rigged in favor of heads. This is because the claim your roommate wants to prove is that you are flipping a rigged coin in your favor, which implies that the probability of heads is less than 0.5. If this probability is greater than or equal to 0.5, then the coin is rigged in his favor -- surely he wouldn't complain about that! \n\nThis is an example of a **one-sided test**. In a one-sided test, we only consider one direction of the alternative hypothesis (in this case, that the coin is rigged in favor of tails).\n\nIn contrast, a **two-sided test** would consider both directions of the alternative hypothesis (i.e. that the coin is rigged in favor of heads or tails). In this case, we would have to reject the null hypothesis if the probability of heads is either less than 0.5 or greater than 0.5. We saw this in the example with the normal distribution above, where we were testing whether the mean was equal to 0 or not.\n:::\n\n#### Simulate the null distribution\nSo how do we actually compute the $p$-value? This is where the **sampling and simulation** come into play. We will simulate the process of flipping a fair coin many times, and then see how often we get a result at least as extreme as the one we observed (getting 3 heads out 10). This will give us a **sampling distribution** of the proportion of heads we would expect to see if the coin is fair.\n\nSampling distribution\n: The distribution of a statistic (like the number of heads in 10 coin flips) computed from many repeated samples of the same size from the same population\n\nWe will use the sampling distribution as a proxy for the null distribution for our hypothesis test.\n\nNow we can see that the probability of getting 3 heads out of 10 flips of a fair coin is approximately 0.17! That's not small at all, so your roommate does not have much evidence to support their claim.\n\n\n## A worked example: The NBA's most valuable player (MVP)\n\"[Shai Gilgeous Alexander](https://en.wikipedia.org/wiki/Shai_Gilgeous-Alexander) (SGA) is the best player in the NBA\" is a very broad claim that is hard to test. What makes a player \"the best\"? Does stating that they scored the most points make them the best? That his team won the most games?\n\nLet's try to instead design a hypothesis that is more specific and testable. We can choose a specific metric to evaluate players, such as points per game (PPG). You can care about other metrics, like assists or rebounds, but let's just focus on PPG for now.\n\nTo make the claim more specific and testable, we'll start off with some assumptions:\n- Every player's scoring follows a distribution with some mean $\\mu$ and standard deviation $\\sigma$. Becuase PPG is an average over the points scored in each game, the CLT tells us that the distribution of average PPG is approximately normal (even if \"points scored\" is not). Specifically, PPG is a normal distribution with mean $\\mu$ and standard deviation $\\sigma / \\sqrt{n}$, where $n$ is the number of games played.\n- The player with the highest $\\mu$ is the best scorer in the league (i.e. assume that teammates, coaches, conferences, etc. are all negligible).\n\nTo make the claim that SGA is a better scorer than his competitors, we need to show how unlikely it is that he has the highest scoring average if his skill level is not actually the highest in the league.\n\n$H_0$: $\\mu_{\\text{SGA}} \\leq c$\n\n$H_1$: $\\mu_{\\text{SGA}} > c$\n\nAll of a sudden, this is way more familiar, specific, and testable. We can now use data to compute the $p$-value for this hypothesis test.\n\nShai scored 32.7 PPG in 76 games the 2024-2025 season, which is the highest average in the league. \nThe next-highest scorer was Giannis Antetokounmpo with 30.4 PPG in 72 games.\n\nWe will use data from the 2024-2025 NBA season, courtesy of [Basketball Reference](https://www.basketball-reference.com/), to evaluate our hypothesis.\n\nNow what is the null distribution? Our null hypothesis is that SGA's scoring ability is not higher than his competitors'. So let's say that under the null, SGA's inherent scoring ability is equal to Giannis's observed PPG of 30.4. That's the mean of our normal distribution given by CLT. For the standard error, we can use the sample standard deviation of Shai's points scored and divide it by the square root of the number of games he played (76). \n\nNow the question is: what is the probability of SGA scoring at least 32.7 PPG if his true scoring ability is actually 30.4 PPG?\n\nWe can just use the normal distribution to compute this probability!\n\n:::{.callout-note title=\"The Cumulative Distribution Function (CDF)\" collapse=true}\nThe CDF of a normal distribution gives us the probability that a random variable is less than or equal to a certain value.\nIn other words, it computes are under a probability density function (PDF) up to a certain point. \n\nTo compute the probability of a random variable being greater than a certain value, we can just subtract the CDF from 1.\n$$\nP(X > x) = 1 - P(X \\leq x) = 1 - \\text{CDF}(x) = 1 - \\int_{-\\infty}^{x} f(t) dt\n$$\n:::\n\nThis analysis tells us that if Shai's true scoring ability is 30.4 PPG, the probability of him scoring at least 32.7 PPG over 76 games is very small. \n\nWe can also try simulating the null distribution through individual game scores to see how likely it is that SGA would score at least 32.7 PPG if his true scoring ability is actually 30.4 PPG. That is, we'll simulate 76 games of scoring, we each game is drawn from a distribution with mean 30.4 and standard deviation equal to the observed sample standard deviation of Shai's points scored. We'll repeat this simulation many times to get a distribution of PPG scores under the null hypothesis. \n\nFor this second approach, we didn't use the CLT at all! Instead, we relied on simulation to understand the distribution of Shai's scoring under the null hypothesis. And yet, the results are almost identical to the ones we got using the normal approximation!\n\nCan you think of any limitations of the approach we took? What assumptions did we make that might not be right? Next lecture we'll look at approaches that do not make any assumptions about the distribution of the data, and instead rely on resampling techniques to evaluate hypotheses.\n\n## Errors in hypothesis testing\n\nThere are two types of errors that can occur in hypothesis testing:\n\n1. False positive: reject the null hypothesis when it is actually true. \n2. False negative: fail to reject the null hypothesis when it is actually false.\n\nThese errors are also known as Type I and Type II errors, respectively.\n\nThis is a bit easier undestand if you tabulate the possible outcomes of a hypothesis test:\n\n|  | Reject $H_0$ | Do not reject $H_0$ |\n|----------------------|--------------------------|-------------------------------|\n| **$H_0$ is true** | Type I error (false positive) | Correct decision |\n| **$H_0$ is false** | Correct decision | Type II error (false negative) |\n\nThe convention in statistics is to denote the probability of a Type I error (false positive) as $\\alpha$ and the probability of a Type II error (false negative) as $\\beta$.\n\n## Summary\n\nIn this lecture, we studied hypothesis testing -- a framework for evaluating claims about data and making data-driven decisions under uncertainty. We learned about the need to formalize claims and quantify uncertainty due to sampling variability, how to establish a null hypothesis, the meaning of the $p$-value, and how to compute it based on sampling distributions.\n\nIn the next lecture, we will think about what to do when we don't have any idea about the underlying distribution of the data, which makes it hard to generate sampling distributions. \n\n"},"formats":{"live-html":{"identifier":{"display-name":"HTML","target-format":"live-html","base-format":"html","extension-name":"live"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["/Users/jrudoler/Documents/teaching/understanding-uncertainty/_extensions/r-wasm/live/live.lua"],"toc":true,"reference-location":"margin","output-file":"lecture-05.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","ojs-engine":true,"revealjs-plugins":[],"theme":{"light":"flatly","dark":"darkly"},"title":"Lecture 05: Hypothesis Testing","date":"now","citation-location":"margin","code-annotations":"hover","pyodide":{"packages":["matplotlib","numpy"]},"toc-location":"right","code-summary":"Code","code-block-name":"Code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["live-html"]}