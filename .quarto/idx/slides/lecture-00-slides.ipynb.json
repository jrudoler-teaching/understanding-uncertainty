{"title":"Why Statistics?","markdown":{"yaml":{"title":"Why Statistics?","date":"now","format":{"revealjs":{"incremental":true,"smaller":true,"scrollable":true,"echo":false,"chalkboard":true,"fig-align":"center"}},"pyodide":{"packages":["matplotlib","numpy","seaborn"]},"execute":{"warning":false}},"headingText":"Course Goals","containsRefs":false,"markdown":"\n\n\nOur basic goals for the course are:\n\n1) to build a strong intuition about data, where it comes from, and what questions it can answer. \n\n2) to learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!\n\n## Why statistics? \nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: \n\n1. description \n2. inference\n3. prediction \n\n## Description {style=\"font-size:20px;\"}\n::: {.notes}\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand. \n:::\nLet's load in some data and take a look at it.\n\nThe [dataset](https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata) contains Airbnb listings in New York City, including prices, locations, and other features. \n\n## Description\n<!-- ^[from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata] -->\n\n\n---\n\nNow there's a lot you can do, but let's start by visualizing the prices of listings.\n\n\n---\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\n---\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the `geopandas` library to plot the locations of listings on a map of New York City. \n\n\n\n---\n::: {.notes}\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics **only describe the data**. \n\nWhy is this limiting? After all, we like data -- it tells us things about the world and it's objective and quantifiable. \n\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\n:::\n\nLet's look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small \"sample\" or subset of the data?\n\n## Sample $\\neq$ Population\n:::: {.notes}\nNotice how the samples differ from one another. They have different geography and different prices. This means you can't just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n:::\n\n**Population**\n\nthe entire set of data that you are interested in. \n\n**Sample**\n\na subset of a population. \n\nA *random sample* is a sample that is selected randomly from the population. \n\n. . .\n\n:::{.callout-tip title=\"Example: Airbnb listings in New York City\"} \nWe want to know the average price of Airbnb listings in New York City.\n\n- *population*: all Airbnb listings in New York City\n- *sample*: a smaller subset of those listings, which may or may not be representative of the entire population.\n\n:::\n\n## What is the population?\n\nFlexible definition:\n\n- Average price of all short-term rentals in New York City? Population: all rentals (not just Airbnb listings) in New York City.\n\n. . .\n\nOften, the population is actually more abstract or theoretical\n \n- Average price of all possible Airbnb listings in New York City? Population: all potential listings, not just the ones that currently exist.\n\n. . .\n\nDescriptive statistics are useful for understanding the data at hand, but they don't necessarily tell us much about the world outside of the data. For that, we need to do something more. \n\n## Quiz: restaurant survey\n\n## Inference\nWhat if we want to answer questions about a population *based* on a sample? \n\nThis is where **inference** comes in. \n\n- Use the given sample to **infer** something about the population.\n\n. . . \n\nHow do we do this if we can't ever see the entire population? \n\n- Need a link which connects the sample to the population \n- Treat the sample as the outcome of a **data-generating process** (DGP).\n\n. . .\n\n:::{.callout-caution title=\"There is always a DGP\"}\nA **data-generating process** (DGP) is a theoretical construct that describes how data is generated in a population. \n\n- Encompasses all the factors that influence the data (incl. the mechanisms and relationships between variables).\n- There *has* to be a DGP, even if we don't know what it is. \n- The DGP is the process that generates the data we observe.\n- The full, true DGP is usually unknown.\n  - We can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n:::\n\n::: {.notes}\nOf course, we don't necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\n:::\n\n## Statistical models\n\nWhen the full DGP is too complicated / unknown, we use a **model**\n\n- simplified mathematical representation of the DGP \n- allows us to make inferences about the population based on the sample\n- ultimately sort of a guess -- about where your data come from.  \n\n## Example: Airbnb listings. \n\n- Assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. \n- Probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.\n\n. . .\n\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n<!-- ### A simple model\n\nWe'll talk about probability distributions in more detail later in the course, but for now let's just say that a probability distribution is a mathematical function that describes the likelihood of different outcomes in a random process. -->\n\n<!-- The most basic example is a coin flip. If we flip a fair coin, there are two possible outcomes: heads or tails. The probability of each outcome is 0.5, so we can represent this with a simple probability distribution.\n\n\\begin{equation}\nP(X) = \\begin{cases}\n0.5 & \\text{if } X = \\text{heads} \\\\\n0.5 & \\text{if } X = \\text{tails} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{equation} -->\n\n\n. . . \n\n**Question**: \"If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?\"\n\n- question about the probability of the sample, given a certain model of the DGP\n- it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\n\n## Evaluating models\n\nWhat should we do now? \n- Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. \n- Model is just a \"guess\" about the DGP, while the sample is real data that we have observed. \n\n. . .\n\n::: {.callout-note title=\"Unlikely data or unlikely model?\"}\nThere are two main culprits when we see a sample that is unlikely under our model:\n\n1. The sample! Think of this as \"luck of the draw\". This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there's actually a 3% chance of this happening); if you flip a coin 100 times, there's virtually no chance that you'll get all tails (less than 10^-30^ chance). \n2. The model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won't go away just by collecting more data.\n:::\n\n. . .\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n## Inference requires domain knowledge\n\n:::{.callout-warning title=\"Don't try this at home!\" collapse=\"true\"}\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story. \n\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. \n\nThis requires *domain knowledge* and *subject matter expertise*. \n:::\n\nIn the Airbnb example:\n\n- Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption \n  - Manhattan sees vastly more tourism than the other boroughs \n  - Brooklyn and Queens have by far the most residents according to recent census data.\n\n## Prediction\n\nPrediction is the process of using a model to make predictions about unseen (or future) data. \n\nBack to the Airbnb data: we might want to **predict** which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\n\nTo that end we will *fit* a predictive model to the data. Basic idea of the model: \n- we assume the features of the listing (e.g., price) are related to the probability of it being in a certain borough\n  - e.g., perhaps more expensive listings are more likely to be in Manhattan\n\n. . .\n\n:::{.callout-note title=\"Fitting a model\"}\nModels generally have *parameters*, which are adjustable values that affect the model's behavior. Think of them like \"knobs\" you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker. \n\nCoin flip has a single parameter: the probability of landing on heads. \n\n- If you turn the knob to 0.5, you get a fair coin; \n- if you turn it to 1.0, you get a coin that always lands on heads; \n- if you turn it to 0.0, you get a coin that always lands on tails.\n\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by *minimizing* some kind of **error function**, which provides a measure of how well the model fits the data.\n:::\n\n## Predicting the borough of a listing\n\n## Evaluating predictions\nOk, so the model is around 45% accurate at predicting the borough of a listing. \n\n:::{.callout-tip title='What is a \"good\" prediction rate?' collapse='false'}\nFor discussion / reflection: What is a \"good\" prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\n:::\n\n::: {.notes}\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. \n\nFor now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. \n\nFor example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team's performance.\n:::\n\n## Model predictions (distribution)\nNow let's take a look at the distribution of the model's predictions.\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n::: {.notes}\nPrediction and inference are closely related, and they can often be done simultaneously. \n\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.\n:::\n\n## Summary\n\n3 objectives of data analysis: description, inference, and prediction.\n\nHopefully you now have a better understanding of **what** statistics is supposed to help you do with data. Of course,\nwe haven't actually gone into any of the details of **how** to do anything. (Don't worry, we'll get there!)\n\nUp next: \n- basic programming concepts that are important for data science. \n- After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. \n\n. . .\n\nFrom there, the sky is the limit! We'll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\n\nSince we haven't learned any programming or statistics yet, we won't have any real exercises for this lecture. \nThere's just a quick [Assignment 0](../assignments/assignment-00.ipynb) to make sure you are set up to run Python code for future assignments.\n\n<!-- {{< embed ../assignments/assignment-00.ipynb#hello-world >}} -->\n","srcMarkdownNoYaml":"\n\n## Course Goals \n\nOur basic goals for the course are:\n\n1) to build a strong intuition about data, where it comes from, and what questions it can answer. \n\n2) to learn the basic computational skills needed to manipulate and analyze data. Working with data also helps with (1)!\n\n## Why statistics? \nStatistics is, essentially, the study of data and how to use it. People argue about the purpose of statistics, but basically you can do 3 things with data: \n\n1. description \n2. inference\n3. prediction \n\n## Description {style=\"font-size:20px;\"}\n::: {.notes}\nDescriptive statistics is the process of summarizing data. This can be done with numbers (e.g., mean, median, standard deviation) or with visualizations (e.g., histograms, boxplots). Descriptive statistics, importantly, are completely limited to the sample of data at hand. \n:::\nLet's load in some data and take a look at it.\n\nThe [dataset](https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata) contains Airbnb listings in New York City, including prices, locations, and other features. \n\n## Description\n<!-- ^[from https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata] -->\n\n\n---\n\nNow there's a lot you can do, but let's start by visualizing the prices of listings.\n\n\n---\n\nComputing statistics like the mean (average), standard deviation (average distance from the mean), and quartiles (top 25% and bottom 25%) is easy.\n\n\n---\n\nWe can even use specialized libraries to make use of the geographic information in the data. For example, we can use the `geopandas` library to plot the locations of listings on a map of New York City. \n\n\n\n---\n::: {.notes}\nThere is a lot of information in the data, and we can summarize it in many different ways. But descriptive statistics **only describe the data**. \n\nWhy is this limiting? After all, we like data -- it tells us things about the world and it's objective and quantifiable. \n\nThe problem is that data is not always complete. In fact, it almost never is. And incomplete data can lead to misleading conclusions.\n:::\n\nLet's look at our Airbnb data again. What if instead of looking at the entire dataset, we only looked at a small \"sample\" or subset of the data?\n\n## Sample $\\neq$ Population\n:::: {.notes}\nNotice how the samples differ from one another. They have different geography and different prices. This means you can't just look at the descriptive statistics of a single sample and draw conclusions about the entire population.\n:::\n\n**Population**\n\nthe entire set of data that you are interested in. \n\n**Sample**\n\na subset of a population. \n\nA *random sample* is a sample that is selected randomly from the population. \n\n. . .\n\n:::{.callout-tip title=\"Example: Airbnb listings in New York City\"} \nWe want to know the average price of Airbnb listings in New York City.\n\n- *population*: all Airbnb listings in New York City\n- *sample*: a smaller subset of those listings, which may or may not be representative of the entire population.\n\n:::\n\n## What is the population?\n\nFlexible definition:\n\n- Average price of all short-term rentals in New York City? Population: all rentals (not just Airbnb listings) in New York City.\n\n. . .\n\nOften, the population is actually more abstract or theoretical\n \n- Average price of all possible Airbnb listings in New York City? Population: all potential listings, not just the ones that currently exist.\n\n. . .\n\nDescriptive statistics are useful for understanding the data at hand, but they don't necessarily tell us much about the world outside of the data. For that, we need to do something more. \n\n## Quiz: restaurant survey\n\n## Inference\nWhat if we want to answer questions about a population *based* on a sample? \n\nThis is where **inference** comes in. \n\n- Use the given sample to **infer** something about the population.\n\n. . . \n\nHow do we do this if we can't ever see the entire population? \n\n- Need a link which connects the sample to the population \n- Treat the sample as the outcome of a **data-generating process** (DGP).\n\n. . .\n\n:::{.callout-caution title=\"There is always a DGP\"}\nA **data-generating process** (DGP) is a theoretical construct that describes how data is generated in a population. \n\n- Encompasses all the factors that influence the data (incl. the mechanisms and relationships between variables).\n- There *has* to be a DGP, even if we don't know what it is. \n- The DGP is the process that generates the data we observe.\n- The full, true DGP is usually unknown.\n  - We can make assumptions about it and use those assumptions to draw inferences about the population (in the case that our assumptions are correct).\n:::\n\n::: {.notes}\nOf course, we don't necessarily know what the DGP is. If we we knew everything about how the data was generated, we probably would not have any questions to ask in the first place!\n:::\n\n## Statistical models\n\nWhen the full DGP is too complicated / unknown, we use a **model**\n\n- simplified mathematical representation of the DGP \n- allows us to make inferences about the population based on the sample\n- ultimately sort of a guess -- about where your data come from.  \n\n## Example: Airbnb listings. \n\n- Assume that the all Airbnb listings in New York City are equally likely to be in any one of the five boroughs. \n- Probability of a listing being in Manhattan is 1/5, the probability of it being in Brooklyn is 1/5, etc.\n\n. . .\n\nThen we can look at the actual sample of listings and see if it matches our assumption:\n\n<!-- ### A simple model\n\nWe'll talk about probability distributions in more detail later in the course, but for now let's just say that a probability distribution is a mathematical function that describes the likelihood of different outcomes in a random process. -->\n\n<!-- The most basic example is a coin flip. If we flip a fair coin, there are two possible outcomes: heads or tails. The probability of each outcome is 0.5, so we can represent this with a simple probability distribution.\n\n\\begin{equation}\nP(X) = \\begin{cases}\n0.5 & \\text{if } X = \\text{heads} \\\\\n0.5 & \\text{if } X = \\text{tails} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{equation} -->\n\n\n. . . \n\n**Question**: \"If we assume that all boroughs are equally likely to produce each listing, how likely is it that we would see the distribution of listings that we actually observe?\"\n\n- question about the probability of the sample, given a certain model of the DGP\n- it intuitively seems unlikely that we would see so many more listings in Manhattan and Brooklyn than in the other boroughs if all boroughs were equally likely to produce listings.\n\n## Evaluating models\n\nWhat should we do now? \n- Now that we realize our sample is very unlikely under our model, then perhaps we should reconsider our model. \n- Model is just a \"guess\" about the DGP, while the sample is real data that we have observed. \n\n. . .\n\n::: {.callout-note title=\"Unlikely data or unlikely model?\"}\nThere are two main culprits when we see a sample that is unlikely under our model:\n\n1. The sample! Think of this as \"luck of the draw\". This is only really a risk if your sample is small or systematically biased in some way. Usually if you collect enough data, the sample will start to look more like the population. If you flip a coin 5 times, you might get all tails (there's actually a 3% chance of this happening); if you flip a coin 100 times, there's virtually no chance that you'll get all tails (less than 10^-30^ chance). \n2. The model! This means that our assumptions about the DGP are incorrect or incomplete. This is a more serious problem, and it won't go away just by collecting more data.\n:::\n\n. . .\n\nStatistical inference is basically just a bunch of mathematical machinery and techniques that help us to quantify this guesswork precisely and make it rigorous.\n\n## Inference requires domain knowledge\n\n:::{.callout-warning title=\"Don't try this at home!\" collapse=\"true\"}\nWe just said that statistical inference makes guesswork rigorous, but this is not the whole story. \n\nWe will always do a much better job of inference if was have a good understanding of the DGP and the context of the data. \n\nThis requires *domain knowledge* and *subject matter expertise*. \n:::\n\nIn the Airbnb example:\n\n- Assuming that all boroughs are equally likely to produce listings is a pretty bad assumption \n  - Manhattan sees vastly more tourism than the other boroughs \n  - Brooklyn and Queens have by far the most residents according to recent census data.\n\n## Prediction\n\nPrediction is the process of using a model to make predictions about unseen (or future) data. \n\nBack to the Airbnb data: we might want to **predict** which borough a new listing belongs to based on its features (e.g., listing type, review ratings, price, etc.).\n\nTo that end we will *fit* a predictive model to the data. Basic idea of the model: \n- we assume the features of the listing (e.g., price) are related to the probability of it being in a certain borough\n  - e.g., perhaps more expensive listings are more likely to be in Manhattan\n\n. . .\n\n:::{.callout-note title=\"Fitting a model\"}\nModels generally have *parameters*, which are adjustable values that affect the model's behavior. Think of them like \"knobs\" you can turn to tune the model to do what you want, like adjusting the volume or the bass/treble on a speaker. \n\nCoin flip has a single parameter: the probability of landing on heads. \n\n- If you turn the knob to 0.5, you get a fair coin; \n- if you turn it to 1.0, you get a coin that always lands on heads; \n- if you turn it to 0.0, you get a coin that always lands on tails.\n\nFitting a model means adjusting the parameters of the model so that it best matches the data. This is usually done by *minimizing* some kind of **error function**, which provides a measure of how well the model fits the data.\n:::\n\n## Predicting the borough of a listing\n\n## Evaluating predictions\nOk, so the model is around 45% accurate at predicting the borough of a listing. \n\n:::{.callout-tip title='What is a \"good\" prediction rate?' collapse='false'}\nFor discussion / reflection: What is a \"good\" prediction rate or accuracy? Is 45% good? What about 60%? 80%? How would you tell?\n:::\n\n::: {.notes}\nLater in the course, we will talk about how to evaluate models and prediction accuracy more rigorously. \n\nFor now, just keep in mind that there is no one-size-fits-all answer to this question. It depends on factors like what you want to do with the model or how good simple alternatives might be. \n\nFor example, you want self-driving cars to be nearly 100% accurate because the cost of a mistake is so high. Perhaps a general manager drafting prospective players for a sports team would be satisfied with 60% accuracy, since they only need to be right about some players to make a big difference in the team's performance.\n:::\n\n## Model predictions (distribution)\nNow let's take a look at the distribution of the model's predictions.\n\nIt looks like the model is a bit crude (it predicts no listings in the Bronx or Staten Island), but it does at least capture the general trend that listings are more likely to be in Manhattan and Brooklyn than in the other boroughs.\n\n::: {.notes}\nPrediction and inference are closely related, and they can often be done simultaneously. \n\nThe guiding logic is that a model that makes good predictions is probably doing a good job of capturing the underlying DGP.\n:::\n\n## Summary\n\n3 objectives of data analysis: description, inference, and prediction.\n\nHopefully you now have a better understanding of **what** statistics is supposed to help you do with data. Of course,\nwe haven't actually gone into any of the details of **how** to do anything. (Don't worry, we'll get there!)\n\nUp next: \n- basic programming concepts that are important for data science. \n- After that we will learn some foundational concepts in probability that will help us think about data and models more rigorously. \n\n. . .\n\nFrom there, the sky is the limit! We'll cover a wide range of topics, including statistical inference, uncertainty quantification, machine learning, and more.\n\nSince we haven't learned any programming or statistics yet, we won't have any real exercises for this lecture. \nThere's just a quick [Assignment 0](../assignments/assignment-00.ipynb) to make sure you are set up to run Python code for future assignments.\n\n<!-- {{< embed ../assignments/assignment-00.ipynb#hello-world >}} -->\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"lecture-00-slides.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.7.31","auto-stretch":true,"title":"Why Statistics?","date":"now","pyodide":{"packages":["matplotlib","numpy","seaborn"]},"smaller":true,"scrollable":true,"chalkboard":true}}},"projectFormats":["live-html"]}